---
title: Preprocessing
jupyter: python3
html-table-processing: none
---

```{python}
#| echo: false
#| output: false
import pointblank as pb
pb.config(report_incl_footer=False)
```

While the validation methods available can do a lot for you, there's alternatively a lot of things
you *can't* easily do with them. What if you wanted to validate:

- string lengths in a column are less than 10 characters
- the median of values in a column is less than the median of values in another column
- there are at least three instances of every categorical value in a column

These are more complicated types of validations, yet checks of this type are very commonplace. We
don't need to have a very large library of validation methods to tackle each an every case; the
number of combinations indeed seems exceedingly large. Instead, let's transform the table through a
preprocessing step and expose the key values. In conjunction with that sort of table mutation, we
then can use the existing library of validation methods.

At the heart of this approach is the idea of composability. And Pointblank makes it easy to safely
transform the input table in a given validation step via the `pre=` argument. Any computed columns
are available for the lifetime of the validation step during interrogation.

Now, through a series of examples, let's look at the process of performing the validations mentioned
above. We'll use the `small_table` dataset for all of the examples. Here it is in its entirety:

```{python}
# | echo: false
pb.preview(pb.load_dataset(dataset="small_table"), n_head=20, n_tail=20)
```

## The Basics of Preprocessing the Input Table

In getting to grips with the basics, we'll try to validate that string lengths in the `b` column are
less than 10 characters. We can't directly use the
[`col_vals_lt()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_lt.html)
validation method with that column because it is meant to be used with a column of numeric values.
Let's just give that method what it needs and create a column with string lengths! The table is
a Polars DataFrame so we'll provide a lambda function that uses the Polars API to add in that
numeric column:

```{python}
import polars as pl

(
    pb.Validate(
        data=pb.load_dataset("small_table"),
        tbl_name="small_table",
        label="String lengths"
    )
    .col_vals_lt(
        columns="string_lengths",  # the generated column through `pre=`
        value=10,                  # the string length value to be less than
        pre=lambda df: df.with_columns(string_lengths=pl.col("b").str.len_chars())
    )
    .interrogate()
)
```

The validation was successfully constructed and we can see from the validation report table that all
strings in `b` had lengths less than 10 characters. We can see from the code itself that we're not
directly testing the `b` column. Instead the test is of the `string_lengths` column generated by
the lambda provided to `pre=`. We used Polars to do the transformation work here and that's the
piece that generates numerical values of string lengths in the computed column.

That transformation occurs only during interrogation and only for that validation step. Any prior or
subsequent steps would normally use the as-provided `small_table`. Having the possibility for
data transformation isolated at the step level means that you don't have to generate separate
validation plans for each form of the data, you're free to transform at will and perform validations
on different representations of the data.

Aside from using a lambda, you can pass in a custom function. Just make sure not to evaluate it at
the `pre=` parameter (everything is stored lazily until interrogation time). Here's an example of
that approach:

```{python}
def add_string_lengths(df):
    return df.with_columns(string_lengths=pl.col("b").str.len_chars())

(
    pb.Validate(
        data=pb.load_dataset("small_table"),
        tbl_name="small_table",
        label="String lengths"
    )
    .col_vals_lt(columns=pb.last_n(1), value=10, pre=add_string_lengths)
    .interrogate()
)
```

The column-generating logic was placed in the `add_string_lengths()` function, which is then passed
to `pre=`. We also know that the column generated will be the final one in the column series, so the
`last_n()` column selector obviates the need to provide `"string_lengths"` here (you'll still find
the target column name echoed on the validation report table).

## Using Narwhals to Preprocess Many Types of DataFrames

In this previous example we used a Polars table (the `load_dataset()` returns a Polars DataFrame by
default). You might have a situation where where you perform data validation variously on Pandas and
Polars DataFrames. This is where Narwhals becomes handy.

Let's obtain `small_table` as a Pandas DataFrame. We'll construct a validation step to verify that
the median of column `c` is greater than the median in column `a`.

```{python}
import narwhals as nw

(
    pb.Validate(
        data=pb.load_dataset("small_table", tbl_type="pandas"),
        tbl_name="small_table",
        label="String lengths",
    )
    .col_vals_gt(
        columns="c",
        value=pb.col("a"),
        pre=lambda df: nw.from_native(df).select(nw.median("c"), nw.median("a"))
    )
    .interrogate()
)
```

There's a bit to unpack here so let's look at at the lambda function first. Narwhals can translate
a Pandas DataFrame to a Narwhals DataFrame with its `from_native()` function. After that initiating
step, you're free to use the Narwhals API (which is modeled on a subset of the Polars API) to do the
necessary data transformation. In this case, we are getting the medians of the `c` and `a` columns
and ending up with a one-row, two-column table.

The goal is to check that the median value of `c` is greater than the corresponding median of
column `a`, so we set up `columns=` and `value=` parameters in that way within the
[`col_vals_gt()`](https://posit-dev.github.io/pointblank/reference/Validate.col_vals_gt.html)
validation method call.
