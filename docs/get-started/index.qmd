---
title: Intro
jupyter: python3
html-table-processing: none
---
 ```{python}
#| echo: false
#| output: false
import pointblank as pb
pb.config(report_incl_footer=False)
```

The Pointblank library is all about assessing the state of data quality in a table. You provide the
validation rules and the library will dutifully interrogate the data and provide useful reporting.
We can use different types of tables like Polars and Pandas DataFrames, Parquet files, or various
database tables (thanks to Ibis support). Let's walk through what table validation looks like in
Pointblank.

## A Simple Validation Table

This is a validation report table that is produced from a validation of a Polars DataFrame:

```{python}
# | code-fold: true

import pointblank as pb

validation_1 = (
    pb.Validate(data=pb.load_dataset(dataset="small_table"), label="Example Validation")
    .col_vals_lt(columns="a", value=10)
    .col_vals_between(columns="d", left=0, right=5000)
    .col_vals_in_set(columns="f", set=["low", "mid", "high"])
    .col_vals_regex(columns="b", pattern=r"^[0-9]-[a-z]{3}-[0-9]{3}$")
    .interrogate()
)

validation_1
```

Each row in this reporting table constitutes a single validation step. Roughly, the left-hand side
outlines the validation rules and the right-hand side provides the results of each validation step.
While simple in principle, there's a lot of useful information packed into this validation table.

Here's a diagram that describes a few of the important parts of the validation table:

![](/assets/pointblank-validation-table.png){width=100%}

There are three things that should be noted here:

- validation steps: each step is a separate test on the table, focused on a certain aspect of the
table
- validation rules: the validation type is provided here along with key constraints
- validation results: interrogation results are provided here, with a breakdown of test units
(*total*, *passing*, and *failing*), threshold flags, and more

The intent is to provide the key information in one place, and have it be interpretable by data
stakeholders.

## Example Code, Step-by-Step

Here's the code that performs the validation on the Polars table.

```{python}
import pointblank as pb

validation_2 = (
    pb.Validate(data=pb.load_dataset(dataset="small_table"))
    .col_vals_lt(columns="a", value=10)
    .col_vals_between(columns="d", left=0, right=5000)
    .col_vals_in_set(columns="f", set=["low", "mid", "high"])
    .col_vals_regex(columns="b", pattern=r"^[0-9]-[a-z]{3}-[0-9]{3}$")
    .interrogate()
)

validation_2
```

Note these three key pieces in the code:

- the `Validate(data=...)` argument takes a DataFrame or database table that you want to validate
- the methods starting with `col_*` specify validation steps that run on specific columns
- the `interrogate()` method executes the validation plan on the table

This common pattern is used in a validation workflow, where `Validate()` and `interrogate()` bookend
a validation plan generated through calling validation methods. And that's data validation with
Pointblank in a nutshell! In the next section we'll go a bit further by introducing a means to gauge
data quality with failure thresholds.

## Understanding Test Units

Each validation step will execute a type of validation test on the target table. For example, a
`col_vals_lt()` validation step can test that each value in a column is less than a specified
number. The key finding that's reported as a result of this test is the number of test units that
pass or fail.

Test units are dependent on the test being run. The collection of `col_vals_*` validation methods
will test each and every value in a particular column, so each value will be a test unit (and the
number of test units is the number of rows in the target table). Some validation methods like
`col_exists()` or `row_count_match()` have only a single test unit since they aren't testing
individual values but rather if a single condition is true or false.

Knowing about the numbers of test units across validation methods matters because you have the
option to set thresholds (that can signal 'warning', 'error', and 'critical' flags) based on either
the relative proportion or absolute number of failing test units.

Here's a simple example that uses a single `col_vals_lt()` step along with thresholds set in the
`thresholds=` argument of the validation method.

```{python}
validation_3 = (
    pb.Validate(data=pb.load_dataset(dataset="small_table"))
    .col_vals_lt(columns="a", value=7, thresholds=(2, 4))
    .interrogate()
)

validation_3
```

The code uses `thresholds=(2, 4)` to set a 'warning' threshold of `2` and an `error` threshold of
`4`. If you look at the validation report table, we can see:

- the `FAIL` column shows that 2 tests units have failed
- the `W` column (short for 'warning') shows a filled gray circle indicating those failing test
units reached that threshold value
- the `E` column (short for 'error') shows an open yellow circle indicating that the number of
failing test units is below that threshold

The one final threshold, `C` (for 'critical'), wasn't set so it appears on the validation table as a
long dash.

Thresholds let you take action at different levels of severity. The next section discusses setting
and acting on thresholds in detail.

## Using Threshold Levels

Thresholds enable you to signal failure at different severity levels. In the near future, thresholds
will be able to trigger custom actions (should those actions be defined).

Here's an example where we test if a certain column has null/missing values with
`col_vals_not_null()`. This is a case where we want a warning on *any* null values and a flag at a
greater severity level when there are 20% or more null values in the column.

```{python}
validation_4 = (
    pb.Validate(data=pb.load_dataset(dataset="small_table"))
    .col_vals_not_null(columns="c", thresholds=(1, 0.2))
    .interrogate()
)

validation_4
```

In this case, the `thresholds=` argument in the `cols_vals_not_null()` step was set to `(1, 0.2)`,
indicating 1 failing test unit is set for 'warning' and a `0.2` fraction of all failing test units
is set to 'error'.

For more on thresholds, see the [Thresholds article](./thresholds.qmd).
