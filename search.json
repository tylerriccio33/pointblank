[
  {
    "objectID": "demos/01-starter/index.html",
    "href": "demos/01-starter/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Starter Validation\nA validation with the basics.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    A starter validationPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:21:21 UTC&lt; 1 s2025-05-23 02:21:21 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate( # Use pb.Validate to start\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"A starter validation\"\n    )\n    .col_vals_gt(columns=\"d\", value=1000)       # STEP 1 |\n    .col_vals_le(columns=\"c\", value=5)          # STEP 2 | &lt;-- Build up a validation plan\n    .col_exists(columns=[\"date\", \"date_time\"])  # STEP 3 |\n    .interrogate()  # This will execute all validation steps and collect intel\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/04-sundered-data/index.html",
    "href": "demos/04-sundered-data/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Sundered Data\nSplitting your data into ‘pass’ and ‘fail’ subsets.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Sundering DataPandassmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-05-23 02:21:14 UTC&lt; 1 s2025-05-23 02:21:14 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows4Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    3\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    4\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Sundering Data\"\n    )\n    .col_vals_gt(columns=\"d\", value=1000)\n    .col_vals_le(columns=\"c\", value=5)\n    .interrogate()\n)\n\nvalidation\npb.preview(validation.get_sundered_data(type=\"pass\"))\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/02-advanced/index.html",
    "href": "demos/02-advanced/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Advanced Validation\nA validation with a comprehensive set of rules.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Comprehensive validation examplePolarsgame_revenueWARNING0.1ERROR0.25CRITICAL0.35\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    ^[A-Z]{12}[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    item_revenue\n    0.02\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19410.97\n    590.03\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    5\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    acquisition\n    google, facebook, organic, crosspromo, other_campaign\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19750.99\n    250.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    6\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    country\n    Mongolia, Germany\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17750.89\n    2250.11\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    session_duration\n    [10, 50]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    8\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    player_id, session_id, time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19780.99\n    220.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    9\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    11\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    12\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    13\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    14\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:21:07 UTC&lt; 1 s2025-05-23 02:21:07 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\"),\n        tbl_name=\"game_revenue\",\n        label=\"Comprehensive validation example\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"^[A-Z]{12}[0-9]{3}$\")        # STEP 1\n    .col_vals_gt(columns=\"session_duration\", value=5)                           # STEP 2\n    .col_vals_ge(columns=\"item_revenue\", value=0.02)                            # STEP 3\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])                    # STEP 4\n    .col_vals_in_set(                                                           # STEP 5\n        columns=\"acquisition\",\n        set=[\"google\", \"facebook\", \"organic\", \"crosspromo\", \"other_campaign\"]\n    )\n    .col_vals_not_in_set(columns=\"country\", set=[\"Mongolia\", \"Germany\"])        # STEP 6\n    .col_vals_between(                                                          # STEP 7\n        columns=\"session_duration\",\n        left=10, right=50,\n        pre = lambda df: df.select(pl.median(\"session_duration\"))\n    )\n    .rows_distinct(columns_subset=[\"player_id\", \"session_id\", \"time\"])          # STEP 8\n    .row_count_match(count=2000)                                                # STEP 9\n    .col_count_match(count=11)                                                  # STEP 10\n    .col_vals_not_null(columns=pb.starts_with(\"item\"))                          # STEPS 11-13\n    .col_exists(columns=\"start_day\")                                            # STEP 14\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    6\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:08:56+00:00\n    ad\n    ad_10sec\n    0.07\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    7\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:14:08+00:00\n    ad\n    ad_10sec\n    0.08\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    8\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:21:44+00:00\n    ad\n    ad_30sec\n    1.17\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    9\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:24:20+00:00\n    ad\n    ad_10sec\n    0.14\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    10\n    FXWUORGYNJAE271\n    FXWUORGYNJAE271-et7bs639\n    2015-01-01 15:17:18+00:00\n    2015-01-01 15:19:36+00:00\n    ad\n    ad_5sec\n    0.08\n    30.7\n    2015-01-01\n    organic\n    Canada\n  \n  \n    1991\n    VPNRYLMBKJGT925\n    VPNRYLMBKJGT925-vt26q9gb\n    2015-01-21 01:07:24+00:00\n    2015-01-21 01:26:12+00:00\n    ad\n    ad_survey\n    0.72\n    24.9\n    2015-01-21\n    other_campaign\n    Germany\n  \n  \n    1992\n    JVBZCPKXHFMU491\n    JVBZCPKXHFMU491-wvi6hs2t\n    2015-01-21 01:49:36+00:00\n    2015-01-21 01:53:36+00:00\n    iap\n    gold6\n    41.99\n    7.1\n    2015-01-07\n    organic\n    United States\n  \n  \n    1993\n    JVBZCPKXHFMU491\n    JVBZCPKXHFMU491-wvi6hs2t\n    2015-01-21 01:49:36+00:00\n    2015-01-21 01:55:42+00:00\n    iap\n    gems3\n    17.49\n    7.1\n    2015-01-07\n    organic\n    United States\n  \n  \n    1994\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:01:20+00:00\n    ad\n    ad_playable\n    1.116\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1995\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:14+00:00\n    ad\n    ad_15sec\n    0.225\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/05-step-report-column-check/index.html",
    "href": "demos/05-step-report-column-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Step Report: Column Data Checks\nA step report for column checks shows what went wrong.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Step reports for column data checksPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    c\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    \\d-[a-z]{3}-\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:21:00 UTC&lt; 1 s2025-05-23 02:21:00 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION c ≥ 44 / 13 TEST UNIT FAILURES IN COLUMN 5 EXTRACT OF ALL 4 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2 ✓ASSERTION b matches regex \\d-[a-z]{3}-\\d{3}13 TEST UNITS ALL PASSED IN COLUMN 4PREVIEW OF TARGET TABLE:\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        tbl_name=\"small_table\",\n        label=\"Step reports for column data checks\"\n    )\n    .col_vals_ge(columns=\"c\", value=4, na_pass=True)                # has failing test units\n    .col_vals_regex(columns=\"b\", pattern=r\"\\d-[a-z]{3}-\\d{3}\")      # no failing test units\n    .interrogate()\n)\n\nvalidation\nvalidation.get_step_report(i=1)\nvalidation.get_step_report(i=2)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/mutate-table-in-step/index.html",
    "href": "demos/mutate-table-in-step/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Mutate the Table in a Validation Step\nFor far more specialized validations, modify the table with the pre= argument before checking it.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:54Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    a\n    [3, 6]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_eq\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_eq()\n        \n        \n        \n    b_len\n    9\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:54 UTC&lt; 1 s2025-05-23 02:20:54 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\nimport narwhals as nw\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_between(\n        columns=\"a\",\n        left=3, right=6,\n        pre=lambda df: df.select(pl.median(\"a\"))    # Use a Polars expression to aggregate\n    )\n    .col_vals_eq(\n        columns=\"b_len\",\n        value=9,\n        pre=lambda dfn: dfn.with_columns(           # Use a Narwhals expression, identified\n            b_len=nw.col(\"b\").str.len_chars()       # by the 'dfn' here\n        )\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/checks-for-missing/index.html",
    "href": "demos/checks-for-missing/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Checks for Missing Values\nPerform validations that check whether missing/NA/Null values are present.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:48Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    d\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    5\n    \n        \n            \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    00.00\n    131.00\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-05-23 02:20:48 UTC&lt; 1 s2025-05-23 02:20:48 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_not_null(columns=\"a\")                  # expect no Null values\n    .col_vals_not_null(columns=\"b\")                  # \"\" \"\"\n    .col_vals_not_null(columns=\"c\")                  # \"\" \"\"\n    .col_vals_not_null(columns=\"d\")                  # \"\" \"\"\n    .col_vals_null(columns=\"a\")                      # expect all values to be Null\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/failure-thresholds/index.html",
    "href": "demos/failure-thresholds/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Set Failure Threshold Levels\nSet threshold levels to better gauge adverse data quality.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:44DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #AAAAAA\n    4\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19931.00\n    70.00\n    ●\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    5\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    end_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    ●\n    ●\n    ●\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:44 UTC&lt; 1 s2025-05-23 02:20:44 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(  # setting relative threshold defaults for all steps\n            warning=0.05,          # 5% failing test units: warning threshold (gray)\n            error=0.10,            # 10% failed test units: error threshold (yellow)\n            critical=0.15          # 15% failed test units: critical threshold (red)\n        ),\n    )\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=4,\n        thresholds=(5, 10, 20)     # setting absolute thresholds for *this* step (W, E, C)\n    )\n    .col_exists(columns=\"end_day\")\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/expect-no-duplicate-rows/index.html",
    "href": "demos/expect-no-duplicate-rows/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Expect No Duplicate Rows\nWe can check for duplicate rows in the table with rows_distinct().\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:37Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-05-23 02:20:37 UTC&lt; 1 s2025-05-23 02:20:37 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .rows_distinct()    # expect no duplicate rows\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/apply-checks-to-several-columns/index.html",
    "href": "demos/apply-checks-to-several-columns/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Apply Validation Rules to Multiple Columns\nCreate multiple validation steps by using a list of column names with columns=.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:31Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    a\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    c\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    d\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:31 UTC&lt; 1 s2025-05-23 02:20:31 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_ge(columns=[\"a\", \"c\", \"d\"], value=0)   # check values in 'a', 'c', and 'd'\n    .col_exists(columns=[\"date_time\", \"date\"])       # check for the existence of two columns\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/06-step-report-schema-check/index.html",
    "href": "demos/06-step-report-schema-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Step Report: Schema Check\nWhen a schema doesn’t match, a step report gives you the details.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Step report for a schema checkDuckDBsmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:24 UTC&lt; 1 s2025-05-23 02:20:24 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✗COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    date_time\n    timestamp\n    1\n    date_time\n    ✓\n    timestamp\n    ✓\n  \n  \n    2\n    date\n    date\n    2\n    dates\n    ✗\n    date\n    —\n  \n  \n    3\n    a\n    int64\n    3\n    a\n    ✓\n    int64\n    ✓\n  \n  \n    4\n    b\n    string\n    4\n    b\n    ✓\n    —\n    \n  \n  \n    5\n    c\n    int64\n    5\n    c\n    ✓\n    —\n    \n  \n  \n    6\n    d\n    float64\n    6\n    d\n    ✓\n    float64\n    ✓\n  \n  \n    7\n    e\n    boolean\n    7\n    e\n    ✓\n    bool | boolean\n    ✓\n  \n  \n    8\n    f\n    string\n    8\n    f\n    ✓\n    str\n    ✗\n  \n\n  \n  \n  \n    Supplied Column Schema:[('date_time', 'timestamp'), ('dates', 'date'), ('a', 'int64'), ('b',), ('c',), ('d', 'float64'), ('e', ['bool', 'boolean']), ('f', 'str')]\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\n# Create a schema for the target table (`small_table` as a DuckDB table)\nschema = pb.Schema(\n    columns=[\n        (\"date_time\", \"timestamp\"),     # this dtype doesn't match\n        (\"dates\", \"date\"),              # this column name doesn't match\n        (\"a\", \"int64\"),\n        (\"b\",),                         # omit dtype to not check for it\n        (\"c\",),                         # \"\"   \"\"   \"\"  \"\"\n        (\"d\", \"float64\"),\n        (\"e\", [\"bool\", \"boolean\"]),     # try several dtypes (second one matches)\n        (\"f\", \"str\"),                   # this dtype doesn't match\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform a schema check\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"duckdb\"),\n        tbl_name=\"small_table\",\n        label=\"Step report for a schema check\"\n    )\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\nvalidation.get_step_report(i=1)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows13Columns8\n  \n\n  \n  date_timetimestamp\n  datedate\n  aint64\n  bstring\n  cint64\n  dfloat64\n  eboolean\n  fstring\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    NULL\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    NULL\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/schema-check/index.html",
    "href": "demos/schema-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Check the Schema of a Table\nThe schema of a table can be flexibly defined with Schema and verified with col_schema_match().\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:18Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:18 UTC&lt; 1 s2025-05-23 02:20:18 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n        \"c\": [1.1, 2.2, 3.3, 4.4],\n    }\n)\n\n# Use the Schema class to define the column schema as loosely or rigorously as required\nschema = pb.Schema(\n    columns=[\n        (\"a\", \"String\"),          # Column 'a' has dtype 'String'\n        (\"b\", [\"Int\", \"Int64\"]),  # Column 'b' has dtype 'Int' or 'Int64'\n        (\"c\", )                   # Column 'c' follows 'b' but we don't specify a dtype here\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform the schema check\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Data validation made beautiful and powerful\nPointblank is a powerful, yet elegant data validation framework for Python that transforms how you ensure data quality. With its intuitive, chainable API, you can quickly validate your data against comprehensive quality checks and visualize results through stunning, interactive reports that make data issues immediately actionable.\nWhether you’re a data scientist, data engineer, or analyst, Pointblank helps you catch data quality issues before they impact your analyses or downstream systems."
  },
  {
    "objectID": "index.html#getting-started-in-30-seconds",
    "href": "index.html#getting-started-in-30-seconds",
    "title": "Pointblank",
    "section": "Getting Started in 30 Seconds",
    "text": "Getting Started in 30 Seconds\nimport pointblank as pb\n\nvalidation = (\n   pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n   .col_vals_gt(columns=\"d\", value=100)             # Validate values &gt; 100\n   .col_vals_le(columns=\"c\", value=5)               # Validate values &lt;= 5\n   .col_exists(columns=[\"date\", \"date_time\"])       # Check columns exist\n   .interrogate()                                   # Execute and collect results\n)\n\n# Get the validation report from the REPL with:\nvalidation.get_tabular_report().show()\n\n# From a notebook simply use:\nvalidation"
  },
  {
    "objectID": "index.html#real-world-example",
    "href": "index.html#real-world-example",
    "title": "Pointblank",
    "section": "Real-World Example",
    "text": "Real-World Example\nimport pointblank as pb\nimport polars as pl\n\n# Load your data\nsales_data = pl.read_csv(\"sales_data.csv\")\n\n# Create a comprehensive validation\nvalidation = (\n   pb.Validate(\n      data=sales_data,\n      tbl_name=\"sales_data\",           # Name of the table for reporting\n      label=\"Real-world example.\",     # Label for the validation, appears in reports\n      thresholds=(0.01, 0.02, 0.05),   # Set thresholds for warnings, errors, and critical issues\n      actions=pb.Actions(              # Define actions for any threshold exceedance\n         critical=\"Major data quality issue found in step {step} ({time}).\"\n      ),\n      final_actions=pb.FinalActions(   # Define final actions for the entire validation\n         pb.send_slack_notification(\n            webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n         )\n      ),\n      brief=True,                      # Add automatically-generated briefs for each step\n   )\n   .col_vals_between(            # Check numeric ranges with precision\n      columns=[\"price\", \"quantity\"],\n      left=0, right=1000\n   )\n   .col_vals_not_null(           # Ensure that columns ending with '_id' don't have null values\n      columns=pb.ends_with(\"_id\")\n   )\n   .col_vals_regex(              # Validate patterns with regex\n      columns=\"email\",\n      pattern=\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n   )\n   .col_vals_in_set(             # Check categorical values\n      columns=\"status\",\n      set=[\"pending\", \"shipped\", \"delivered\", \"returned\"]\n   )\n   .conjointly(                  # Combine multiple conditions\n      lambda df: pb.expr_col(\"revenue\") == pb.expr_col(\"price\") * pb.expr_col(\"quantity\"),\n      lambda df: pb.expr_col(\"tax\") &gt;= pb.expr_col(\"revenue\") * 0.05\n   )\n   .interrogate()\n)\nMajor data quality issue found in step 7 (2025-04-16 15:03:04.685612+00:00).\n# Get an HTML report you can share with your team\nvalidation.get_tabular_report().show(\"browser\")\n\n\n\n# Get a report of failing records from a specific step\nvalidation.get_step_report(i=3).show(\"browser\")  # Get failing records from step 3"
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "Pointblank",
    "section": "Join the Community",
    "text": "Join the Community\nWe’d love to hear from you! Connect with us:\n\nGitHub Issues for bug reports and feature requests\nDiscord server for discussions and help\nContributing guidelines if you’d like to help improve Pointblank"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Pointblank",
    "section": "Installation",
    "text": "Installation\nYou can install Pointblank using pip:\npip install pointblank\nYou can also install Pointblank from Conda-Forge by using:\nconda install conda-forge::pointblank\nIf you don’t have Polars or Pandas installed, you’ll need to install one of them to use Pointblank.\npip install \"pointblank[pl]\" # Install Pointblank with Polars\npip install \"pointblank[pd]\" # Install Pointblank with Pandas\nTo use Pointblank with DuckDB, MySQL, PostgreSQL, or SQLite, install Ibis with the appropriate backend:\npip install \"pointblank[duckdb]\"   # Install Pointblank with Ibis + DuckDB\npip install \"pointblank[mysql]\"    # Install Pointblank with Ibis + MySQL\npip install \"pointblank[postgres]\" # Install Pointblank with Ibis + PostgreSQL\npip install \"pointblank[sqlite]\"   # Install Pointblank with Ibis + SQLite"
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "Pointblank",
    "section": "Technical Details",
    "text": "Technical Details\nPointblank uses Narwhals to work with Polars and Pandas DataFrames, and integrates with Ibis for database and file format support. This architecture provides a consistent API for validating tabular data from various sources."
  },
  {
    "objectID": "index.html#contributing-to-pointblank",
    "href": "index.html#contributing-to-pointblank",
    "title": "Pointblank",
    "section": "Contributing to Pointblank",
    "text": "Contributing to Pointblank\nThere are many ways to contribute to the ongoing development of Pointblank. Some contributions can be simple (like fixing typos, improving documentation, filing issues for feature requests or problems, etc.) and others might take more time and care (like answering questions and submitting PRs with code changes). Just know that anything you can do to help would be very much appreciated!\nPlease read over the contributing guidelines for information on how to get started."
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "Pointblank",
    "section": "Roadmap",
    "text": "Roadmap\nWe’re actively working on enhancing Pointblank with:\n\nAdditional validation methods for comprehensive data quality checks\nAdvanced logging capabilities\nMessaging actions (Slack, email) for threshold exceedances\nLLM-powered validation suggestions and data dictionary generation\nJSON/YAML configuration for pipeline portability\nCLI utility for validation from the command line\nExpanded backend support and certification\nHigh-quality documentation and examples\n\nIf you have any ideas for features or improvements, don’t hesitate to share them with us! We are always looking for ways to make Pointblank better."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Pointblank",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that the Pointblank project is released with a contributor code of conduct. By participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Pointblank",
    "section": "📄 License",
    "text": "📄 License\nPointblank is licensed under the MIT license.\n© Posit Software, PBC."
  },
  {
    "objectID": "index.html#governance",
    "href": "index.html#governance",
    "title": "Pointblank",
    "section": "🏛️ Governance",
    "text": "🏛️ Governance\nThis project is primarily maintained by Rich Iannone. Other authors may occasionally assist with some of these duties."
  },
  {
    "objectID": "user-guide/extracts.html",
    "href": "user-guide/extracts.html",
    "title": "Data Extracts",
    "section": "",
    "text": "When validating data, identifying exactly which rows failed is critical for diagnosing and resolving data quality issues. This is where data extracts come in. Data extracts consist of target table rows containing at least one cell that failed validation. While the validation report provides an overview of pass/fail statistics, data extracts give you the actual problematic records for deeper investigation.\nThis article will cover:",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#the-validation-methods-that-work-with-data-extracts",
    "href": "user-guide/extracts.html#the-validation-methods-that-work-with-data-extracts",
    "title": "Data Extracts",
    "section": "The Validation Methods that Work with Data Extracts",
    "text": "The Validation Methods that Work with Data Extracts\nThe following validation methods are row-based and will have rows extracted when there are failing test units:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\nrows_distinct()\nrows_complete()\nconjointly()\n\nNote that some validation methods like col_exists() or col_schema_match() don’t generate row-based data extracts because they validate structural aspects of the data rather than row values.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#accessing-data-extracts",
    "href": "user-guide/extracts.html#accessing-data-extracts",
    "title": "Data Extracts",
    "section": "Accessing Data Extracts",
    "text": "Accessing Data Extracts\nThere are three primary ways to access data extracts in Pointblank:\n\nthe CSV buttons in validation reports\nthrough the get_data_extracts() method\ninspecting a subset of failed rows in step reports\n\nLet’s explore each approach using examples.\n\nCSV Data from Validation Reports\nData extracts are embedded within validation report tables. Let’s look at an example, using the small_table dataset, where data extracts are collected in a single validation step due to failing test units:\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_lt(\n        columns=\"d\",\n        value=3000\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    3000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe single validation step checks whether values in d are less than 3000. Within that column, values range from 108.34 to 9999.99 so it makes sense that we can see 4 failing test units in the FAIL column.\nIf you look at the far right of the validation report you’ll find there’s a CSV button. Pressing it initiates the download of a CSV file, and that file contains the data extract for this validation step. The CSV button only appears when:\n\nthere is a non-zero number of failing test units\nthe validation step is based on the use of a row-based validation method (the methods outlined above)\n\nAccess to CSV data for the row-based errors is useful when the validation report is shared with other data quality stakeholders, since it is easily accessible and doesn’t require further use of Pointblank. The stakeholder can simply open the downloaded CSV in their preferred spreadsheet software, import it into a different analysis environment like R or Julia, or process it with any tool that supports CSV files. This cross-platform compatibility makes the CSV export particularly valuable in mixed-language data teams where different members might be working with different tools.\n\n\nget_data_extracts()\nFor programmatic access to data extracts, Pointblank provides the get_data_extracts() method. This allows you to work with extract data directly in your Python workflow:\n\n# Get data extracts from step 1\nextract_1 = validation.get_data_extracts(i=1, frame=True)\n\nextract_1\n\n\nshape: (4, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr12016-01-04 11:00:002016-01-042\"1-bcd-345\"33423.29true\"high\"22016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"42016-01-06 17:23:002016-01-062\"5-jdo-903\"null3892.4false\"mid\"62016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"\n\n\nThe extracted table is of the same type (a Polars DataFrame) as the target table. Previously we used load_dataset() with the tbl_type=\"polars\" option to fetch the dataset in that form.\nNote these important details about using get_data_extracts():\n\nthe parameter i=1 corresponds to the step number shown in the validation report (1-indexed, not 0-indexed)\nsetting frame=True returns the data as a DataFrame rather than a dictionary (only works when i is a single integer)\nthe extract includes all columns from the original data, not just the column being validated\nan additional _row_num_ column is added to identify the original row positions\n\n\n\nStep Reports\nStep reports provide another way to access and visualize failing data. When you generate a step report for a validation step that has failing rows, those failing rows are displayed directly in the report:\n\n# Get a step report for the first validation step\nstep_report = validation.get_step_report(i=1)\n\nstep_report\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION d &lt; 30004 / 13 TEST UNIT FAILURES IN COLUMN 6 EXTRACT OF ALL 4 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n\n\n\n\n\n\n        \n\n\nStep reports offer several advantages for working with data extracts as they:\n\nprovide immediate visual context by highlighting the specific column being validated\nformat the data for better readability, especially useful when sharing results with colleagues\ninclude additional metadata about the validation step and failure statistics\n\nFor steps with many failures, you can customize how many rows to display:\n\n# Limit to just 2 rows of failing data\nlimited_report = validation.get_step_report(i=1, limit=2)\n\nlimited_report\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION d &lt; 30004 / 13 TEST UNIT FAILURES IN COLUMN 6 EXTRACT OF FIRST 2 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n\n\n\n\n\n\n        \n\n\nStep reports are particularly valuable when you want to quickly inspect the failing data without extracting it into a separate DataFrame. They provide a bridge between the high-level validation report and the detailed data extracts.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#viewing-data-extracts-with-preview",
    "href": "user-guide/extracts.html#viewing-data-extracts-with-preview",
    "title": "Data Extracts",
    "section": "Viewing Data Extracts with preview()",
    "text": "Viewing Data Extracts with preview()\nTo get a consistent HTML representation of any data extract (regardless of the table type), we can use the preview() function:\n\npb.preview(data=extract_1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows4Columns9\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n\n\n\n\n\n\n        \n\n\nThe view is optimized for readability, with column names and data types displayed in a compact format. Notice that the _row_num_ column is now part of the table stub and doesn’t steal focus from the table’s original columns.\nThe preview() function is designed to provide the head and tail (5 rows each) of the table so very large extracts won’t overflow the display.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#working-with-multiple-validation-steps",
    "href": "user-guide/extracts.html#working-with-multiple-validation-steps",
    "title": "Data Extracts",
    "section": "Working with Multiple Validation Steps",
    "text": "Working with Multiple Validation Steps\nWhen validating data with multiple steps, you can extract failing rows from any step or combine extracts from multiple steps:\n\n# Create a validation with multiple steps\nmulti_validation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_gt(columns=\"a\", value=3)                                  # Step 1\n    .col_vals_lt(columns=\"d\", value=3000)                               # Step 2\n    .col_vals_regex(columns=\"b\", pattern=\"^[0-9]-[a-z]{3}-[0-9]{3}$\")   # Step 3\n    .interrogate()\n)\n\nmulti_validation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    3000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^[0-9]-[a-z]{3}-[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\n\nExtracting Data from a Specific Step\nYou can access extracts from any specific validation step:\n\n# Get extracts from step 2 (`d &lt; 3000` validation)\nless_than_failures = multi_validation.get_data_extracts(i=2, frame=True)\n\nless_than_failures\n\n\nshape: (4, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr12016-01-04 11:00:002016-01-042\"1-bcd-345\"33423.29true\"high\"22016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"42016-01-06 17:23:002016-01-062\"5-jdo-903\"null3892.4false\"mid\"62016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"\n\n\nUsing frame=True means that returned value will be a DataFrame (not a dictionary that contains a single DataFrame).\nIf a step has no failing rows, an empty DataFrame will be returned:\n\n# Get extracts from step 3 (regex check)\nregex_failures = multi_validation.get_data_extracts(i=3, frame=True)\n\nregex_failures\n\n\nshape: (0, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr\n\n\n\n\nGetting All Extracts at Once\nTo retrieve extracts from all steps with failures in one command:\n\n# Get all extracts ()\nall_extracts = multi_validation.get_data_extracts()\n\n# Display the step numbers that have extracts\nprint(f\"Steps with data extracts: {list(all_extracts.keys())}\")\n\nSteps with data extracts: [1, 2, 3]\n\n\nA dictionary of DataFrames is returned and only steps with failures will appear in this dictionary.\n\n\nGetting Specific Extracts\nYou can also retrieve data extracts from several specified steps as a dictionary:\n\n# Get extracts from steps 1 and 2 as a dictionary\nextract_dict = multi_validation.get_data_extracts(i=[1, 2])\n\n# The keys are the step numbers\nprint(f\"Dictionary keys: {list(extract_dict.keys())}\")\n\n# Get the number of failing rows in each extract\nfor step, extract in extract_dict.items():\n    print(f\"Step {step}: {len(extract)} failing rows\")\n\nDictionary keys: [1, 2]\nStep 1: 7 failing rows\nStep 2: 4 failing rows\n\n\nNote that frame=True cannot be used when retrieving multiple extracts.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#applications-of-data-extracts",
    "href": "user-guide/extracts.html#applications-of-data-extracts",
    "title": "Data Extracts",
    "section": "Applications of Data Extracts",
    "text": "Applications of Data Extracts\nOnce you have extracted the failing data, there are numerous ways to analyze and use this information to improve data quality. Let’s explore some practical applications.\n\nFinding Patterns Across Validation Steps\nYou can analyze patterns across different validation steps by combining extracts:\n\n# Get a consolidated view of all rows that failed any validation\nall_failure_rows = set()\nfor step, extract in all_extracts.items():\n    if len(extract) &gt; 0:\n        all_failure_rows.update(extract[\"_row_num_\"])\n\nprint(f\"Total unique rows with failures: {len(all_failure_rows)}\")\nprint(f\"Row numbers with failures: {sorted(all_failure_rows)}\")\n\nTotal unique rows with failures: 8\nRow numbers with failures: [1, 2, 4, 6, 9, 10, 12, 13]\n\n\n\n\nIdentifying Rows with Multiple Failures\nYou might want to find rows that failed multiple validation checks, as these often represent more serious data quality issues:\n\n# Get row numbers from each extract\nstep1_rows = set(multi_validation.get_data_extracts(i=1, frame=True)[\"_row_num_\"])\nstep2_rows = set(multi_validation.get_data_extracts(i=2, frame=True)[\"_row_num_\"])\n\n# Find rows that failed both validations\ncommon_failures = step1_rows.intersection(step2_rows)\nprint(f\"Rows failing both step 1 and step 2: {common_failures}\")\n\nRows failing both step 1 and step 2: {1, 2, 4}\n\n\n\n\nStatistical Analysis of Failing Values\nOnce you have data extracts, you can perform statistical analysis to identify patterns in the failing data:\n\n# Get extracts from step 2\nd_value_failures = multi_validation.get_data_extracts(i=2, frame=True)\n\n# Basic statistical analysis of the failing values\nif len(d_value_failures) &gt; 0:\n    print(f\"Min failing value: {d_value_failures['d'].min()}\")\n    print(f\"Max failing value: {d_value_failures['d'].max()}\")\n    print(f\"Mean failing value: {d_value_failures['d'].mean()}\")\n\nMin failing value: 3291.03\nMax failing value: 9999.99\nMean failing value: 5151.6775\n\n\nThese analysis techniques help you thoroughly investigate data quality issues by examining failing data from multiple perspectives. Rather than treating failures as isolated incidents, you can identify patterns that might indicate systematic problems in your data pipeline.\n\n\nDetailed Analysis with col_summary_tbl()\nFor a more comprehensive view of the statistical properties of your extract data, you can use the col_summary_tbl() function:\n\n# Get extracts from step 2\nd_value_failures = multi_validation.get_data_extracts(i=2, frame=True)\n\n# Generate a comprehensive statistical summary of the failing data\npb.col_summary_tbl(d_value_failures)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows4Columns9\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    _row_num_UInt32\n    0 0.00\n    4 1.00\n    3.25\n    2.22\n    1.00\n    1.15\n    1.75\n    3.00\n    4.50\n    5.70\n    6.00\n    2.75\n  \n  \n    2\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    date_timeDatetime(time_unit='us', time_zone=None)\n    0 0.00\n    4 1.00\n    —\n    —\n     2016-01-04 00:32:00 – 2016-01-11 06:15:00\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    3\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dateDate\n    0 0.00\n    3 0.75\n    —\n    —\n     2016-01-04 – 2016-01-11\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    4\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    aInt64\n    0 0.00\n    3 0.75\n    2.75\n    0.96\n    2.00\n    2.00\n    2.00\n    2.50\n    3.25\n    3.85\n    4.00\n    1.25\n  \n  \n    5\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    bString\n    0 0.00\n    4 1.00\n    9.00SL\n    0.00SL\n    9SL\n    —\n    —\n    9SL\n    —\n    —\n    9SL\n    —\n  \n  \n    6\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    cInt64\n    1 0.25\n    3 0.75\n    5.00\n    2.65\n    3.00\n    3.10\n    3.50\n    4.00\n    6.00\n    7.60\n    8.00\n    2.50\n  \n  \n    7\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dFloat64\n    0 0.00\n    4 1.00\n    5152\n    3242\n    3291\n    3311\n    3390\n    3658\n    5419\n    9084\n    10000\n    2029\n  \n  \n    8\n    \n    boolean\n    \n        \n            \n            \n                \n            \n            \n                \n            \n            \n        \n    \n\n    eBoolean\n    0 0.00\n    T 0.75F 0.25\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n  \n    9\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    fString\n    0 0.00\n    3 0.75\n    3.25SL\n    0.50SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    4SL\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis statistical overview provides:\n\nCount of values (including missing values)\nType information for each column\nDistribution metrics like min, max, mean, and quartiles for numeric columns\nFrequency of common values for categorical columns\nMissing value counts and proportions\n\nUsing col_summary_tbl() on data extracts lets you quickly understand the characteristics of failing data without writing custom analysis code. This approach is particularly valuable when:\n\nYou need to understand the statistical properties of failing records\nYou want to compare distributions of failing vs passing data\nYou’re looking for anomalies or unexpected patterns within the failing rows\n\nFor example, if values failing a validation check are concentrated at certain quantiles or have an unusual distribution shape, this might indicate a systematic data collection or processing issue rather than random errors.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#using-extracts-for-data-quality-improvement",
    "href": "user-guide/extracts.html#using-extracts-for-data-quality-improvement",
    "title": "Data Extracts",
    "section": "Using Extracts for Data Quality Improvement",
    "text": "Using Extracts for Data Quality Improvement\nData extracts are especially valuable for:\n\nRoot Cause Analysis: examining the full context of failing rows to understand why they failed\nData Cleaning: creating targeted cleanup scripts that focus only on problematic records\nFeedback Loops: sharing specific examples with data providers to improve upstream quality\nPattern Recognition: identifying systemic issues by analyzing groups of failing records\n\nHere’s an example of using extracts to create a corrective action plan:\n\nimport polars as pl\n\n# Create a new sample of an extract DF\nsample_extract = pl.DataFrame({\n    \"id\": range(1, 11),\n    \"value\": [3500, 4200, 3800, 9800, 5500, 7200, 8300, 4100, 7600, 3200],\n    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\", \"A\", \"B\"],\n    \"region\": [\n        \"South\", \"South\", \"North\", \"East\", \"South\",\n        \"South\", \"East\", \"South\", \"West\", \"South\"\n    ]\n})\n\n# Identify which regions have the most failures\nregion_counts = (\n    sample_extract\n    .group_by(\"region\")\n    .agg(pl.len().alias(\"failure_count\"))\n    .sort(\"failure_count\", descending=True)\n)\n\nregion_counts\n\n\nshape: (4, 2)regionfailure_countstru32\"South\"6\"East\"2\"North\"1\"West\"1\n\n\nAnalysis shows that 6 out of 10 failing records (60%) are from the \"South\" region, making it the highest priority area for data quality investigation. This suggests a potential systemic issue with data collection or processing in that specific region.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#best-practices-for-working-with-data-extracts",
    "href": "user-guide/extracts.html#best-practices-for-working-with-data-extracts",
    "title": "Data Extracts",
    "section": "Best Practices for Working with Data Extracts",
    "text": "Best Practices for Working with Data Extracts\nWhen incorporating data extracts into your data quality workflow:\n\nUse extracts for investigation, not just reporting: the real value is in the insights you gain from analyzing the problematic data\nCombine with other Pointblank features: data extracts work well with step reports and can inform threshold settings for future validations\nConsider sampling for very large datasets: if your extracts contain thousands of rows, focus your investigation on a representative sample\nLook beyond individual validation steps: cross-reference extracts from different steps to identify complex issues that span multiple validation rules\nDocument patterns in failing data: record and share insights about common failure modes to build organizational knowledge about data quality issues.\n\nBy integrating these practices into your data validation workflow, you’ll transform data extracts from simple error lists into powerful diagnostic tools. The most successful data quality initiatives treat extracts as the starting point for investigation rather than the end result of validation. When systematically analyzed and documented, patterns in failing data can reveal underlying issues in data systems, collection methods, or business processes that might otherwise remain hidden. Remember that the ultimate goal isn’t just to identify problematic records, but to use that information to implement targeted improvements that prevent similar issues from occurring in the future.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#conclusion",
    "href": "user-guide/extracts.html#conclusion",
    "title": "Data Extracts",
    "section": "Conclusion",
    "text": "Conclusion\nData extracts bridge the gap between high-level validation statistics and the detailed context needed to fix data quality issues. By providing access to the actual failing records, Pointblank enables you to:\n\npinpoint exactly which data points caused validation failures\nunderstand the full context around problematic values\ndevelop targeted strategies for data cleanup and quality improvement\ncommunicate specific examples to stakeholders\n\nWhether you’re accessing extracts through CSV downloads, the get_data_extracts() method, or step reports, this feature provides the detail needed to move from identifying problems to implementing solutions.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html",
    "href": "user-guide/step-reports.html",
    "title": "Step Reports",
    "section": "",
    "text": "While validation reports provide a comprehensive overview of all validation steps, sometimes you need to focus on a specific validation step in greater detail. This is where step reports come in. A step report is a detailed examination of a single validation step, providing in-depth information about the test units that were validated and their pass/fail status.\nStep reports are especially useful when debugging validation failures, investigating problematic data, or communicating detailed findings to colleagues who are responsible for specific data quality issues.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#creating-a-step-report",
    "href": "user-guide/step-reports.html#creating-a-step-report",
    "title": "Step Reports",
    "section": "Creating a Step Report",
    "text": "Creating a Step Report\nTo create a step report, you first need to run a validation and then use the get_step_report() method, specifying which validation step you want to examine:\n\nimport pointblank as pb\nimport polars as pl\n\n# Sample data as a Polars DataFrame\ndata = pl.DataFrame({\n    \"id\": range(1, 11),\n    \"value\": [10, 20, 3, 35, 50, 2, 70, 8, 20, 4],\n    \"category\": [\"A\", \"B\", \"C\", \"A\", \"D\", \"F\", \"A\", \"E\", \"H\", \"G\"],\n    \"ratio\": [0.5, 0.7, 0.3, 1.2, 0.8, 0.9, 0.4, 1.5, 0.6, 0.2],\n    \"status\": [\"active\", \"active\", \"inactive\", \"active\", \"inactive\",\n               \"active\", \"inactive\", \"active\", \"active\", \"inactive\"]\n})\n\n# Create a validation\nvalidation = (\n    pb.Validate(data=data, tbl_name=\"example_data\")\n    .col_vals_gt(\n        columns=\"value\",\n        value=10\n    )\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"]\n    )\n    .interrogate()\n)\n\n# Get step report for the second validation step (i=2)\nstep_report = validation.get_step_report(i=2)\n\nstep_report\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION category ∈ {A, B, C}5 / 10 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF ALL 5 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  valueInt64\n  categoryString\n  ratioFloat64\n  statusString\n\n\n\n  \n    5\n    5\n    50\n    D\n    0.8\n    inactive\n  \n  \n    6\n    6\n    2\n    F\n    0.9\n    active\n  \n  \n    8\n    8\n    8\n    E\n    1.5\n    active\n  \n  \n    9\n    9\n    20\n    H\n    0.6\n    active\n  \n  \n    10\n    10\n    4\n    G\n    0.2\n    inactive\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we first create and interrogate a validation object with two steps. We then generate a step report for the second validation step (i=2), which checks if the values in the category column are in the set [\"A\", \"B\", \"C\"].\nNote that step numbers in Pointblank start at 1, matching what you see in the validation report’s STEP column (i.e., not 0-based indexing). So the first step is referred to with i=1, the second step with i=2, and so on.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#understanding-step-report-components",
    "href": "user-guide/step-reports.html#understanding-step-report-components",
    "title": "Step Reports",
    "section": "Understanding Step Report Components",
    "text": "Understanding Step Report Components\nA step report consists of several key components that provide detailed information about the validation step:\n\nHeader: displays the validation step number, type of validation, and a brief description\nTable Body: presents either the failing rows, a sample of completely passing data, or an expected/actual comparison (for a col_schema_match() step)\n\nThe step report table highlights passing and failing rows, making it easy to identify problematic data points. This is especially useful for diagnosing issues when dealing with large datasets.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#different-types-of-step-reports",
    "href": "user-guide/step-reports.html#different-types-of-step-reports",
    "title": "Step Reports",
    "section": "Different Types of Step Reports",
    "text": "Different Types of Step Reports\nIt’s important to note that step reports vary in appearance and structure depending on the type of validation method used:\n\nValue-based validations (like col_vals_gt(), col_vals_in_set()): show individual rows that failed validation\nUniqueness checks (rows_distinct()): group together the duplicate records in order of appearance\nSchema validations (col_schema_match()): display column-level information about expected vs. actual data types\n\nAdditionally, step reports for value-based validations and uniqueness checks operate in two distinct modes:\n\nWhen errors are present: The report shows only the failing rows and, for value-based validations, clearly highlights the column under study\nWhen no errors exist: The report header clearly indicates success, and a sample of the data is shown (along with the studied column highlighted, for value-based validations)\n\nThis variation in reporting style allows step reports to effectively communicate the specific type of validation being performed and display relevant information in the most appropriate format. When you’re working with different validation types, expect to see different step report layouts optimized for each context.\n\nValue-Based Validation Step Reports\nValue-based step reports focus on showing individual rows where values in the target column failed the validation check. These reports highlight the specific column being validated and clearly display which values violated the condition.\n\n# Create sample data with some validation failures\ndata = pl.DataFrame({\n    \"id\": range(1, 8),\n    \"value\": [120, 85, 47, 210, 30, 10, 5],\n    \"category\": [\"A\", \"B\", \"C\", \"A\", \"D\", \"B\", \"E\"]\n})\n\n# Create a validation with a value-based check\nvalidation_values = (\n    pb.Validate(data=data, tbl_name=\"sales_data\")\n    .col_vals_gt(\n        columns=\"value\",\n        value=50,\n        brief=\"Sales values should exceed $50\"\n    )\n    .interrogate()\n)\n\n# Display the step report for the value-based validation\nvalidation_values.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION value &gt; 504 / 7 TEST UNIT FAILURES IN COLUMN 2 EXTRACT OF ALL 4 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  valueInt64\n  categoryString\n\n\n\n  \n    3\n    3\n    47\n    C\n  \n  \n    5\n    5\n    30\n    D\n  \n  \n    6\n    6\n    10\n    B\n  \n  \n    7\n    7\n    5\n    E\n  \n\n\n\n\n\n\n        \n\n\nThis report clearly identifies which rows contain values that don’t meet our threshold, making it easy to investigate these specific data points.\n\n\nUniqueness Validation Step Reports\nUniqueness checks produce a different type of step report that groups duplicate records together. This format makes it easy to identify patterns in duplicate data.\n\n# Create sample data with some duplicate rows based on the combination of columns\ndata = pl.DataFrame({\n    \"customer_id\": [101, 102, 103, 101, 104, 105, 102],\n    \"order_date\": [\"2023-01-15\", \"2023-01-16\", \"2023-01-16\",\n                   \"2023-01-15\", \"2023-01-17\", \"2023-01-18\", \"2023-01-19\"],\n    \"product\": [\"Laptop\", \"Phone\", \"Tablet\", \"Laptop\",\n                \"Monitor\", \"Keyboard\", \"Headphones\"]\n})\n\n# Create a validation checking for unique customer-product combinations\nvalidation_duplicates = (\n    pb.Validate(data=data, tbl_name=\"order_data\")\n    .rows_distinct(\n        columns_subset=[\"customer_id\", \"product\"],\n        brief=\"Customer should not order the same product twice\"\n    )\n    .interrogate()\n)\n\n# Display the step report for the uniqueness validation\nvalidation_duplicates.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1Rows are distinct across a subset of columns2 / 7 TEST UNIT FAILURESEXTRACT OF ALL 2 ROWS:\n  \n\n  \n  customer_idInt64\n  productString\n\n\n\n  \n    1\n    101\n    Laptop\n  \n  \n    4\n    101\n    Laptop\n  \n\n\n\n\n\n\n        \n\n\nThe report organizes duplicate records together, making it easy to see which combinations are repeated and how many times they appear.\n\n\nSchema Validation Step Reports\nSchema validation step reports have a completely different structure, comparing expected versus actual column data types and presence.\n\nschema = pb.Schema(\n    columns=[\n        (\"date_time\", \"timestamp\"),\n        (\"dates\", \"date\"),\n        (\"a\", \"int64\"),\n        (\"b\",),\n        (\"c\",),\n        (\"d\", \"float64\"),\n        (\"e\", [\"bool\", \"boolean\"]),\n        (\"f\", \"str\"),\n    ]\n)\n\nvalidation_schema = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"duckdb\"),\n        tbl_name=\"small_table\",\n        label=\"Step report for a schema check\"\n    )\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\n# Display the step report for the schema validation\nvalidation_schema.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✗COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    date_time\n    timestamp\n    1\n    date_time\n    ✓\n    timestamp\n    ✓\n  \n  \n    2\n    date\n    date\n    2\n    dates\n    ✗\n    date\n    —\n  \n  \n    3\n    a\n    int64\n    3\n    a\n    ✓\n    int64\n    ✓\n  \n  \n    4\n    b\n    string\n    4\n    b\n    ✓\n    —\n    \n  \n  \n    5\n    c\n    int64\n    5\n    c\n    ✓\n    —\n    \n  \n  \n    6\n    d\n    float64\n    6\n    d\n    ✓\n    float64\n    ✓\n  \n  \n    7\n    e\n    boolean\n    7\n    e\n    ✓\n    bool | boolean\n    ✓\n  \n  \n    8\n    f\n    string\n    8\n    f\n    ✓\n    str\n    ✗\n  \n\n  \n  \n  \n    Supplied Column Schema:[('date_time', 'timestamp'), ('dates', 'date'), ('a', 'int64'), ('b',), ('c',), ('d', 'float64'), ('e', ['bool', 'boolean']), ('f', 'str')]\n  \n\n\n\n\n\n\n        \n\n\nThis report style focuses on comparing the expected schema against the actual table structure, highlighting mismatches in data types or missing/extra columns. The table format makes it easy to see exactly where the schema expectations differ from reality.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#customizing-step-reports",
    "href": "user-guide/step-reports.html#customizing-step-reports",
    "title": "Step Reports",
    "section": "Customizing Step Reports",
    "text": "Customizing Step Reports\nStep reports can be customized with several parameters to better focus your analysis and tailor the output to your specific needs. The get_step_report() method offers multiple customization options to help you create more effective reports.\nWhen a dataset has many columns, you might want to focus on just those relevant to your analysis. You can create a step report containing only a subset of the columns in the target table:\n\nvalidation.get_step_report(\n    i=2,\n    columns_subset=[\"id\", \"category\", \"status\"]  # Only show these columns\n)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION category ∈ {A, B, C}5 / 10 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF ALL 5 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  categoryString\n  statusString\n\n\n\n  \n    5\n    5\n    D\n    inactive\n  \n  \n    6\n    6\n    F\n    active\n  \n  \n    8\n    8\n    E\n    active\n  \n  \n    9\n    9\n    H\n    active\n  \n  \n    10\n    10\n    G\n    inactive\n  \n\n\n\n\n\n\n        \n\n\nThis approach makes step reports much easier to interpret by highlighting just the essential columns that help understand the validation failures.\nFor large datasets with many failing rows, you might want to use limit= to set a cap on the number of rows shown in the report:\n\nvalidation.get_step_report(\n    i=2,\n    limit=2  # Only show up to 2 failing rows\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION category ∈ {A, B, C}5 / 10 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF FIRST 2 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  valueInt64\n  categoryString\n  ratioFloat64\n  statusString\n\n\n\n  \n    5\n    5\n    50\n    D\n    0.8\n    inactive\n  \n  \n    6\n    6\n    2\n    F\n    0.9\n    active\n  \n\n\n\n\n\n\n        \n\n\nThe report header can also be extensively customized to provide more specific context. You can replace the default header with plain text or Markdown formatting:\n\nvalidation.get_step_report(\n    i=2,\n    header=\"Category Values Validation: *Critical Analysis*\"\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Category Values Validation: Critical Analysis\n  \n\n  \n  idInt64\n  valueInt64\n  categoryString\n  ratioFloat64\n  statusString\n\n\n\n  \n    5\n    5\n    50\n    D\n    0.8\n    inactive\n  \n  \n    6\n    6\n    2\n    F\n    0.9\n    active\n  \n  \n    8\n    8\n    8\n    E\n    1.5\n    active\n  \n  \n    9\n    9\n    20\n    H\n    0.6\n    active\n  \n  \n    10\n    10\n    4\n    G\n    0.2\n    inactive\n  \n\n\n\n\n\n\n        \n\n\nFor more advanced header customization, you can use the templating system with the {title} and {details} elements to retain parts of the default header while adding your own content. The {title} template is the default title whereas {details} provides information on the assertion, number of failures, etc. Let’s move away from the default template of {title}{details} and provide a custom title to go with the details text:\n\nvalidation.get_step_report(\n    i=2,\n    header=\"Custom Category Validation Report {details}\"\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Custom Category Validation Report ASSERTION category ∈ {A, B, C}5 / 10 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF ALL 5 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  valueInt64\n  categoryString\n  ratioFloat64\n  statusString\n\n\n\n  \n    5\n    5\n    50\n    D\n    0.8\n    inactive\n  \n  \n    6\n    6\n    2\n    F\n    0.9\n    active\n  \n  \n    8\n    8\n    8\n    E\n    1.5\n    active\n  \n  \n    9\n    9\n    20\n    H\n    0.6\n    active\n  \n  \n    10\n    10\n    4\n    G\n    0.2\n    inactive\n  \n\n\n\n\n\n\n        \n\n\nWe can keep {title} and {details} and add some more context in between the two:\n\nvalidation.get_step_report(\n    i=2,\n    header=(\n        \"{title}&lt;br&gt;\"\n        \"&lt;span style='font-size: 0.75em;'&gt;\"\n        \"This validation is critical for our data quality standards.\"\n        \"&lt;/span&gt;&lt;br&gt;\"\n        \"{details}\"\n    )\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2This validation is critical for our data quality standards.ASSERTION category ∈ {A, B, C}5 / 10 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF ALL 5 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  valueInt64\n  categoryString\n  ratioFloat64\n  statusString\n\n\n\n  \n    5\n    5\n    50\n    D\n    0.8\n    inactive\n  \n  \n    6\n    6\n    2\n    F\n    0.9\n    active\n  \n  \n    8\n    8\n    8\n    E\n    1.5\n    active\n  \n  \n    9\n    9\n    20\n    H\n    0.6\n    active\n  \n  \n    10\n    10\n    4\n    G\n    0.2\n    inactive\n  \n\n\n\n\n\n\n        \n\n\nYou could always use more HTML and CSS to do a lot of customization:\n\nvalidation.get_step_report(\n    i=2,\n    header=(\n        \"VALIDATION SUMMARY\\n\\n{details}\\n\\n\"\n        \"&lt;hr style='color: lightblue;'&gt;\"\n        \"&lt;div style='font-size: smaller; padding-bottom: 5px; text-transform: uppercase'&gt;\"\n        \"{title}\"\n        \"&lt;/div&gt;\"\n    )\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    VALIDATION SUMMARY\nASSERTION category ∈ {A, B, C}5 / 10 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF ALL 5 ROWS (WITH TEST UNIT FAILURES IN RED):\nReport for Validation Step 2\n\n  \n\n  \n  idInt64\n  valueInt64\n  categoryString\n  ratioFloat64\n  statusString\n\n\n\n  \n    5\n    5\n    50\n    D\n    0.8\n    inactive\n  \n  \n    6\n    6\n    2\n    F\n    0.9\n    active\n  \n  \n    8\n    8\n    8\n    E\n    1.5\n    active\n  \n  \n    9\n    9\n    20\n    H\n    0.6\n    active\n  \n  \n    10\n    10\n    4\n    G\n    0.2\n    inactive\n  \n\n\n\n\n\n\n        \n\n\nIf you prefer no header at all, simply set header=None:\n\nvalidation.get_step_report(\n    i=2,\n    header=None\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  idInt64\n  valueInt64\n  categoryString\n  ratioFloat64\n  statusString\n\n\n\n  \n    5\n    5\n    50\n    D\n    0.8\n    inactive\n  \n  \n    6\n    6\n    2\n    F\n    0.9\n    active\n  \n  \n    8\n    8\n    8\n    E\n    1.5\n    active\n  \n  \n    9\n    9\n    20\n    H\n    0.6\n    active\n  \n  \n    10\n    10\n    4\n    G\n    0.2\n    inactive\n  \n\n\n\n\n\n\n        \n\n\nThese customization options can be combined to create highly focused reports tailored to specific needs:\n\nvalidation.get_step_report(\n    i=2,\n    columns_subset=[\"id\", \"category\"],\n    header=\"*Category Validation:* Top Issues\",\n    limit=2\n)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n  \n    Category Validation: Top Issues\n  \n\n  \n  idInt64\n  categoryString\n\n\n\n  \n    5\n    5\n    D\n  \n  \n    6\n    6\n    F\n  \n\n\n\n\n\n\n        \n\n\nThrough these customization options, you can craft step reports that effectively communicate the most important information to different audiences. Technical teams might benefit from seeing all columns but with a limited number of examples. Business stakeholders might prefer a focused view with only the most relevant columns. For documentation purposes, custom headers provide important context about what’s being validated.\nRemember that customizing your step reports is about more than aesthetics: it’s about making complex validation information more accessible and actionable for all stakeholders involved in data quality.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#using-step-reports-for-data-investigation",
    "href": "user-guide/step-reports.html#using-step-reports-for-data-investigation",
    "title": "Step Reports",
    "section": "Using Step Reports for Data Investigation",
    "text": "Using Step Reports for Data Investigation\nStep reports can be powerful tools for investigating data quality issues. Let’s look at a more complex example:\n\n# Create a more complex dataset with multiple issues\ncomplex_data = pl.DataFrame({\n    \"id\": range(1, 11),\n    \"value\": [10, 20, 3, 40, 50, 2, 70, 80, 90, 7],\n    \"ratio\": [0.1, 0.2, 0.3, 1.4, 0.5, 0.6, 0.7, 0.8, 1.2, 0.9],\n    \"category\": [\"A\", \"B\", \"C\", \"A\", \"D\", \"B\", \"A\", \"C\", \"B\", \"E\"]\n})\n\n# Create a validation with multiple steps\nvalidation_complex = (\n    pb.Validate(data=complex_data, tbl_name=\"complex_data\")\n    .col_vals_gt(columns=\"value\", value=10)\n    .col_vals_le(columns=\"ratio\", value=1.0)\n    .col_vals_in_set(columns=\"category\", set=[\"A\", \"B\", \"C\"])\n    .interrogate()\n)\n\n# Get step report for the ratio validation (step 2)\nratio_report = validation_complex.get_step_report(i=2)\n\nratio_report\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION ratio ≤ 1.02 / 10 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF ALL 2 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  valueInt64\n  ratioFloat64\n  categoryString\n\n\n\n  \n    4\n    4\n    40\n    1.4\n    A\n  \n  \n    9\n    9\n    90\n    1.2\n    B\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we’re investigating issues with the ratio column by generating a step report specifically for that validation step. The step report shows exactly which rows have values that exceed our maximum threshold of 1.0.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#combining-step-reports-with-extracts",
    "href": "user-guide/step-reports.html#combining-step-reports-with-extracts",
    "title": "Step Reports",
    "section": "Combining Step Reports with Extracts",
    "text": "Combining Step Reports with Extracts\nFor more advanced analysis, you can extract the actual data from a step report into a DataFrame:\n\n# Extract the data from the step report\nfailing_ratios = validation_complex.get_data_extracts(i=2)\n\nfailing_ratios\n\n{2: shape: (2, 5)\n ┌───────────┬─────┬───────┬───────┬──────────┐\n │ _row_num_ ┆ id  ┆ value ┆ ratio ┆ category │\n │ ---       ┆ --- ┆ ---   ┆ ---   ┆ ---      │\n │ u32       ┆ i64 ┆ i64   ┆ f64   ┆ str      │\n ╞═══════════╪═════╪═══════╪═══════╪══════════╡\n │ 4         ┆ 4   ┆ 40    ┆ 1.4   ┆ A        │\n │ 9         ┆ 9   ┆ 90    ┆ 1.2   ┆ B        │\n └───────────┴─────┴───────┴───────┴──────────┘}\n\n\nThis extracts the failing rows from the validation step, which you can then further analyze or fix as needed. Note that the parameter i=2 corresponds directly to the step number shown in the validation report; it’s the same numbering system used for get_step_report().\nThese extracts are particularly valuable for analysts who need to:\n\nperform additional calculations on problematic data\nfeed failing records into correction pipelines\ncreate visualizations of data patterns that led to validation failures\nexport problem records to share with data owners\n\nIt’s worth noting that the validation report itself includes export buttons on the far right of each row that allow you to download CSV files of the failing data directly. This serves as a convenient delivery mechanism for sharing extracts with colleagues who may not be working in Python, making the validation report not just a visual tool but also a practical means of distributing problematic data for further investigation.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#step-reports-with-segmented-data",
    "href": "user-guide/step-reports.html#step-reports-with-segmented-data",
    "title": "Step Reports",
    "section": "Step Reports with Segmented Data",
    "text": "Step Reports with Segmented Data\nWhen working with segmented validation, step reports become even more valuable as they allow you to investigate issues within specific segments:\n\n# Create data with different regions\nsegmented_data = pl.DataFrame({\n    \"id\": range(1, 10),\n    \"value\": [10, 20, 3, 40, 50, 2, 6, 8, 60],\n    \"region\": [\"North\", \"North\", \"South\", \"South\", \"East\", \"East\", \"West\", \"West\", \"West\"]\n})\n\n# Create a validation with segments\nsegmented_validation = (\n    pb.Validate(data=segmented_data, tbl_name=\"regional_data\")\n    .col_vals_gt(\n        columns=\"value\",\n        value=10,\n        segments=\"region\"  # Segment by region\n    )\n    .interrogate()\n)\n\n# Get step report for a specific segment (the 'West' region)\n# For segmented validations, each segment gets its own step number\nnorth_report = segmented_validation.get_step_report(i=4)\n\nnorth_report\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 4ASSERTION value &gt; 102 / 3 TEST UNIT FAILURES IN COLUMN 2 EXTRACT OF ALL 2 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  idInt64\n  valueInt64\n  regionString\n\n\n\n  \n    1\n    7\n    6\n    West\n  \n  \n    2\n    8\n    8\n    West\n  \n\n\n\n\n\n\n        \n\n\nFor segmented validations, each segment is treated as a separate validation step with its own step number. This allows you to investigate issues specific to each data segment using the appropriate step number from the validation report.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#best-practices-for-using-step-reports",
    "href": "user-guide/step-reports.html#best-practices-for-using-step-reports",
    "title": "Step Reports",
    "section": "Best Practices for Using Step Reports",
    "text": "Best Practices for Using Step Reports\nHere are some guidelines for effectively using step reports in your data validation workflow:\n\nGenerate step reports selectively: create reports only for steps that require detailed investigation rather than for all steps\nUse the limit= parameter for large datasets: when working with large datasets, focus only on a subset of failing rows to avoid information overload\nShare specific step reports with stakeholders: when collaborating with domain experts, share relevant step reports to help them understand and address specific data quality issues (and customize the header to improve clarity)\nCombine with extracts for deeper analysis: use the get_data_extracts() method to extract the failing rows for further analysis or correction\nDocument findings from step reports: when you discover patterns or insights from step reports, document them to inform future data quality improvements\n\nRemember that step reports are most valuable when used strategically as part of a broader data quality framework. By following these best practices, you can use step reports not just for troubleshooting, but to develop a deeper understanding of your data’s characteristics and quality patterns over time. This approach transforms step reports from simple debugging tools into strategic assets for continuous data quality improvement.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/step-reports.html#conclusion",
    "href": "user-guide/step-reports.html#conclusion",
    "title": "Step Reports",
    "section": "Conclusion",
    "text": "Conclusion\nStep reports provide a focused lens into specific validation steps, allowing you to investigate data quality issues in detail. By generating targeted reports for specific validation steps, you can:\n\npinpoint exactly which data points are causing validation failures\ncommunicate specific issues to relevant stakeholders\ngather insights that might be missed in the aggregate validation report\ntrack improvements in specific aspects of data quality over time\n\nWhether you’re debugging validation failures, investigating edge cases, or communicating specific data quality issues to colleagues, step reports can give you the detailed information you need to understand and resolve data quality problems effectively.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Step Reports"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html",
    "href": "user-guide/validation-overview.html",
    "title": "Overview",
    "section": "",
    "text": "This article provides a quick overview of the data validation features in Pointblank. It introduces the key concepts and shows examples of the main functionality, giving you a foundation for using the library effectively.\nLater articles in the User Guide will expand on each section covered here, providing more explanations and examples.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#validation-methods",
    "href": "user-guide/validation-overview.html#validation-methods",
    "title": "Overview",
    "section": "Validation Methods",
    "text": "Validation Methods\nPointblank’s core functionality revolves around validation steps, which are individual checks that verify different aspects of your data. These steps are created by calling validation methods from the Validate class. When combined they create a comprehensive validation plan for your data.\nHere’s an example of a validation that incorporates three different validation methods:\n\nimport pointblank as pb\nimport polars as pl\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Three different validation methods.\"\n    )\n    .col_vals_gt(columns=\"a\", value=0)\n    .rows_distinct()\n    .col_exists(columns=\"date\")\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Three different validation methods.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis example showcases how you can combine different types of validations in a single validation plan:\n\na column value validation with col_vals_gt()\na row-based validation with rows_distinct()\na table structure validation with col_exists()\n\nMost validation methods share common parameters that enhance their flexibility and power. These shared parameters (overviewed in the next few sections) create a consistent interface across all validation steps while allowing you to customize validation behavior for specific needs.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#column-selection-patterns",
    "href": "user-guide/validation-overview.html#column-selection-patterns",
    "title": "Overview",
    "section": "Column Selection Patterns",
    "text": "Column Selection Patterns\nYou can apply the same validation logic to multiple columns at once through use of column selection patterns (used in the columns= parameter). This reduces repetitive code and makes your validation plans more maintainable:\n\nimport narwhals.selectors as nws\n\n# Map validations across multiple columns\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Applying column mapping in `columns`.\"\n    )\n\n    # Apply validation rules to multiple columns ---\n    .col_vals_not_null(\n        columns=[\"a\", \"b\", \"c\"]\n    )\n\n    # Apply to numeric columns only with a Narwhals selector ---\n    .col_vals_gt(\n        columns=nws.numeric(),\n        value=0\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Applying column mapping in `columns`.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    5\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis technique is particularly valuable when working with wide datasets containing many similarly-structured columns or when applying standard quality checks across an entire table. It also ensures consistency in how validation rules are applied across related data columns.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#preprocessing",
    "href": "user-guide/validation-overview.html#preprocessing",
    "title": "Overview",
    "section": "Preprocessing",
    "text": "Preprocessing\nPreprocessing (with the pre= parameter) allows you to transform or modify your data before applying validation checks, enabling you to validate derived or modified data without altering the original dataset:\n\nimport polars as pl\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Preprocessing validation steps via `pre=`.\"\n    )\n    .col_vals_gt(\n        columns=\"a\", value=5,\n\n        # Apply transformation before validation ---\n        pre=lambda df: df.with_columns(\n            pl.col(\"a\") * 2  # Double values before checking\n        )\n    )\n    .col_vals_lt(\n        columns=\"c\", value=100,\n\n        # Apply more complex transformation ---\n        pre=lambda df: df.with_columns(\n            pl.col(\"c\").pow(2)  # Square values before checking\n        )\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Preprocessing validation steps via `pre=`.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    c\n    100\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nPreprocessing enables validation of transformed data without modifying your original dataset, making it ideal for checking derived metrics, or validating normalized values. This approach keeps your validation code clean while allowing for sophisticated data quality checks on calculated results.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#segmentation",
    "href": "user-guide/validation-overview.html#segmentation",
    "title": "Overview",
    "section": "Segmentation",
    "text": "Segmentation\nSegmentation (through the segments= parameter) allows you to validate data across different groups, enabling you to identify segment-specific quality issues that might be hidden in aggregate analyses:\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Segmenting validation steps via `segments=`.\"\n    )\n    .col_vals_gt(\n        columns=\"c\", value=3,\n\n        # Split into steps by categorical values in column 'f' ---\n        segments=\"f\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Segmenting validation steps via `segments=`.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    SEGMENT  f / high \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    3\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    20.33\n    40.67\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    SEGMENT  f / low \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    3\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    40.80\n    10.20\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    3\n    SEGMENT  f / mid \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    3\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    2\n    10.50\n    10.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nSegmentation is powerful for detecting patterns of quality issues that may exist only in specific data subsets, such as certain time periods, categories, or geographical regions. It helps ensure that all significant segments of your data meet quality standards, not just the data as a whole.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#thresholds",
    "href": "user-guide/validation-overview.html#thresholds",
    "title": "Overview",
    "section": "Thresholds",
    "text": "Thresholds\nThresholds (set through the thresholds= parameter) let you set acceptable levels of failure before triggering warnings, errors, or critical notifications for individual validation steps:\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Using thresholds.\"\n    )\n\n    # Add validation steps with different thresholds ---\n    .col_vals_gt(\n        columns=\"a\", value=1,\n        thresholds=pb.Thresholds(warning=0.1, error=0.2, critical=0.3)\n    )\n\n    # Add another step with stricter thresholds ---\n    .col_vals_lt(\n        columns=\"c\", value=10,\n        thresholds=pb.Thresholds(warning=0.05, error=0.1)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Using thresholds.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    c\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ●\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThresholds provide a nuanced way to monitor data quality, allowing you to set different severity levels based on the importance of each validation and your organization’s tolerance for specific types of data issues.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#actions",
    "href": "user-guide/validation-overview.html#actions",
    "title": "Overview",
    "section": "Actions",
    "text": "Actions\nActions (which can be configured in the actions= parameter) allow you to define specific responses when validation thresholds are crossed. You can use simple string messages or custom functions for more complex behavior:\n\n# Example 1: Action with a string message ---\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Using actions with a string message.\"\n    )\n    .col_vals_gt(\n        columns=\"c\", value=2,\n        thresholds=pb.Thresholds(warning=0.1, error=0.2),\n\n        # Add a print-to-console action for the 'warning' threshold ---\n        actions=pb.Actions(\n            warning=\"WARNING: Values below `{value}` detected in column 'c'.\"\n        )\n    )\n    .interrogate()\n)\n\nWARNING: Values below `2` detected in column 'c'.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Using actions with a string message.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    100.77\n    30.23\n    ●\n    ●\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\n\n# Example 2: Action with a callable function ---\n\ndef custom_action():\n    from datetime import datetime\n    print(f\"Data quality issue found ({datetime.now()}).\")\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Using actions with a callable function.\"\n    )\n    .col_vals_gt(\n        columns=\"a\", value=5,\n        thresholds=pb.Thresholds(warning=0.1, error=0.2),\n\n        # Apply the function to the 'error' threshold ---\n        actions=pb.Actions(error=custom_action)\n    )\n    .interrogate()\n)\n\nData quality issue found (2025-05-23 02:19:49.029982).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Using actions with a callable function.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    30.23\n    100.77\n    ●\n    ●\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWith custom action functions, you can implement sophisticated responses like sending notifications or logging to external systems.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#briefs",
    "href": "user-guide/validation-overview.html#briefs",
    "title": "Overview",
    "section": "Briefs",
    "text": "Briefs\nBriefs (which can be set through the brief= parameter) allow you to customize descriptions associated with validation steps, making validation results more understandable to stakeholders. Briefs can be either automatically generated by setting brief=True or defined as custom messages for more specific explanations:\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Using `brief=` for displaying brief messages.\"\n    )\n    .col_vals_gt(\n        columns=\"a\", value=0,\n\n        # Use `True` for automatic generation of briefs ---\n        brief=True\n    )\n    .col_exists(\n        columns=[\"date\", \"date_time\"],\n\n        # Add a custom brief for this validation step ---\n        brief=\"Verify required date columns exist for time-series analysis\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Using `brief=` for displaying brief messages.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in a should be &gt; 0.\n\n        \n    a\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        Verify required date columns exist for time-series analysis\n\n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        Verify required date columns exist for time-series analysis\n\n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nBriefs make validation results more meaningful by providing context about why each check matters. They’re particularly valuable in shared reports where stakeholders from various disciplines need to understand validation results in domain-specific terms.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#getting-more-information",
    "href": "user-guide/validation-overview.html#getting-more-information",
    "title": "Overview",
    "section": "Getting More Information",
    "text": "Getting More Information\nEach validation step can be further customized and has additional options. See these pages for more information:\n\nValidation Methods: A closer look at the more common validation methods\nColumn Selection Patterns: Techniques for targeting specific columns\nPreprocessing: Transform data before validation\nSegmentation: Apply validations to specific segments of your data\nThresholds: Set quality standards and trigger severity levels\nActions: Respond to threshold exceedances with notifications or custom functions\nBriefs: Add context to validation steps",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/validation-overview.html#conclusion",
    "href": "user-guide/validation-overview.html#conclusion",
    "title": "Overview",
    "section": "Conclusion",
    "text": "Conclusion\nValidation steps are the building blocks of data validation in Pointblank. By combining steps from different categories and leveraging common features like thresholds, actions, and preprocessing, you can create comprehensive data quality checks tailored to your specific needs.\nThe next sections of this guide will dive deeper into each of these topics, providing detailed explanations and examples.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Overview"
    ]
  },
  {
    "objectID": "user-guide/langs.html",
    "href": "user-guide/langs.html",
    "title": "Languages",
    "section": "",
    "text": "It’s possible to generate reporting in various spoken languages. We do this via the lang= argument in Validate."
  },
  {
    "objectID": "user-guide/thresholds.html",
    "href": "user-guide/thresholds.html",
    "title": "Thresholds",
    "section": "",
    "text": "Thresholds are a key concept in Pointblank that allow you to define acceptable limits for failing validation tests. Rather than a simple pass/fail model, thresholds enable you to signal failure at different severity levels (‘warning’, ‘error’, and ‘critical’), giving you fine-grained control over how data quality issues are reported and handled.\nWhen used with Actions (covered in the next section), thresholds create a robust system for responding to data quality issues based on their severity. This approach allows you to:",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#a-simple-example",
    "href": "user-guide/thresholds.html#a-simple-example",
    "title": "Thresholds",
    "section": "A Simple Example",
    "text": "A Simple Example\nLet’s start with a basic example that demonstrates how thresholds work in practice:\n\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_not_null(\n        columns=\"c\",\n\n        # Set thresholds for the validation step ---\n        thresholds=pb.Thresholds(warning=1, error=0.2)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:35Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we’re validating that column c contains no Null values. We’ve set:\n\nA ‘warning’ threshold of 1 (triggers when 1 or more values are Null)\nAn ‘error’ threshold of 0.2 (triggers when 20% or more values are Null)\n\nLooking at the results:\n\nthe FAIL column shows that 2 test units have failed\nthe W column (for ‘warning’) shows a filled gray circle, indicating the warning threshold has been exceeded\nthe E column (for ‘error’) shows an open yellow circle, indicating the error threshold has not been exceeded\nthe C column (for ‘critical’) shows a dash since we didn’t set a critical threshold",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#types-of-threshold-values",
    "href": "user-guide/thresholds.html#types-of-threshold-values",
    "title": "Thresholds",
    "section": "Types of Threshold Values",
    "text": "Types of Threshold Values\nThresholds in Pointblank can be specified in two different ways:\n\nAbsolute Thresholds\nAbsolute thresholds are specified as integers and represent a fixed number of failing test units:\n# Warning threshold of exactly 5 failing test units\nthresholds_absolute = pb.Thresholds(warning=5)\nWith this configuration, the ‘warning’ threshold would be triggered if 5 or more test units fail.\n\n\nProportional Thresholds\nProportional thresholds are specified as decimals between 0 and 1, representing a percentage of the total test units:\n# Error threshold of 10% of test units failing\nthresholds_proportional = pb.Thresholds(error=0.1)\nWith this configuration, the ‘error’ threshold would be triggered if 10% or more of the test units fail.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#understanding-severity-levels",
    "href": "user-guide/thresholds.html#understanding-severity-levels",
    "title": "Thresholds",
    "section": "Understanding Severity Levels",
    "text": "Understanding Severity Levels\nThe three threshold levels in Pointblank (‘warning’, ‘error’, and ‘critical’) are inspired by traditional logging levels used in software development. These names suggest a progression of severity:\n\n‘warning’ (level 30): indicates potential issues that don’t necessarily prevent normal operation\n‘error’ (level 40): suggests more serious problems that might impact data quality\n‘critical’ (level 50): represents the most severe issues that likely require immediate attention\n\nThese numerical values (30, 40, 50) are used internally by Pointblank when determining threshold hierarchy and can be accessed through the {level_num} field in action metadata (covered in the next User Guide article).\nWhile these names imply certain severity levels, they’re ultimately just convenient labels for different thresholds. You have complete flexibility in how you use them:\n\nyou could use ‘warning’ for issues that should block a pipeline\nyou might configure ‘critical’ for minor issues that just need documentation\nthe ‘error’ level could trigger informational emails rather than actual error handling\n\nThe naming is primarily a suggestion to help organize your validation strategy. What matters most is how you configure actions for each threshold level to suit your specific data quality requirements.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#threshold-behavior",
    "href": "user-guide/thresholds.html#threshold-behavior",
    "title": "Thresholds",
    "section": "Threshold Behavior",
    "text": "Threshold Behavior\nIt’s important to understand a few key behaviors of thresholds:\n\nthresholds are inclusive: a value equal to or exceeding the threshold will trigger the associated level\nthresholds can be mixed: you can use absolute values for some levels and proportional for others\nthreshold levels are hierarchical: ‘critical’ is more severe than ‘error’, which is more severe than ‘warning’\nwhen a test fails, all applicable threshold levels are marked in the report (though actions may only execute for the highest level by default)",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#setting-global-thresholds",
    "href": "user-guide/thresholds.html#setting-global-thresholds",
    "title": "Thresholds",
    "section": "Setting Global Thresholds",
    "text": "Setting Global Thresholds\nYou can set thresholds globally for all validation steps in a workflow using the thresholds= parameter in Validate:\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n\n        # Setting thresholds for all validation steps ---\n        thresholds=pb.Thresholds(warning=1, error=0.1)\n    )\n    .col_vals_not_null(columns=\"a\")\n    .col_vals_gt(columns=\"a\", value=2)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:35PolarsWARNING1ERROR0.1CRITICAL—\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    —\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    ●\n    ●\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWith this approach, the same thresholds are applied to every validation step in the workflow.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#overriding-thresholds-for-specific-steps",
    "href": "user-guide/thresholds.html#overriding-thresholds-for-specific-steps",
    "title": "Thresholds",
    "section": "Overriding Thresholds for Specific Steps",
    "text": "Overriding Thresholds for Specific Steps\nYou can override global thresholds for specific validation steps by providing the thresholds= parameter in individual validation methods:\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n\n        # Setting global thresholds ---\n        thresholds=pb.Thresholds(warning=1, error=0.1)\n    )\n    .col_vals_not_null(columns=\"a\")\n    .col_vals_gt(\n        columns=\"a\", value=2,\n\n        # Step-specific threshold that overrides global ---\n        thresholds=pb.Thresholds(warning=3)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:35PolarsWARNING1ERROR0.1CRITICAL—\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    —\n    —\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    ●\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nIn this example, the second validation step uses its own ‘warning’ threshold of 3, overriding the global setting of 1.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#ways-to-define-thresholds",
    "href": "user-guide/thresholds.html#ways-to-define-thresholds",
    "title": "Thresholds",
    "section": "Ways to Define Thresholds",
    "text": "Ways to Define Thresholds\nPointblank offers multiple ways to define thresholds to accommodate different coding styles and requirements.\n\n1. Using the Thresholds Class (Recommended)\nThe most explicit and flexible approach is using the Thresholds class:\n\n# Set individual thresholds for different levels\nthresholds_all_levels = pb.Thresholds(warning=0.05, error=0.1, critical=0.25)\n\n# Set only specific levels\nthresholds_error_only = pb.Thresholds(error=0.15)\n\nThis approach allows you to:\n\nset any combination of threshold levels\nuse descriptive parameter names for clarity\nskip levels you don’t need to set\n\n\n\n2. Using a Tuple\nFor concise code, you can use a tuple where positions represent ‘warning’, ‘error’, and ‘critical’ levels in that order:\n\n# (warning, error, critical)\nthresholds_tuple = (1, 0.1, 0.25)\n\n# Shorter tuples are also allowed\nthresholds_tuple_warning = (3,)            # Only the 'warning' threshold\nthresholds_tuple_warning_error = (3, 0.2)  # Both 'warning' and 'error' thresholds\n\nWhile concise, this approach requires you to start with the ‘warning’ level and add levels in order.\n\n\n3. Using a Dictionary\nYou can also use a dictionary with keys that match the threshold level names:\n\n# Can use any combination of threshold levels\nthresholds_dict = {\"warning\": 1, \"critical\": 0.15}\n\nThe dictionary must use the exact keys \"warning\", \"error\", and/or \"critical\".\n\n\n4. Using a Single Value\nThe simplest approach is using a single numeric value, which sets just the ‘warning’ threshold:\n\n# Sets 'warning' threshold to `5`\nthresholds_single = 5\n\nThis is equivalent to pb.Thresholds(warning=5).",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#thresholds-and-validation-steps",
    "href": "user-guide/thresholds.html#thresholds-and-validation-steps",
    "title": "Thresholds",
    "section": "Thresholds and Validation Steps",
    "text": "Thresholds and Validation Steps\nLet’s look at a more complete validation workflow that demonstrates different threshold configurations:\n\n# Create a validation workflow with global and step-specific thresholds\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n\n        # Global thresholds applied to all steps unless overridden ---\n        thresholds=pb.Thresholds(warning=0.05, error=0.1, critical=0.2)\n    )\n\n    # Step 1: Uses global thresholds ---\n    .col_vals_not_null(columns=\"b\")\n\n    # Step 2: Overrides with step-specific thresholds ---\n    .col_vals_gt(\n        columns=\"a\", value=2,\n        thresholds=pb.Thresholds(warning=1, critical=0.3) # No 'error' threshold\n    )\n\n    # Step 3: Uses a simplified tuple notation ---\n    .col_vals_not_null(columns=\"c\", thresholds=(2, 0.15))\n\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:35PolarsWARNING0.05ERROR0.1CRITICAL0.2\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    ●\n    —\n    ●\n    CSV\n  \n  \n    #EBBC14\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ●\n    —\n    CSV",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#thresholds-and-actions",
    "href": "user-guide/thresholds.html#thresholds-and-actions",
    "title": "Thresholds",
    "section": "Thresholds and Actions",
    "text": "Thresholds and Actions\nWhile thresholds by themselves provide visual indicators of validation severity in reports, their real power emerges when combined with Actions. The Actions system (covered in the next article) allows you to specify what happens when a threshold is exceeded.\nFor example, you might configure:\n\nA ‘warning’ threshold that logs a message\nAn ‘error’ threshold that sends an email notification\nA ‘critical’ threshold that blocks a data pipeline\n\nHere’s a simple preview of how thresholds and actions work together:\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n\n        # Define thresholds for all three severity levels ---\n        thresholds=pb.Thresholds(warning=1, error=2, critical=3),\n\n        # Define actions for different threshold levels ---\n        actions=pb.Actions(\n            warning=\"Warning: {step} has {FAIL} failing values\",\n            error=\"ERROR: Step {step} exceeded the 'error' threshold\",\n            critical=\"CRITICAL: Data quality issue in column {col}\"\n        )\n    )\n    .col_vals_not_null(columns=\"c\")\n    .interrogate()\n)\n\nERROR: Step 1 exceeded the 'error' threshold\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:35PolarsWARNING1ERROR2CRITICAL3\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ●\n    ○\n    CSV",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#conclusion",
    "href": "user-guide/thresholds.html#conclusion",
    "title": "Thresholds",
    "section": "Conclusion",
    "text": "Conclusion\nThresholds are a powerful feature that transform Pointblank from a simple validation tool into a sophisticated data quality monitoring system. By setting appropriate thresholds, you can:\n\nDefine different severity levels for data quality issues\nCustomize tolerance levels for different types of validation checks\nCreate a more nuanced approach to data validation than binary pass/fail\nEnable targeted actions based on the severity of issues detected\n\nIn the next article, we’ll explore the Actions system in depth, showing you how to define automatic responses when thresholds are exceeded.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/column-selection-patterns.html",
    "href": "user-guide/column-selection-patterns.html",
    "title": "Column Selection Patterns",
    "section": "",
    "text": "Data validation often requires working with columns in flexible ways. Pointblank offers two powerful approaches:\nThis guide covers both approaches in detail with practical examples.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Column Selection Patterns"
    ]
  },
  {
    "objectID": "user-guide/column-selection-patterns.html#part-1-applying-rules-across-multiple-columns",
    "href": "user-guide/column-selection-patterns.html#part-1-applying-rules-across-multiple-columns",
    "title": "Column Selection Patterns",
    "section": "Part 1: Applying Rules Across Multiple Columns",
    "text": "Part 1: Applying Rules Across Multiple Columns\nMany of Pointblank’s validation methods perform column-level checks. These methods provide the columns= parameter, which accepts not just a single column name but multiple columns through various selection methods.\nWhy is this useful? Often you’ll want to perform the same validation check (e.g., checking that numerical values are all positive) across multiple columns. Rather than defining the same rules multiple times, you can map the validation across those columns in a single step.\nLet’s explore this using the game_revenue dataset:\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\n\nUsing a List of Column Names\nThe simplest way to validate multiple columns is to provide a list to the columns= parameter. In the game_revenue dataset, we have two columns with numerical data: item_revenue and session_duration. If we expect all values in both columns to be greater than 0, we can write:\n\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(\n        columns=[\"item_revenue\", \"session_duration\"],\n        value=0\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation report shows two validation steps were created from a single method call! All validation parameters are shared across all generated steps, including thresholds and briefs:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(\n        columns=[\"item_revenue\", \"session_duration\"],\n        value=0,\n        thresholds=(0.1, 0.2, 0.3),\n        brief=\"`{col}` must be greater than zero.\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        item_revenue must be greater than zero.\n\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        session_duration must be greater than zero.\n\n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example, you can see that the validation report displays customized briefs for each column (“item_revenue must be greater than zero.” and “session_duration must be greater than zero.”), automatically substituting the column name using the {col} placeholder in the brief template. This feature is particularly helpful when reviewing reports, as it provides clear, human-readable descriptions of what each validation step is checking. When working with multiple columns through a single validation call, these dynamically generated briefs make your validation reports more understandable for both technical and non-technical stakeholders.\n\n\nUsing Pointblank’s Column Selectors\nFor more advanced column selection, Pointblank provides selector functions that resolve columns based on:\n\ntext patterns in column names\ncolumn position\ncolumn data type\n\nTwo common selectors, starts_with() and ends_with(), resolve columns based on text patterns in column names.\nThe game_revenue dataset has three columns starting with “item”: item_type, item_name, and item_revenue. Let’s check that these columns contain no missing values:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.starts_with(\"item\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThree validation steps were automatically created because three columns matched the pattern.\nThe complete list of column selectors includes:\n\nstarts_with()\nends_with()\ncontains()\nmatches()\neverything()\nfirst_n()\nlast_n()\n\n\n\nCombining Column Selectors\nColumn selectors can be combined for more powerful selection. To do this, use the col() helper function with logical operators:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nFor example, to select all columns except the first four:\n\ncol_selection = pb.col(pb.everything() - pb.first_n(4))\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(\n        columns=col_selection,\n        thresholds=(1, 0.05, 0.1)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    session_duration\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    acquisition\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    country\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis selects every column except the first four, resulting in seven validation steps.\n\n\nNarwhals Selectors\nPointblank also supports column selectors from the Narwhals library, which include:\n\nmatches()\nby_dtype()\nboolean()\ncategorical()\ndatetime()\nnumeric()\nstring()\n\nHere’s an example selecting all numeric columns:\n\nimport narwhals.selectors as ncs\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(\n        columns=ncs.numeric(),\n        value=0\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nAnd selecting all string columns matching “item_”:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.col(ncs.string() & ncs.matches(\"item_\")))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis example demonstrates the power of combining Narwhals selectors with logical operators. By using ncs.string() to select string columns and then filtering with ncs.matches(\"item_\"), we can precisely target text columns with specific naming patterns. This type of targeted selection is particularly valuable when working with wide datasets that have consistent column naming conventions, allowing you to apply appropriate validation rules to logically grouped columns without explicitly listing each one.\n\n\nCaveats for Using Column Selectors\nWhile column selectors are powerful, there are some caveats. If a selector doesn’t match any columns, the validation won’t fail but will show an ‘explosion’ in the report:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.starts_with(\"items\"))\n    .col_vals_gt(\n        columns=\"item_revenue\",\n        value=0\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    StartsWith(text='items', case_sensitive=False)\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    💥\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nNotice that although there was a problem with Step 1 (that should be addressed), the interrogation did move on to Step 2 without complication.\nTo mitigate uncertainty, include validation steps that check for the existence of key columns with col_exists() or verify the schema with col_schema_match().",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Column Selection Patterns"
    ]
  },
  {
    "objectID": "user-guide/column-selection-patterns.html#part-2-comparing-values-between-columns",
    "href": "user-guide/column-selection-patterns.html#part-2-comparing-values-between-columns",
    "title": "Column Selection Patterns",
    "section": "Part 2: Comparing Values Between Columns",
    "text": "Part 2: Comparing Values Between Columns\nSometimes you need to compare values across different columns rather than against fixed values. Pointblank enables this through the col() helper function.\nLet’s look at examples using the small_table dataset:\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\n\nUsing col() to Specify a Comparison Column\nWhile we typically use validation methods to compare column values against fixed values:\n...\n.col_vals_gt(columns=\"a\", value=2, ...)\n...\nWe can also compare values between columns by using col() in the value= parameter:\n...\n.col_vals_gt(columns=\"a\", value=pb.col(\"x\"), ...)\n...\nThis checks that each value in column a is greater than the corresponding value in column x. Here’s a concrete example:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_gt(\n        columns=\"d\",\n        value=pb.col(\"c\")\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nNotice that the validation report shows both column names (d and c). There are two failing test units because of missing values in column c. When comparing across columns, missing values in either column can cause failures.\nTo handle missing values, use na_pass=True:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_gt(\n        columns=\"d\",\n        value=pb.col(\"c\"),\n        na_pass=True\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nNow all tests pass.\nThe following validation methods accept a col() expression in their value= parameter:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\n\n\n\nUsing col() in Range Checks\nFor range validations via col_vals_between() and col_vals_outside() you can use a mix of column references and fixed values:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_between(\n        columns=\"d\",\n        left=pb.col(\"c\"),\n        right=10_000,\n        na_pass=True\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [c, 10000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation report shows the range as [c, 10000], indicating that the lower bound comes from column c while the upper bound is fixed at 10000.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Column Selection Patterns"
    ]
  },
  {
    "objectID": "user-guide/column-selection-patterns.html#advanced-examples-combining-both-approaches",
    "href": "user-guide/column-selection-patterns.html#advanced-examples-combining-both-approaches",
    "title": "Column Selection Patterns",
    "section": "Advanced Examples: Combining Both Approaches",
    "text": "Advanced Examples: Combining Both Approaches\nThe true power comes from combining both approaches: validating multiple columns and using cross-column comparisons:\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_gt(\n        columns=[\"c\", \"d\"],\n        value=pb.col(\"a\"),\n        na_pass=True\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    a\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    80.62\n    50.38\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    a\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis creates validation steps checking that values in both columns d and e are greater than their corresponding values in column a.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Column Selection Patterns"
    ]
  },
  {
    "objectID": "user-guide/column-selection-patterns.html#conclusion",
    "href": "user-guide/column-selection-patterns.html#conclusion",
    "title": "Column Selection Patterns",
    "section": "Conclusion",
    "text": "Conclusion\nPointblank provides flexible approaches to working with columns:\n\nColumn selection: validate multiple columns with a single validation rule\nCross-column comparison: compare values between columns\n\nThese capabilities allow you to:\n\nwrite more concise validation code\napply consistent validation rules across similar columns\ncreate dynamic validations that check relationships between columns\nbuild comprehensive data quality checks with minimal code\n\nBy mastering these techniques, you can create more elegant and powerful validation plans while reducing repetition in your code.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Column Selection Patterns"
    ]
  },
  {
    "objectID": "user-guide/index.html",
    "href": "user-guide/index.html",
    "title": "Introduction",
    "section": "",
    "text": "The Pointblank library is all about assessing the state of data quality for a table. You provide the validation rules and the library will dutifully interrogate the data and provide useful reporting. We can use different types of tables like Polars and Pandas DataFrames, Parquet files, or various database tables. Let’s walk through what data validation looks like in Pointblank.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#a-simple-validation-table",
    "href": "user-guide/index.html#a-simple-validation-table",
    "title": "Introduction",
    "section": "A Simple Validation Table",
    "text": "A Simple Validation Table\nThis is a validation report table that is produced from a validation of a Polars DataFrame:\n\n\nShow the code\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"), label=\"Example Validation\")\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [0, 5000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^[0-9]-[a-z]{3}-[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nEach row in this reporting table constitutes a single validation step. Roughly, the left-hand side outlines the validation rules and the right-hand side provides the results of each validation step. While simple in principle, there’s a lot of useful information packed into this validation table.\nHere’s a diagram that describes a few of the important parts of the validation table:\n\nThere are three things that should be noted here:\n\nvalidation steps: each step is a separate test on the table, focused on a certain aspect of the table\nvalidation rules: the validation type is provided here along with key constraints\nvalidation results: interrogation results are provided here, with a breakdown of test units (total, passing, and failing), threshold flags, and more\n\nThe intent is to provide the key information in one place, and have it be interpretable by data stakeholders. For example, a failure can be seen in the second row (notice there’s a CSV button). A data quality stakeholder could click this to download a CSV of the failing rows for that step.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#example-code-step-by-step",
    "href": "user-guide/index.html#example-code-step-by-step",
    "title": "Introduction",
    "section": "Example Code, Step-by-Step",
    "text": "Example Code, Step-by-Step\nThis section will walk you through the example code used above.\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\nNote these three key pieces in the code:\n\ndata: the Validate(data=) argument takes a DataFrame or database table that you want to validate\nsteps: the methods starting with col_vals_ specify validation steps that run on specific columns\nexecution: the interrogate() method executes the validation plan on the table\n\nThis common pattern is used in a validation workflow, where Validate and interrogate() bookend a validation plan generated through calling validation methods.\nIn the next few sections we’ll go a bit further by understanding how we can measure data quality and respond to failures.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#understanding-test-units",
    "href": "user-guide/index.html#understanding-test-units",
    "title": "Introduction",
    "section": "Understanding Test Units",
    "text": "Understanding Test Units\nEach validation step will execute a type of validation test on the target table. For example, a col_vals_lt() validation step can test that each value in a column is less than a specified number. And the key finding that’s reported in each step is the number of test units that pass or fail.\nIn the validation report table, test unit metrics are displayed under the UNITS, PASS, and FAIL columns. This diagram explains what the tabulated values signify:\n\nTest units are dependent on the test being run. Some validation methods might test every value in a particular column, so each value will be a test unit. Others will only have a single test unit since they aren’t testing individual values but rather if the overall test passes or fails.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#setting-thresholds-for-data-quality-signals",
    "href": "user-guide/index.html#setting-thresholds-for-data-quality-signals",
    "title": "Introduction",
    "section": "Setting Thresholds for Data Quality Signals",
    "text": "Setting Thresholds for Data Quality Signals\nUnderstanding test units is essential because they form the foundation of Pointblank’s threshold system. Thresholds let you define acceptable levels of data quality, triggering different severity signals (‘warning’, ‘error’, or ‘critical’) when certain failure conditions are met.\nHere’s a simple example that uses a single validation step along with thresholds set using the Thresholds class:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_lt(\n        columns=\"a\",\n        value=7,\n\n        # Set the 'warning' and 'error' thresholds ---\n        thresholds=pb.Thresholds(warning=2, error=4)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:18Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nIf you look at the validation report table, we can see:\n\nthe FAIL column shows that 2 tests units have failed\nthe W column (short for ‘warning’) shows a filled gray circle indicating those failing test units reached that threshold value\nthe E column (short for ‘error’) shows an open yellow circle indicating that the number of failing test units is below that threshold\n\nThe one final threshold level, C (for ‘critical’), wasn’t set so it appears on the validation table as a long dash.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#taking-action-on-threshold-exceedances",
    "href": "user-guide/index.html#taking-action-on-threshold-exceedances",
    "title": "Introduction",
    "section": "Taking Action on Threshold Exceedances",
    "text": "Taking Action on Threshold Exceedances\nPointblank becomes even more powerful when you combine thresholds with actions. The Actions class lets you trigger responses when validation failures exceed threshold levels, turning passive reporting into active notifications.\nHere’s a simple example that adds an action to the previous validation:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_lt(\n        columns=\"a\",\n        value=7,\n        thresholds=pb.Thresholds(warning=2, error=4),\n\n        # Set an action for the 'warning' threshold ---\n        actions=pb.Actions(\n            warning=\"WARNING: Column 'a' has values that aren't less than 7.\"\n        )\n    )\n    .interrogate()\n)\n\nWARNING: Column 'a' has values that aren't less than 7.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:18Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nNotice the printed warning message: \"WARNING: Column 'a' has values that aren't less than 7.\". The warning indicator (filled gray circle) visually confirms this threshold was reached and the action should trigger.\nActions make your validation workflows more responsive and integrated with your data pipelines. For example, you can generate console messages, Slack notifications, and more.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#navigating-the-user-guide",
    "href": "user-guide/index.html#navigating-the-user-guide",
    "title": "Introduction",
    "section": "Navigating the User Guide",
    "text": "Navigating the User Guide\nAs you continue exploring Pointblank’s capabilities, you’ll find the **User Guide* organized into sections that will help you navigate the various features.\n\nGetting Started\nThe Getting Started section introduces you to Pointblank:\n\nIntroduction: Overview of Pointblank and core concepts (this article)\nInstallation: How to install and set up Pointblank\n\n\n\nValidation Plan\nThe Validation Plan section covers everything you need to know about creating robust validation plans:\n\nOverview: Survey of validation methods and their shared parameters\nValidation Methods: A closer look at the more common validation methods\nColumn Selection Patterns: Techniques for targeting specific columns\nPreprocessing: Transform data before validation\nSegmentation: Apply validations to specific segments of your data\nThresholds: Set quality standards and trigger severity levels\nActions: Respond to threshold exceedances with notifications or custom functions\nBriefs: Add context to validation steps\n\n\n\nAdvanced Validation\nThe Advanced Validation section explores more specialized validation techniques:\n\nExpression-Based Validation: Use column expressions for advanced validation\nSchema Validation: Enforce table structure and column types\nAssertions: Raise exceptions to enforce data quality requirements\nDraft Validation: Create validation plans from existing data\n\n\n\nPost Interrogation\nAfter validating your data, the Post Interrogation section helps you analyze and respond to results:\n\nStep Reports: View detailed results for individual validation steps\nData Extracts: Extract and analyze failing data\nSundering Validated Data: Split data based on validation results\n\n\n\nData Inspection\nThe Data Inspection section provides tools to explore and understand your data:\n\nPreviewing Data: View samples of your data\nColumn Summaries: Get statistical summaries of your data\nMissing Values Reporting: Identify and visualize missing data\n\nBy following this guide, you’ll gain a comprehensive understanding of how to validate, monitor, and maintain high-quality data with Pointblank.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/actions.html",
    "href": "user-guide/actions.html",
    "title": "Actions",
    "section": "",
    "text": "Actions transform data validation from passive reporting to active response by automatically executing code when quality issues arise. They bridge the gap between detection and intervention, enabling immediate notifications and comprehensive logging when thresholds are exceeded.\nWhether you need simple console messages for interactive analysis or complex alerting for production pipelines, Actions provide the framework to make your validation workflows responsive. For example, when validating revenue values, you can configure immediate alerts if failures exceed acceptable thresholds, ensuring data issues are addressed promptly rather than discovered later.\nIn this article, we’ll explore how to use Actions to respond to threshold violations during data validation, and Final Actions to execute code after all validation steps are complete, giving you powerful tools to monitor, alert, and report on your data’s quality.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#how-actions-work",
    "href": "user-guide/actions.html#how-actions-work",
    "title": "Actions",
    "section": "How Actions Work",
    "text": "How Actions Work\nLet’s look at an example on how this works in practice. The following validation plan contains a single step (using col_vals_gt()) where the thresholds= and actions= parameters are set using Thresholds and Actions calls:\n\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_gt(\n        columns=\"c\", value=2,\n        thresholds=pb.Thresholds(warning=1, error=5),\n\n        # Emit a console message when the warning threshold is exceeded ---\n        actions=pb.Actions(warning=\"WARNING: failing test found.\")\n    )\n    .interrogate()\n)\n\nWARNING: failing test found.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:09Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    100.77\n    30.23\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe code uses thresholds=pb.Thresholds(warning=1, error=5) to set a ‘warning’ threshold of 1 and an ‘error’ threshold of 5 failing test units. The results part of the validation table shows that:\n\nThe FAIL column shows that 3 tests units have failed\nThe W column (short for ‘warning’) shows a filled gray circle indicating it’s reached its threshold level\nThe E (‘error’) column shows an open yellow circle indicating it’s below the threshold level\n\nMore importantly, the text \"WARNING: failing test found.\" has been emitted. Here it appears above the validation table and that’s because the action is executed eagerly during interrogation (before the report has even been generated).\nSo, an action is executed for a particular condition (e.g., ‘warning’) within a validation step if these three things are true:\n\nthere is a threshold set for that condition (either globally, or as part of that step)\nthere is an associated action set for the condition (again, either set globally or within the step)\nduring interrogation, the threshold value for the condition was exceeded by the number or proportion of failing test units\n\nThere is a lot of flexibility for setting both thresholds and actions and everything here is considered optional. Put another way, you can set various thresholds and various actions as needed and the interrogation phase will determine whether all the requirements are met for executing an action.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#defining-actions",
    "href": "user-guide/actions.html#defining-actions",
    "title": "Actions",
    "section": "Defining Actions",
    "text": "Defining Actions\nActions can be defined in several ways, providing flexibility for different notification needs.\n\nUsing String Messages\nThere are a few options in how to define the actions:\n\nString: a message to be displayed in the console\nCallable: a function to be called\nList of Strings/Callables: for execution of multiple messages or functions\n\nThe actions are executed at interrogation time when the threshold level assigned to the action is exceeded by the number or proportion of failing test units. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of interrogation. If providing a list of strings or callables, each item in the list will be executed in order. Such a list can contain a mix of strings and callables.\nDisplaying console messages may be a simple approach, but it is effective. And the strings don’t have to be static, there are templating features that can be useful for constructing strings for a variety of situations. The following placeholders are available for use:\n\n{type}: The validation step type where the action is executed (e.g., ‘col_vals_gt’, etc.)\n{level}: The threshold level where the action is executed (‘warning’, ‘error’, or ‘critical’)\n{step} or {i}: The step number in the validation workflow where the action is executed\n{col} or {column}: The column name where the action is executed\n{val} or {value}: An associated value for the validation method\n{time}: A datetime value for when the action was executed\n\nHere’s an example where we prepare a console message with a number of value placeholders (action_str) and use it globally at Actions(critical=):\n\naction_str = \"[{LEVEL}: {TYPE}]: Step {step} has failed validation. ({time})\"\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n\n        # Use `action_str` for any critical thresholds exceeded ---\n        actions=pb.Actions(critical=action_str),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.10)\n    .col_vals_ge(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\n[CRITICAL: COL_VALS_GT]: Step 2 has failed validation. (2025-05-23 02:19:10.124093+00:00)\n[CRITICAL: COL_VALS_GE]: Step 3 has failed validation. (2025-05-23 02:19:10.147370+00:00)\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:10DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    14400.72\n    5600.28\n    ●\n    ●\n    ●\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16860.84\n    3140.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nWhat we get here are two messages in the console, corresponding to critical failures in steps 2 and 3. The placeholders were replaced with the correct text for the context. Note that some of the resulting text is capitalized (e.g., \"CRITICAL\", \"COL_VALS_GT\", etc.) and this is because we capitalized the placeholder text itself. Have a look at the documentation article of Actions for more details on this.\n\n\nUsing Callable Functions\nAside from strings, any callable can be used as an action value. Here’s an example where we use a custom function as part of an action:\n\ndef duration_issue():\n    from datetime import datetime\n    print(f\"Data quality issue found ({datetime.now()}).\")\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\", value=15,\n\n        # Use the `duration_issue()` function as an action for this step ---\n        actions=pb.Actions(warning=duration_issue),\n    )\n    .interrogate()\n)\n\nData quality issue found (2025-05-23 02:19:10.498573).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:10DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the ‘warning’ action is set to call the user’s dq_issue() function. This action is only executed when the ‘warning’ threshold is exceeded in step 3. Because all three thresholds are exceeded in that step, the ‘warning’ action of executing the function occurs (resulting in a message being printed to the console).\nThis is an example where actions can be defined locally for an individual validation step. The global threshold setting applied to all three validation steps but the step-level action only applied to step 3. You are free to mix and match both threshold and action settings at the global level (i.e., set in the Validate call) or at the step level. The key thing to be aware of is that step-level settings of thresholds and actions take precedence.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#accessing-context-in-actions",
    "href": "user-guide/actions.html#accessing-context-in-actions",
    "title": "Actions",
    "section": "Accessing Context in Actions",
    "text": "Accessing Context in Actions\nWhile string templates provide helpful placeholders to access information about validation steps, callable functions offer more flexibility through access to detailed metadata. When using functions as actions, you can retrieve comprehensive information about the validation context, allowing for complex logic and dynamic responses to validation issues.\n\nUsing get_action_metadata() in Callables\nTo access information about the validation step where an action was triggered, we can call get_action_metadata() in the body of a function to be used within Actions. This provides useful context about the validation step that triggered the action.\n\ndef print_problem():\n    m = pb.get_action_metadata()\n    print(f\"{m['level']} ({m['level_num']}) for Step {m['step']}: {m['failure_text']}\")\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n\n        # Use the `print_problem()` function as the action ---\n        actions=pb.Actions(default=print_problem),\n        brief=True,\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nerror (40) for Step 2: Exceedance of failed test units where values in `item_revenue` should have been &gt; `0.05`.\ncritical (50) for Step 3: Exceedance of failed test units where values in `session_duration` should have been &gt; `15`.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:10DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        Expect that values in player_id should match the regular expression: [A-Z]{12}\\d{3}.\n\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in item_revenue should be &gt; 0.05.\n\n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in session_duration should be &gt; 15.\n\n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we’re creating a function called print_problem() that prints information about each validation step that fails. We then apply this function as the default action for all threshold levels using actions=pb.Actions(default=print_problem). (Note that the default= and highest_only= parameters will be covered in more detail in following sections.)\nWe end up seeing two messages printed for failures in Steps 2 and 3. And though those steps had more than one threshold exceeded, only the most severe level in each yielded a console message (due to the default highest_only=True behavior).\nBy setting the action in Validate(actions=), we applied it to all validation steps where thresholds are exceeded. This eliminates the need to set actions= at every validation step (though you can do this as a local override, even setting actions=None to disable globally set actions).\n\n\nAvailable Metadata Fields\nThe dictionary returned by get_action_metadata() contains the following fields:\n\nstep: The step number.\ncolumn: The column name.\nvalue: The value being compared (only available in certain validation steps).\ntype: The assertion type (e.g., \"col_vals_gt\", etc.).\ntime: The time the validation step was executed (in ISO format).\nlevel: The severity level (\"warning\", \"error\", or \"critical\").\nlevel_num: The severity level as a numeric value (30, 40, or 50).\nautobrief: A localized and brief statement of the expectation for the step.\nfailure_text: Localized text that explains how the validation step failed.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#customizing-action-behavior",
    "href": "user-guide/actions.html#customizing-action-behavior",
    "title": "Actions",
    "section": "Customizing Action Behavior",
    "text": "Customizing Action Behavior\nThe Actions class has two additional parameters that provide more control over how actions are executed:\n\nSetting Default Actions with default=\nInstead of specifying actions separately for each threshold level, you can use the default parameter to set a common action for all levels:\n\ndef log_all_issues():\n    m = pb.get_action_metadata()\n    print(f\"[{m['level'].upper()}] Validation failed in step {m['step']} with level {m['level']}\")\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n\n        # The `log_all_issues()` callable is set to every threshold ---\n        actions=pb.Actions(default=log_all_issues),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\n[ERROR] Validation failed in step 2 with level error\n[CRITICAL] Validation failed in step 3 with level critical\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:10DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe default= parameter sets the same action for all threshold levels. If you later specify an action for a specific level, it will override this default for that level only.\nWhen using the default= parameter, be aware that your action (whether a string template or callable function) needs to work across all validation steps where thresholds might be exceeded. Not all validation methods provide the same context for string templates or in the metadata dictionary returned by get_action_metadata().\nFor example, some validation steps like col_vals_gt() provide a value field that can be accessed with {value} in string templates, while others like col_exists() don’t have this concept. When creating default actions, either use only the universally available placeholders ({step}, {level}, {type}, and {time}), or include conditional logic in your callable functions to handle different validation types appropriately.\n\n\nControlling Action Execution with highest_only=\nBy default, Pointblank only executes the action for the most severe threshold level that’s been exceeded. If you want actions for all exceeded thresholds to be executed, you can set highest_only=False:\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(\n            warning=\"Warning threshold exceeded in step {step}\",\n            error=\"Error threshold exceeded in step {step}\",\n            critical=\"Critical threshold exceeded in step {step}\",\n\n            # Execute all applicable actions ---\n            highest_only=False\n        ),\n    )\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nCritical threshold exceeded in step 1\nError threshold exceeded in step 1\nWarning threshold exceeded in step 1\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:11DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example, if all three thresholds are exceeded in a step, you’ll see all three messages printed, rather than just the critical one.\nThe default behavior (highest_only=True) helps prevent notification fatigue by limiting the number of actions executed when multiple thresholds are exceeded in the same validation step. For example, if a validation step fails with 60% of rows not passing, it would exceed ‘warning’, ‘error’, and ‘critical’ thresholds simultaneously. With highest_only=True, only the critical action would execute.\nYou might want to set highest_only=False when:\n\ndifferent threshold levels need to trigger different types of notifications (e.g., warnings to Slack, errors to email, critical to urgent notifications)\nyou need comprehensive logging of all severity levels for audit purposes\nyou’re building a dashboard that displays counts of issues at each severity level",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#using-multiple-actions-for-a-threshold",
    "href": "user-guide/actions.html#using-multiple-actions-for-a-threshold",
    "title": "Actions",
    "section": "Using Multiple Actions for a Threshold",
    "text": "Using Multiple Actions for a Threshold\nYou can specify multiple actions to be executed for a single threshold level by providing a list:\n\ndef send_notification():\n    print(\"📧 Notification sent to data team\")\n\ndef log_to_system():\n    print(\"📝 Issue logged in system\")\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(critical=0.15),\n\n        # Set multiple actions for the critical threshold exceedance ---\n        actions=pb.Actions(\n            critical=[\n                \"CRITICAL: Data validation failed\",  # First action: display message\n                send_notification,                   # Second action: call function\n                log_to_system                        # Third action: call another function\n            ]\n        ),\n    )\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nCRITICAL: Data validation failed\n📧 Notification sent to data team\n📝 Issue logged in system\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:11DuckDBWARNING—ERROR—CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    —\n    —\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nWhen providing a list of actions, they will be executed in sequence when the threshold is exceeded. This allows you to combine different types of actions such as displaying messages, sending notifications, and logging events.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#final-actions",
    "href": "user-guide/actions.html#final-actions",
    "title": "Actions",
    "section": "Final Actions",
    "text": "Final Actions\n\nCreating Final Actions\nWhen you need to execute actions after all validation steps are complete, Pointblank provides the FinalActions class. Unlike Actions which triggers on a per-step basis during the validation process, FinalActions executes after the entire validation is complete, giving you a way to respond to the overall validation results.\nHere’s how to use FinalActions:\n\ndef send_alert():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        print(f\"ALERT: Critical validation failures found in `{summary['tbl_name']}`\")\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        tbl_name=\"game_revenue\",\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n\n        # Set final actions to be executed after all interrogations ---\n        final_actions=pb.FinalActions(\n            \"Validation complete.\",  # 1. a string message\n            send_alert               # 2. a callable function\n        )\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.10)\n    .interrogate()\n)\n\nValidation complete.\nALERT: Critical validation failures found in `game_revenue`\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:11DuckDBgame_revenueWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    14400.72\n    5600.28\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example:\n\nWe define the function send_alert() that checks the validation summary for critical failures\nWe provide a simple string message \"Validation complete.\" that will print to the console\nBoth actions will execute in order after all validation steps have completed\n\nBecause the ‘critical’ threshold was exceeded in Step 2, we see the printed alert of send_alert() after the simple string message.\nFinalActions accepts any number of actions as positional arguments. Each argument can be:\n\nString: A message to be displayed in the console\nCallable: A function to be called with no arguments\nList of Strings/Callables: Multiple actions to execute in sequence\n\nAll actions will be executed in the order they are provided after all validation steps have completed.\n\n\nUsing get_validation_summary() in Final Actions\nWhen creating a callable function to use with FinalActions, you can access information about the overall validation results using the get_validation_summary() function. This gives you a dictionary with comprehensive information about the validation:\ndef comprehensive_report():\n    summary = pb.get_validation_summary()\n    print(f\"Validation Report for {summary['tbl_name']}:\")\n    print(f\"- Steps: {summary['n_steps']}\")\n    print(f\"- Passing steps: {summary['n_passing_steps']}\")\n    print(f\"- Failing steps: {summary['n_failing_steps']}\")\n\n    # Take additional actions based on results\n    if summary[\"n_failing_steps\"] &gt; 0:\n\n        # Create a Slack notification function ---\n        notify = pb.send_slack_notification(\n            webhook_url=\"https://hooks.slack.com/services/your/webhook/url\",\n            summary_msg=\"\"\"\n            🚨 *Validation Failure Alert*\n            • Table: {tbl_name}\n            • Failed Steps: {n_failing_steps} of {n_steps}\n            • Highest Severity: {highest_severity}\n            • Time: {time}\n            \"\"\",\n        )\n\n        # Execute the notification function\n        notify()\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        tbl_name=\"game_revenue\",\n        final_actions=pb.FinalActions(comprehensive_report),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .interrogate()\n)\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:11DuckDBgame_revenue\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nHere we used the send_slack_notification() function, which is available in Pointblank as a pre-built action. It can be used by itself in final_actions= but here it’s integrated into the user’s comprehensive_report() function to provide finer control with conditional logic.\n\n\nCombining Step-level and Final Actions\nYou can use both Actions and FinalActions together for comprehensive validation control:\n\ndef log_step_failure():\n    m = pb.get_action_metadata()\n    print(f\"Step {m['step']} failed with {m['level']}\")\n\n\ndef generate_summary():\n    summary = pb.get_validation_summary()\n    # Sum up total failed test units across all steps\n    total_failed = sum(summary[\"dict_n_failed\"].values())\n    # Sum up total test units across all steps\n    total_units = sum(summary[\"dict_n\"].values())\n    print(f\"Validation complete: {total_failed} failures out of {total_units} tests\")\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10),\n\n        # Set an action for each step (highest threshold exceeded) ---\n        actions=pb.Actions(default=log_step_failure),\n\n        # Set a final action to get a summary of the validation process ---\n        final_actions=pb.FinalActions(generate_summary),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .interrogate()\n)\n\nStep 2 failed with error\nValidation complete: 299 failures out of 4000 tests\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:11DuckDBWARNING0.05ERROR0.1CRITICAL—\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    —\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis approach allows you to:\n\nlog individual step failures during the validation process using Actions\ngenerate a comprehensive report after all validation steps are complete using FinalActions\n\nUsing both action types gives you fine-grained control over when and how notifications and other actions are triggered in your validation workflow.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#conclusion",
    "href": "user-guide/actions.html#conclusion",
    "title": "Actions",
    "section": "Conclusion",
    "text": "Conclusion\nActions provide a powerful mechanism for responding to data validation results in Pointblank. By combining threshold settings with appropriate actions, you can create sophisticated data quality workflows that:\n\nprovide immediate feedback through console messages\nexecute custom functions when validation thresholds are exceeded\ncustomize notifications based on severity levels\ngenerate comprehensive reports after validation is complete\nautomate responses to data quality issues\n\nThe flexible design of Actions and FinalActions allows you to start simple with basic console messages and gradually build up to complex validation workflows with conditional logic, custom reporting, and integrations with other systems like Slack, email, or logging services.\nWhen designing your validation strategy, consider leveraging both step-level actions for immediate responses and final actions for holistic reporting. This combination provides comprehensive control over your data validation process and helps ensure that data quality issues are detected, reported, and addressed efficiently.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/missing-vals-tbl.html",
    "href": "user-guide/missing-vals-tbl.html",
    "title": "Missing Values Reporting",
    "section": "",
    "text": "Sometimes values just aren’t there: they’re missing. This can either be expected or another thing to worry about. Either way, we can dig a little deeper if need be and use the missing_vals_tbl() function to generate a summary table that can elucidate how many values are missing, and roughly where.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Missing Values Reporting"
    ]
  },
  {
    "objectID": "user-guide/missing-vals-tbl.html#using-and-understanding-missing_vals_tbl",
    "href": "user-guide/missing-vals-tbl.html#using-and-understanding-missing_vals_tbl",
    "title": "Missing Values Reporting",
    "section": "Using and Understanding missing_vals_tbl()",
    "text": "Using and Understanding missing_vals_tbl()\nThe missing values table is arranged a lot like the column summary table (generated via the col_summary_tbl() function) in that columns of the input table are arranged as rows in the reporting table. Let’s use missing_vals_tbl() on the nycflights dataset, which has a lot of missing values:\n\nimport pointblank as pb\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"polars\")\n\npb.missing_vals_tbl(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values   46,595 in total\n  \n  \n    PolarsRows336,776Columns18\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    year\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    month\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    day\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    carrier\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    flight\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    tailnum\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    origin\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dest\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    air_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    distance\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    hour\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    minute\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 3367733678 – 6735467355 – 101031101032 – 134708134709 – 168385168386 – 202062202063 – 235739235740 – 269416269417 – 303093303094 – 336776\n  \n\n\n\n\n\n\n        \n\n\nThere are 18 columns in nycflights and they’re arranged down the missing values table as rows. To the right we see column headers indicating 10 columns that are row sectors. Row sectors are groups of rows and each sector contains a tenth of the total rows in the table. The leftmost sectors are the rows at the top of the table whereas the sectors on the right are closer to the bottom. If you’d like to know which rows make up each row sector, there are details on this in the table footer area (click the ROW SECTORS text or the disclosure triangle).\nNow that we know about row sectors, we need to understand the visuals here. A light blue cell indicates there are no (0) missing values within a given row sector of a column. For nycflights we can see that several columns have no missing values at all (i.e., the light blue color makes up the entire row in the missing values table).\nWhen there are missing values in a column’s row sector, you’ll be met with a grayscale color. The proportion of missing values corresponds to the color ramp from light gray to solid black. Interestingly, most of the columns that have missing values appear to be related to each other in terms of the extent of missing values (i.e., the appearance in the reporting table looks roughly the same, indicating a sort of systematic missingness). These columns are dep_time, dep_delay, arr_time, arr_delay, and air_time.\nThe odd column out with regard to the distribution of missing values is tailnum. By scanning the row and observing that the grayscale color values are all a little different we see that the degree of missingness of more variable and not related to the other columns containing missing values.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Missing Values Reporting"
    ]
  },
  {
    "objectID": "user-guide/missing-vals-tbl.html#missing-value-tables-from-the-other-datasets",
    "href": "user-guide/missing-vals-tbl.html#missing-value-tables-from-the-other-datasets",
    "title": "Missing Values Reporting",
    "section": "Missing Value Tables from the Other Datasets",
    "text": "Missing Value Tables from the Other Datasets\nThe small_table dataset has only 13 rows to it. Let’s use that as a Pandas DataFrame with missing_vals_tbl():\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\n\npb.missing_vals_tbl(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values   2 in total\n  \n  \n    PandasRows13Columns8\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    date_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    date\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    a\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    b\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    c\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    d\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    e\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    f\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 12 – 23 – 34 – 45 – 56 – 67 – 78 – 89 – 910 – 13\n  \n\n\n\n\n\n\n        \n\n\nIt appears that only column c has missing values. And since the table is very small in terms of row count, most of the row sectors contain only a single row.\nThe game_revenue dataset has no missing values. And this can be easily proven by using missing_vals_tbl() with it:\n\ngame_revenue = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\")\n\npb.missing_vals_tbl(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values ✓\n  \n  \n    DuckDBRows2,000Columns11\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    player_id\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    session_id\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    session_start\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    item_type\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    item_name\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    item_revenue\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    session_duration\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    start_day\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    acquisition\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    country\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 200201 – 400401 – 600601 – 800801 – 10001001 – 12001201 – 14001401 – 16001601 – 18001801 – 2000\n  \n\n\n\n\n\n\n        \n\n\nWe see nothing but light blue in this report! The header also indicates that there are no missing values by displaying a large green check mark (the other report tables provided a count of total missing values across all columns).",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Missing Values Reporting"
    ]
  },
  {
    "objectID": "user-guide/assertions.html",
    "href": "user-guide/assertions.html",
    "title": "Assertions",
    "section": "",
    "text": "In addition to validation steps that create reports, Pointblank provides assertions. This is a lightweight way to confirm data quality by raising exceptions when validation conditions aren’t met. Assertions are particularly useful in:",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "user-guide/assertions.html#basic-assertion-workflow",
    "href": "user-guide/assertions.html#basic-assertion-workflow",
    "title": "Assertions",
    "section": "Basic Assertion Workflow",
    "text": "Basic Assertion Workflow\nThe assertion workflow uses your familiar validation steps with assertion methods to check that validations meet your requirements:\n\nimport pointblank as pb\nimport polars as pl\n\n# Create sample data\nsample_data = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"value\": [10.5, 8.3, -2.1, 15.7, 7.2]\n})\n\n# Create a validation plan and assert that all steps pass\n(\n    pb.Validate(data=sample_data)\n    .col_vals_gt(columns=\"id\", value=0, brief=\"IDs must be positive\")\n    .col_vals_gt(columns=\"value\", value=-5, brief=\"Values should exceed -5\")\n\n    # Will automatically `interrogate()` and raise an AssertionError if any validation fails ---\n    .assert_passing()\n)\n\nThis simple pattern allows you to integrate data quality checks into your data pipelines. With it, you can create clear stopping points when data doesn’t meet specified criteria.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "user-guide/assertions.html#assertion-methods",
    "href": "user-guide/assertions.html#assertion-methods",
    "title": "Assertions",
    "section": "Assertion Methods",
    "text": "Assertion Methods\nPointblank offers two types of assertions:\n\nFull Passing Assertions: using assert_passing() to verify that every single test unit passes\nThreshold-Based Assertions: using assert_below_threshold() to verify that failure rates stay within acceptable thresholds\n\n\nassert_passing()\nThe assert_passing() method is the strictest form of assertion, requiring every single validation test unit to pass:\n\ntry:\n    (\n        pb.Validate(data=sample_data)\n        .col_vals_gt(columns=\"value\", value=0)\n\n        # Direct assertion: automatically interrogates ---\n        .assert_passing()\n    )\nexcept AssertionError as e:\n    print(\"AssertionError:\", str(e))\n\nAssertionError: The following assertions failed:\n- Step 1: Expect that values in `value` should be &gt; `0`.\n\n\n\n\nassert_below_threshold()\nThe assert_below_threshold() method is more flexible as it allows some failures as long as they stay below specified threshold levels. Pointblank uses three severity thresholds that increase in order of seriousness:\n\n‘warning’ (least severe): the first threshold that gets triggered when failures exceed this level\n‘error’ (more severe): the middle threshold indicating more serious data quality issues\n‘critical’ (most severe): the highest threshold indicating critical data quality problems\n\n\n# Create a two-column DataFrame for this example\ntbl_pl = pl.DataFrame({\n    \"a\": [4, 6, 9, 7, 12, 8, 7, 12, 10, 7],\n    \"b\": [9, 8, 10, 5, 10, 9, 14, 6, 6, 8],\n\n})\n\n# Set thresholds: warning=0.2 (20%), error=0.3 (30%), critical=0.4 (40%)\nvalidation = (\n    pb.Validate(data=tbl_pl, thresholds=(0.2, 0.3, 0.4))\n    .col_vals_gt(columns=\"b\", value=5)   # 1/10 failing (10% failure rate)\n    .col_vals_lt(columns=\"a\", value=11)  # 2/10 failing (20% failure rate)\n    .col_vals_ge(columns=\"b\", value=8)   # 3/10 failing (30% failure rate)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:52PolarsWARNING0.2ERROR0.3CRITICAL0.4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    b\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    10\n    90.90\n    10.10\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    10\n    80.80\n    20.20\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #EBBC14\n    3\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    b\n    8\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    10\n    70.70\n    30.30\n    ●\n    ●\n    ○\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation report above visually indicates threshold levels with colored circles:\n\ngray circles in the W column indicate the ‘warning’ threshold\nyellow circles in the E column indicate the ‘error’ threshold\nred circles in the C column indicate the ‘critical’ threshold\n\nThis won’t pass the assert_below_threshold() assertion for the ‘error’ level because step 3 exceeds this threshold (30% failure rate matches the error threshold):\n\ntry:\n    validation.assert_below_threshold(level=\"error\")\nexcept AssertionError as e:\n    print(\"AssertionError:\", str(e))\n\nAssertionError: The following steps exceeded the error threshold level:\nStep 3: Expect that values in `b` should be &gt;= `8`.\n\n\nWe can check against the ‘error’ threshold for specific steps with the i= parameter:\n\nvalidation.assert_below_threshold(level=\"error\", i=[1, 2])\n\nThis passes because the highest threshold exceeded in steps 1 and 2 is ‘warning’.\nThe assert_below_threshold() method takes these parameters:\n\nlevel=: threshold level to check against (\"warning\", \"error\", or \"critical\")\ni=: optional specific step number(s) to check\nmessage=: optional custom error message\n\nThis is particularly useful when:\n\nworking with real-world data where some percentage of failures is acceptable\nimplementing different severity levels for data quality rules\ngradually improving data quality with stepped thresholds\n\n\n\n\n\n\n\nNote\n\n\n\nAssertion methods like assert_passing() and assert_below_threshold() will automatically call interrogate() if needed, so you don’t have to explicitly include this step when using assertions directly.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "user-guide/assertions.html#using-status-check-methods",
    "href": "user-guide/assertions.html#using-status-check-methods",
    "title": "Assertions",
    "section": "Using Status Check Methods",
    "text": "Using Status Check Methods\nIn addition to assertion methods that raise exceptions, Pointblank provides status check methods that return boolean values:\n\nall_passed()\nThe all_passed() method will return True only if every single test unit in every validation step passed:\n\nvalidation = (\n    pb.Validate(data=sample_data)\n    .col_vals_gt(columns=\"value\", value=0)\n    .interrogate()\n)\n\nif not validation.all_passed():\n    print(\"Validation failed: some values are not positive\")\n\nValidation failed: some values are not positive\n\n\n\n\nwarning(), error(), and critical()\nThe methods (warning(), error(), and critical()) return information about whether validation steps exceeded that specific threshold level.\nWhile assertion methods raise exceptions to halt execution when thresholds are exceeded, these status methods give you fine-grained control to implement custom logic based on different validation quality levels.\n\nvalidation = (\n    pb.Validate(data=sample_data, thresholds=(0.05, 0.10, 0.20))\n    .col_vals_gt(columns=\"value\", value=0)  # Some values are negative\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:52PolarsWARNING0.05ERROR0.1CRITICAL0.2\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    value\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    40.80\n    10.20\n    ●\n    ●\n    ●\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe warning() method returns a dictionary mapping step numbers to boolean values. A True value means that step exceeds the warning threshold:\n\n# Get dictionary of warning status for each step\nwarning_status = validation.warning()\nprint(f\"Warning status: {warning_status}\")  # {1: True} means step 1 exceeds warning threshold\n\nWarning status: {1: True}\n\n\nYou can check a specific step using the i= parameter, and get a single boolean with scalar=True:\n\n# Check error threshold for specific step\nhas_errors = validation.error(i=1, scalar=True)\n\nif has_errors:\n    print(\"Step 1 exceeded the error threshold.\")\n\nStep 1 exceeded the error threshold.\n\n\nSimilarly, we can check if any steps exceed the ‘critical’ threshold:\n\n# Check against critical threshold\ncritical_status = validation.critical()\nprint(f\"Critical status: {critical_status}\")\n\nCritical status: {1: True}\n\n\nThese methods are particularly useful for:\n\nConditional logic: taking different actions based on threshold severity\nReporting: generating summary reports about validation quality\nMonitoring: tracking data quality trends over time\nGraceful degradation: implementing fallback logic when quality decreases\n\nEach method has these options:\n\nwithout parameters: returns a dictionary mapping step numbers to boolean status values\nwith i=: check specific step(s)\nwith scalar=True: return a single boolean instead of a dictionary (when checking a specific step)\n\nWhile assertion methods raise exceptions to halt execution when thresholds are exceeded, these methods give you fine-grained control to implement custom logic based on different validation quality levels.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "user-guide/assertions.html#customizing-error-messages",
    "href": "user-guide/assertions.html#customizing-error-messages",
    "title": "Assertions",
    "section": "Customizing Error Messages",
    "text": "Customizing Error Messages\nYou can provide custom error messages when assertions fail to make them more meaningful in your specific workflow context:\n\n# Create a validation with potential failures\nvalidation = (\n    pb.Validate(data=sample_data, thresholds=(0.2, 0.3, 0.4))\n    .col_vals_gt(columns=\"value\", value=0)\n    .interrogate()\n)\n\n# Display the validation results\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:52PolarsWARNING0.2ERROR0.3CRITICAL0.4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    value\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    40.80\n    10.20\n    ●\n    ○\n    ○\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWhen you need to customize the error message that appears when an assertion fails, use the message= parameter:\n\ntry:\n    # Custom message for threshold assertion\n    validation.assert_below_threshold(\n        level=\"warning\",\n        message=\"Data quality too low for processing!\"\n    )\nexcept AssertionError as e:\n    print(f\"Custom handling of failure: {e}\")\n\nCustom handling of failure: Data quality too low for processing!\n\n\nDescriptive error messages are essential in production systems where multiple team members might need to interpret validation failures. The custom message lets you provide context appropriate to your specific workflow or data pipeline stage.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "user-guide/assertions.html#combining-assertions-with-actions",
    "href": "user-guide/assertions.html#combining-assertions-with-actions",
    "title": "Assertions",
    "section": "Combining Assertions with Actions",
    "text": "Combining Assertions with Actions\nActions and assertions serve complementary but distinct purposes in data validation workflows:\n\nActions trigger during validation but shouldn’t raise errors (as this would halt report generation)\nAssertions are designed to raise errors based on specific conditions, making them ideal for flow control after validation completes\n\nHere’s a simplified example showing how to use them together. The print statements simulate logging or monitoring that would be valuable in production data pipelines:\n\n# Define a simple action function (won't raise errors)\ndef notify_quality_issue(message=\"Data quality issue detected\"):\n    print(f\"ACTION TRIGGERED: {message}\")\n\n# Create data with known failures\nproblem_data = pl.DataFrame({\n    \"id\": [1, 2, 3, -4, 5],  # One negative ID\n    \"value\": [10.5, 8.3, -2.1, 15.7, 7.2]  # One negative value\n})\n\n# First use actions for automated responses during validation\nprint(\"Running validation with actions...\")\nvalidation = (\n    pb.Validate(data=problem_data, thresholds=(0.1, 0.2, 0.3))\n    .col_vals_gt(\n        columns=\"id\", value=0,\n        brief=\"IDs must be positive\",\n        actions=pb.Actions(warning=notify_quality_issue)\n    )\n    .interrogate()  # Actions trigger here but won't stop report generation\n)\n\n# Then use assertions after validation for workflow control\nprint(\"\\nNow using assertion for flow control...\")\ntry:\n    validation.assert_below_threshold(level=\"warning\")\n    print(\"This line won't execute if the assertion fails\")\nexcept AssertionError as e:\n    print(f\"Validation failed threshold check: {e}\")\n    print(\"Implementing fallback process...\")\n\nRunning validation with actions...\nACTION TRIGGERED: Data quality issue detected\n\nNow using assertion for flow control...\nValidation failed threshold check: The following steps exceeded the warning threshold level:\nStep 1: Expect that values in `id` should be &gt; `0`.\nImplementing fallback process...\n\n\nThis approach gives you the best of both worlds:\n\nActions provide immediate notification during validation without interrupting the process\nAssertions control workflow execution after validation when important thresholds are exceeded\n\nThis pattern works well in data pipelines where you want both: (1) automated responses during validation and (2) clear decision points after validation is complete.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "user-guide/assertions.html#best-practices-for-assertions",
    "href": "user-guide/assertions.html#best-practices-for-assertions",
    "title": "Assertions",
    "section": "Best Practices for Assertions",
    "text": "Best Practices for Assertions\nWhen using assertions in your data workflows, consider these best practices:\n\nChoose the right assertion type:\n\nuse assert_passing() for critical validations where any failure is unacceptable\nuse assert_below_threshold() for validations where some failure rate is acceptable\n\nSet appropriate thresholds that match your data quality requirements:\n# Example threshold strategy\nvalidation = pb.Validate(\n    data=sample_data,\n    # warning at 1%, error at 5%, critical at 10%\n    thresholds=pb.Thresholds(warning=0.01, error=0.05, critical=0.10)\n)\nUse a graduated approach to validation severity:\n# Critical validations: must be perfect\nvalidation_1.assert_passing()\n\n# Important validations: must be below error threshold\nvalidation_2.assert_below_threshold(level=\"error\")\n\n# Monitor-only validations: check warning status\nwarning_status = validation_3.warning()\nPlacement in pipelines: place assertions at critical points where data quality is essential\nError handling: wrap assertions in try-except blocks for better error handling in production systems\nCombine with reporting: use both assertions and reporting approaches for comprehensive quality control",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "user-guide/assertions.html#conclusion",
    "href": "user-guide/assertions.html#conclusion",
    "title": "Assertions",
    "section": "Conclusion",
    "text": "Conclusion\nPointblank’s assertion methods give you flexible options for enforcing data quality requirements:\n\nassert_passing() for strict validation where every test unit must pass\nassert_below_threshold() for more flexible validation where some failures are tolerable\nStatus methods (warning(), error(), and critical()) for programmatic threshold checking\n\nBy using these assertion methods appropriately, you can build robust data pipelines with different levels of quality enforcement (from strict validation of critical data properties to more lenient checks for less critical aspects). This graduated approach to data quality helps create systems that are both reliable and practical in real-world data environments.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Assertions"
    ]
  },
  {
    "objectID": "blog/intro-pointblank/index.html",
    "href": "blog/intro-pointblank/index.html",
    "title": "Introducing Pointblank",
    "section": "",
    "text": "If you have tabular data (and who doesn’t?) this is the package for you! I’ve long been interested in data quality and so I’ve spent a lot of time building tooling that makes it possible to perform data quality checks. And there’s so many reasons to care about data quality. If I were to put down just one good reason for why data quality is worth your time it is because having good data quality strongly determines the quality of decisions.\nHaving the ability to distinguish bad data from good data is the first step in solving DQ issues, and the sustained practice of doing data validation will guard against intrusions of poor-quality data. Pointblank has been designed to really help here. Though it’s a fairly new package it is currently quite capable. And it’s available in PyPI, so you can install it by using:"
  },
  {
    "objectID": "blog/intro-pointblank/index.html#how-pointblank-transforms-your-data-validation-workflow",
    "href": "blog/intro-pointblank/index.html#how-pointblank-transforms-your-data-validation-workflow",
    "title": "Introducing Pointblank",
    "section": "How Pointblank Transforms Your Data Validation Workflow",
    "text": "How Pointblank Transforms Your Data Validation Workflow\nWhat sets Pointblank apart is its intuitive, expressive approach to data validation. Rather than writing dozens of ad-hoc checks scattered throughout your codebase, Pointblank lets you define a comprehensive validation plan with just a few lines of code. The fluent API makes your validation intentions crystal clear, whether you’re ensuring numeric values fall within expected ranges, text fields match specific patterns, or relationships between columns remain consistent.\nBut say you find problems. What are you gonna do about it? Well, Pointblank wants to help at not just finding problems but helping you understand them. When validation failures occur, the detailed reporting capabilities (in the form of beautiful, sharable tables) show you exactly where issues are. Right down to the specific rows and columns. This transforms data validation from a binary pass/fail exercise into a super-insightful diagnostic tool.\n\nHere’s the the best part: Pointblank is designed to work with your existing data stack. Whether you’re using Polars, Pandas, DuckDB, or other database systems, Pointblank tries hard to integrate without forcing you to change your workflow. We also have international spoken language support for reporting, meaning that validation reports can be localized to your team’s preferred language. This making data quality accessible to everyone in your organization (like a team sport!).\n\nAlright! Let’s look at a few demonstrations of Pointblank’s capabilities for data validation."
  },
  {
    "objectID": "blog/intro-pointblank/index.html#the-data-validation-workflow",
    "href": "blog/intro-pointblank/index.html#the-data-validation-workflow",
    "title": "Introducing Pointblank",
    "section": "The Data Validation Workflow",
    "text": "The Data Validation Workflow\nLet’s get right to performing a basic check of a Polars DataFrame. We’ll make use of the included small_table dataset.\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n\nvalidation_1 = (\n    pb.Validate(\n        data=small_table,\n        tbl_name=\"small_table\",\n        label=\"Example Validation\"\n    )\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\n\nvalidation_1\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [0, 5000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^[0-9]-[a-z]{3}-[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:18:42 UTC&lt; 1 s2025-05-23 02:18:42 UTC\n  \n\n\n\n\n\n\n        \n\n\nThere’s a lot to take in here so let’s break down the code first! Note these three key pieces:\n\nthe Validate(data=...) argument takes a DataFrame (or database table) that you want to validate\nthe methods starting with col_* specify validation steps that run on specific columns\nthe interrogate() method executes the validation plan on the table (it’s the finishing step)\n\nThis common pattern is used in a validation workflow, where Validate and interrogate() bookend a validation plan generated through calling validation methods.\nNow, onto the result: it’s a table! Naturally, we’re using the awesome Great Tables package here in Pointblank to really give you the goods on how the validation went down. Each row in this reporting table represents a single validation step (one for each invocation of a col_vals_*() validation method). Generally speaking, the left side of the validation report tables outlines the key validation rules, and the right side provides the results of each validation step.\nWe tried to keep it simple in principle, but a lot of useful information can be packed into this validation table. Here’s a diagram that describes a few of the important parts of the validation report table:\n\nAll of those numbers under the UNITS, PASS, and FAIL columns have to do with test units, a measure of central importance in Pointblank. Each validation step will execute a type of validation test on the target table. For example, a col_vals_lt() validation step can test that each value in a column is less than a specified number. The key finding that’s reported as a result of this test is the number of test units that pass or fail. This little diagram explains what those numbers mean:\n\nFailing test units can be tied to threshold levels, which can provide a better indication of whether failures should raise some basic awareness or spur you into action. Here’s a validation workflow that sets three failure threshold levels that signal the severity of data quality problems:\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation_2 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\"),\n        tbl_name=\"game_revenue\",\n        label=\"Data validation with threshold levels set.\",\n        thresholds=pb.Thresholds(warning=1, error=20, critical=0.10),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"^[A-Z]{12}[0-9]{3}$\")        # STEP 1\n    .col_vals_gt(columns=\"session_duration\", value=5)                           # STEP 2\n    .col_vals_ge(columns=\"item_revenue\", value=0.02)                            # STEP 3\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])                    # STEP 4\n    .col_vals_in_set(                                                           # STEP 5\n        columns=\"acquisition\",\n        set=[\"google\", \"facebook\", \"organic\", \"crosspromo\", \"other_campaign\"]\n    )\n    .col_vals_not_in_set(columns=\"country\", set=[\"Mongolia\", \"Germany\"])        # STEP 6\n    .col_vals_between(                                                          # STEP 7\n        columns=\"session_duration\",\n        left=10, right=50,\n        pre = lambda df: df.select(pl.median(\"session_duration\"))\n    )\n    .rows_distinct(columns_subset=[\"player_id\", \"session_id\", \"time\"])          # STEP 8\n    .row_count_match(count=2000)                                                # STEP 9\n    .col_exists(columns=\"start_day\")                                            # STEP 10\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Data validation with threshold levels set.Polarsgame_revenueWARNING1ERROR20CRITICAL0.1\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    ^[A-Z]{12}[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #EBBC14\n    3\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    item_revenue\n    0.02\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19410.97\n    590.03\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    5\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    acquisition\n    google, facebook, organic, crosspromo, other_campaign\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19750.99\n    250.01\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #FF3300\n    6\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    country\n    Mongolia, Germany\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17750.89\n    2250.11\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    session_duration\n    [10, 50]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    8\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    player_id, session_id, time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19780.99\n    220.01\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    9\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:18:42 UTC&lt; 1 s2025-05-23 02:18:42 UTC\n  \n\n\n\n\n\n\n        \n\n\nThis data validation makes use of the many validation methods available in the library. Because thresholds have been set at the Validate(thresholds=) parameter, we can now see where certain validation steps have greater amounts of failures. Any validation steps with green indicators passed with flying colors, whereas: (1) gray indicates the ‘warning’ condition was met (at least one test unit failing), (2) yellow is for the ‘error’ condition (20 or more test units failing), and (3) red means ‘critical’ and that’s tripped when 10% of all test units are failing ones.\nReporting tables are essential to the package and they help communicate what went wrong (or well) in a validation workflow. Now let’s look at some additional reporting that Pointblank can give you to better understand where things might’ve gone wrong."
  },
  {
    "objectID": "blog/intro-pointblank/index.html#reporting-for-individual-validation-steps",
    "href": "blog/intro-pointblank/index.html#reporting-for-individual-validation-steps",
    "title": "Introducing Pointblank",
    "section": "Reporting for Individual Validation Steps",
    "text": "Reporting for Individual Validation Steps\nThe second validation step of the previous data validation showed 18 failing test units. That translates to 18 spots in a 2,000 row DataFrame where a data quality assertion failed. We often would like to know exactly what that failing data is; it’s usually the next step toward addressing data quality issues.\nPointblank offers a method that gives you a tabular report on a specific step: get_step_report(). The previous tables you’ve seen (the validation report table) dealt with providing a summary of all validation steps. In contrast, a focused report on a single step can help to get to the heart of a data quality issue. Here’s how that looks for Step 2:\n\nvalidation_2.get_step_report(i=2)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION session_duration &gt; 518 / 2000 TEST UNIT FAILURES IN COLUMN 8 EXTRACT OF FIRST 10 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    620\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:25:18+00:00\n    iap\n    offer4\n    17.991\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    621\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:26:24+00:00\n    iap\n    offer5\n    26.09\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    622\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:28:36+00:00\n    ad\n    ad_15sec\n    0.53\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n\n\n\n\n\n\n        \n\n\nThis report provides the 18 rows where the failure occurred. If you scroll the table to the right you’ll see the column that underwent testing (session_duration) is highlighted in red. All of these values are 5.0 or less, which is in violation of the assertion (in the header) that session_duration &gt; 5.\nThese types of bespoke reports are useful for finding a needle in a haystack. Another good use for a step report is when validating a table schema. Using the col_schema_match() validation method with a table schema prepared with the Schema class allows us to verify our understanding of the table structure. Here is a validation that performs a schema validation with the small_table dataset prepared as a DuckDB table:\n\nimport pointblank as pb\n\n# Create a schema for the target table (`small_table` as a DuckDB table)\nschema = pb.Schema(\n    columns=[\n        (\"date_time\", \"timestamp(6)\"),\n        (\"dates\", \"date\"),\n        (\"a\", \"int64\"),\n        (\"b\",),\n        (\"c\",),\n        (\"d\", \"float64\"),\n        (\"e\", [\"bool\", \"boolean\"]),\n        (\"f\", \"str\"),\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform a schema check\nvalidation_3 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"duckdb\"),\n        tbl_name=\"small_table\",\n        label=\"Schema check\"\n    )\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation_3\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Schema checkDuckDBsmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:18:44 UTC&lt; 1 s2025-05-23 02:18:44 UTC\n  \n\n\n\n\n\n\n        \n\n\nThis step fails, but the validation report table doesn’t tell us how (or where). Using `get_step_report() will show us what the underlying issues are:\n\nvalidation_3.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✗COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    date_time\n    timestamp\n    1\n    date_time\n    ✓\n    timestamp(6)\n    ✗\n  \n  \n    2\n    date\n    date\n    2\n    dates\n    ✗\n    date\n    —\n  \n  \n    3\n    a\n    int64\n    3\n    a\n    ✓\n    int64\n    ✓\n  \n  \n    4\n    b\n    string\n    4\n    b\n    ✓\n    —\n    \n  \n  \n    5\n    c\n    int64\n    5\n    c\n    ✓\n    —\n    \n  \n  \n    6\n    d\n    float64\n    6\n    d\n    ✓\n    float64\n    ✓\n  \n  \n    7\n    e\n    boolean\n    7\n    e\n    ✓\n    bool | boolean\n    ✓\n  \n  \n    8\n    f\n    string\n    8\n    f\n    ✓\n    str\n    ✗\n  \n\n  \n  \n  \n    Supplied Column Schema:[('date_time', 'timestamp(6)'), ('dates', 'date'), ('a', 'int64'), ('b',), ('c',), ('d', 'float64'), ('e', ['bool', 'boolean']), ('f', 'str')]\n  \n\n\n\n\n\n\n        \n\n\nThe step report here shows the target table’s schema on the left side and the expectation of the schema on the right side. There appears to be two problems with our supplied schema:\n\nthe second column is actually date instead of dates\nthe dtype of the f column is \"string\" and not \"str\"\n\nThe convenience of this step report means we only have to look at one display of information, rather than having to collect up the individual pieces and make careful comparisons."
  },
  {
    "objectID": "blog/intro-pointblank/index.html#much-more-in-store",
    "href": "blog/intro-pointblank/index.html#much-more-in-store",
    "title": "Introducing Pointblank",
    "section": "Much More in Store",
    "text": "Much More in Store\nPointblank tries really hard to make it easy for you to test your data. All sorts of input tables are supported since we integrate with the brilliant Narwhals and Ibis libraries. And even through the project has only started four months ago, we already have an extensive catalog of well-tested validation methods.\nWe care a great deal about documentation so much recent effort has been placed on getting the User Guide written. We hope it provides for gentle introduction to the major features of the library. If you want some quick examples to get your imagination going, check out our gallery of examples.\nWe really care about what you want in a validation package, so talk to us :) We just started a Discord so feel free to hop on and ask us anything. Alternatively, we always like to get issues so don’t be shy in letting us know how we could improve!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nMay 20, 2025\n\n\nOverhauling Pointblank’s User Guide\n\n\nRich Iannone and Michael Chow\n\n\n\n\n\n\nMay 2, 2025\n\n\nLevel Up Your Data Validation with Actions and FinalActions\n\n\nRich Iannone\n\n\n\n\n\n\nApr 4, 2025\n\n\nIntroducing Pointblank\n\n\nRich Iannone\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "reference/last_n.html",
    "href": "reference/last_n.html",
    "title": "last_n",
    "section": "",
    "text": "last_n(n, offset=0)\nSelect the last n columns in the column list.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The last_n() selector function can be used to select n columns positioned at the end of the column list. So if the set of table columns consists of\n[age, rev_01, rev_02, profit_01, profit_02]\nand you want to validate the last two columns, you can use columns=last_n(2). This will select the profit_01 and profit_02 columns and a validation step will be created for each.\nThe offset= parameter can be used to skip a certain number of columns from the end of the column list. So if you want to select the third and fourth columns from the end, you can use columns=last_n(2, offset=2)."
  },
  {
    "objectID": "reference/last_n.html#parameters",
    "href": "reference/last_n.html#parameters",
    "title": "last_n",
    "section": "Parameters",
    "text": "Parameters\n\nn : int\n\nThe number of columns to select from the end of the column list. Should be a positive integer value. If n is greater than the number of columns in the table, all columns will be selected.\n\noffset : int = 0\n\nThe offset from the end of the column list. The default is 0. If offset is greater than the number of columns in the table, no columns will be selected."
  },
  {
    "objectID": "reference/last_n.html#returns",
    "href": "reference/last_n.html#returns",
    "title": "last_n",
    "section": "Returns",
    "text": "Returns\n\n : LastN\n\nA LastN object, which can be used to select the last n columns."
  },
  {
    "objectID": "reference/last_n.html#relevant-validation-methods-where-last_n-can-be-used",
    "href": "reference/last_n.html#relevant-validation-methods-where-last_n-can-be-used",
    "title": "last_n",
    "section": "Relevant Validation Methods where last_n() can be Used",
    "text": "Relevant Validation Methods where last_n() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe last_n() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/last_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/last_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "last_n",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe last_n() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names starting with “rev” along with the last two columns, you can use the last_n() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(last_n(2) | starts_with(\"rev\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/last_n.html#examples",
    "href": "reference/last_n.html#examples",
    "title": "last_n",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, paid_2021, paid_2022, paid_2023, and paid_2024 and we’d like to validate that the values in the last four columns are greater than 10. We can use the last_n() column selector function to specify that the last four columns in the table are the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.last_n(4), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps. The values in all those columns were all greater than 10.\nWe can also use the last_n() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select the last four columns but also omit those columns that end with \"2023\", we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.last_n(4) - pb.ends_with(\"2023\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get three validation steps, one for paid_2021, paid_2022, and paid_2024."
  },
  {
    "objectID": "reference/matches.html",
    "href": "reference/matches.html",
    "title": "matches",
    "section": "",
    "text": "matches(pattern, case_sensitive=False)\nSelect columns that match a specified regular expression pattern.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The matches() selector function can be used to select one or more columns matching a provided regular expression pattern. So if the set of table columns consists of\n[rev_01, rev_02, profit_01, profit_02, age]\nand you want to validate columns that have two digits at the end of the name, you can use columns=matches(r\"[0-9]{2}$\"). This will select the rev_01, rev_02, profit_01, and profit_02 columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using matches() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/matches.html#parameters",
    "href": "reference/matches.html#parameters",
    "title": "matches",
    "section": "Parameters",
    "text": "Parameters\n\npattern : str\n\nThe regular expression pattern that the column name should match.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/matches.html#returns",
    "href": "reference/matches.html#returns",
    "title": "matches",
    "section": "Returns",
    "text": "Returns\n\n : Matches\n\nA Matches object, which can be used to select columns that match the specified pattern."
  },
  {
    "objectID": "reference/matches.html#relevant-validation-methods-where-matches-can-be-used",
    "href": "reference/matches.html#relevant-validation-methods-where-matches-can-be-used",
    "title": "matches",
    "section": "Relevant Validation Methods where matches() can be Used",
    "text": "Relevant Validation Methods where matches() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe matches() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/matches.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/matches.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "matches",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe matches() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that have the text starting with five digits and end with \"_id\", you can use the matches() and ends_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(matches(r\"^[0-9]{5}\") & ends_with(\"_id\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/matches.html#examples",
    "href": "reference/matches.html#examples",
    "title": "matches",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, id_old, new_identifier, and pay_2021 and we’d like to validate that text values in columns having \"id\" or \"identifier\" in the name have a specific syntax. We can use the matches() column selector function to specify the columns that match the pattern.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"id_old\": [\"ID0021\", \"ID0032\", \"ID0043\"],\n        \"new_identifier\": [\"ID9054\", \"ID9065\", \"ID9076\"],\n        \"pay_2021\": [16.32, 16.25, 15.75],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=pb.matches(\"id|identifier\"), pattern=r\"ID[0-9]{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    id_old\n    ID[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    new_identifier\n    ID[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for id_old and one for new_identifier. The values in both columns all match the pattern \"ID[0-9]{4}\".\nWe can also use the matches() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that contain \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.contains(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2023_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2024_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay_total and one for 2024_pay_total."
  },
  {
    "objectID": "reference/col_summary_tbl.html",
    "href": "reference/col_summary_tbl.html",
    "title": "col_summary_tbl",
    "section": "",
    "text": "col_summary_tbl(data, tbl_name=None)\nGenerate a column-level summary table of a dataset.\nThe col_summary_tbl() function generates a summary table of a dataset, focusing on providing column-level information about the dataset. The summary includes the following information:\nThe summary table is returned as a GT object, which can be displayed in a notebook or saved to an HTML file."
  },
  {
    "objectID": "reference/col_summary_tbl.html#parameters",
    "href": "reference/col_summary_tbl.html#parameters",
    "title": "col_summary_tbl",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to summarize, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name=."
  },
  {
    "objectID": "reference/col_summary_tbl.html#returns",
    "href": "reference/col_summary_tbl.html#returns",
    "title": "col_summary_tbl",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the column-level summaries of the table."
  },
  {
    "objectID": "reference/col_summary_tbl.html#supported-input-table-types",
    "href": "reference/col_summary_tbl.html#supported-input-table-types",
    "title": "col_summary_tbl",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using col_summary_tbl() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/col_summary_tbl.html#examples",
    "href": "reference/col_summary_tbl.html#examples",
    "title": "col_summary_tbl",
    "section": "Examples",
    "text": "Examples\nIt’s easy to get a column-level summary of a table using the col_summary_tbl() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n\npb.col_summary_tbl(data=small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    date_timeDatetime(time_unit='us', time_zone=None)\n    0 0.00\n    12 0.92\n    —\n    —\n     2016-01-04 00:32:00 – 2016-01-30 11:23:00\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    2\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dateDate\n    0 0.00\n    11 0.85\n    —\n    —\n     2016-01-04 – 2016-01-30\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    3\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    aInt64\n    0 0.00\n    7 0.54\n    3.77\n    2.09\n    1.00\n    1.60\n    2.00\n    3.00\n    4.00\n    7.40\n    8.00\n    2.00\n  \n  \n    4\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    bString\n    0 0.00\n    12 0.92\n    9.00SL\n    0.00SL\n    9SL\n    —\n    —\n    9SL\n    —\n    —\n    9SL\n    —\n  \n  \n    5\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    cInt64\n    2 0.15\n    6 0.46\n    5.73\n    2.72\n    2.00\n    2.50\n    3.00\n    7.00\n    8.00\n    9.00\n    9.00\n    5.00\n  \n  \n    6\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dFloat64\n    0 0.00\n    12 0.92\n    2305\n    2631\n    108\n    214\n    838\n    1036\n    3291\n    6335\n    10000\n    2453\n  \n  \n    7\n    \n    boolean\n    \n        \n            \n            \n                \n            \n            \n                \n            \n            \n        \n    \n\n    eBoolean\n    0 0.00\n    T 0.61F 0.39\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n  \n    8\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    fString\n    0 0.00\n    3 0.23\n    3.46SL\n    0.52SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    4SL\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis table used above was a Polars DataFrame, but the col_summary_tbl() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\npb.col_summary_tbl(data=nycflights, tbl_name=\"nycflights\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBnycflightsRows336,776Columns18\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    yearint64\n    0 0.00\n    1&lt;0.01\n    2013\n    0.00\n    2013\n    2013\n    2013\n    2013\n    2013\n    2013\n    2013\n    0\n  \n  \n    2\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    monthint64\n    0 0.00\n    12&lt;0.01\n    6.55\n    3.41\n    1.00\n    1.00\n    4.00\n    7.00\n    9.28\n    12.0\n    12.0\n    6.00\n  \n  \n    3\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dayint64\n    0 0.00\n    31&lt;0.01\n    15.7\n    8.77\n    1.00\n    2.00\n    8.06\n    16.0\n    23.0\n    29.4\n    31.0\n    15.0\n  \n  \n    4\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dep_timeint64\n    8255 0.02\n    1317&lt;0.01\n    1349\n    488\n    1.00\n    623\n    904\n    1401\n    1744\n    2111\n    2400\n    837\n  \n  \n    5\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    sched_dep_timeint64\n    0 0.00\n    1021&lt;0.01\n    1344\n    467\n    106\n    630\n    909\n    1359\n    1729\n    2051\n    2359\n    823\n  \n  \n    6\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dep_delayint64\n    8255 0.02\n    526&lt;0.01\n    12.6\n    40.2\n    −43.0\n    −9.00\n    −5.00\n    −2.00\n    10.7\n    88.3\n    1301\n    16.0\n  \n  \n    7\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    arr_timeint64\n    8713 0.03\n    1410&lt;0.01\n    1502\n    533\n    1.00\n    735\n    1096\n    1535\n    1941\n    2248\n    2400\n    836\n  \n  \n    8\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    sched_arr_timeint64\n    0 0.00\n    1163&lt;0.01\n    1536\n    497\n    1.00\n    816\n    1124\n    1556\n    1945\n    2247\n    2359\n    821\n  \n  \n    9\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    arr_delayint64\n    9430 0.03\n    576&lt;0.01\n    6.90\n    44.6\n    −86.0\n    −32.2\n    −17.0\n    −5.00\n    13.8\n    91.2\n    1272\n    31.0\n  \n  \n    10\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    carrierstring\n    0 0.00\n    16&lt;0.01\n    2.00SL\n    0.00SL\n    2SL\n    —\n    —\n    2SL\n    —\n    —\n    2SL\n    —\n  \n  \n    11\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    flightint64\n    0 0.00\n    3844 0.01\n    1972\n    1632\n    1.00\n    93.7\n    558\n    1496\n    3465\n    4703\n    8500\n    2912\n  \n  \n    12\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    tailnumstring\n    2512&lt;0.01\n    4042 0.01\n    6.00SL\n    0.07SL\n    5SL\n    —\n    —\n    6SL\n    —\n    —\n    6SL\n    —\n  \n  \n    13\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    originstring\n    0 0.00\n    3&lt;0.01\n    3.00SL\n    0.00SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    3SL\n    —\n  \n  \n    14\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    deststring\n    0 0.00\n    105&lt;0.01\n    3.00SL\n    0.00SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    3SL\n    —\n  \n  \n    15\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    air_timeint64\n    9430 0.03\n    508&lt;0.01\n    151\n    93.7\n    20.0\n    40.0\n    82.3\n    129\n    191\n    339\n    695\n    110\n  \n  \n    16\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    distanceint64\n    0 0.00\n    214&lt;0.01\n    1040\n    733\n    17.0\n    198\n    509\n    872\n    1389\n    2477\n    4983\n    887\n  \n  \n    17\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    hourint64\n    0 0.00\n    20&lt;0.01\n    13.2\n    4.66\n    1\n    6\n    9\n    13\n    17\n    20\n    23\n    8\n  \n  \n    18\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    minuteint64\n    0 0.00\n    60&lt;0.01\n    26.2\n    19.3\n    0.00\n    0.00\n    7.94\n    29.0\n    43.4\n    57.8\n    59.0\n    36.0"
  },
  {
    "objectID": "reference/get_column_count.html",
    "href": "reference/get_column_count.html",
    "title": "get_column_count",
    "section": "",
    "text": "get_column_count(data)\nGet the number of columns in a table.\nThe get_column_count() function returns the number of columns in a table. The function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.)."
  },
  {
    "objectID": "reference/get_column_count.html#parameters",
    "href": "reference/get_column_count.html#parameters",
    "title": "get_column_count",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to get the column count, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/get_column_count.html#returns",
    "href": "reference/get_column_count.html#returns",
    "title": "get_column_count",
    "section": "Returns",
    "text": "Returns\n\n : int\n\nThe number of columns in the table."
  },
  {
    "objectID": "reference/get_column_count.html#supported-input-table-types",
    "href": "reference/get_column_count.html#supported-input-table-types",
    "title": "get_column_count",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nMicrosoft SQL Server table (\"mssql\")*\nSnowflake table (\"snowflake\")*\nDatabricks table (\"databricks\")*\nPySpark table (\"pyspark\")*\nBigQuery table (\"bigquery\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using get_column_count() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/get_column_count.html#examples",
    "href": "reference/get_column_count.html#examples",
    "title": "get_column_count",
    "section": "Examples",
    "text": "Examples\nTo get the number of columns in a table, we can use the get_column_count() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table_polars = pb.load_dataset(\"small_table\")\n\npb.get_column_count(small_table_polars)\n\n8\n\n\nThis table is a Polars DataFrame, but the get_column_count() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nsmall_table_duckdb = pb.load_dataset(\"small_table\", tbl_type=\"duckdb\")\n\npb.get_column_count(small_table_duckdb)\n\n8\n\n\nThe function always returns the number of columns in the table as an integer value, which is 8 for the small_table dataset."
  },
  {
    "objectID": "reference/expr_col.html",
    "href": "reference/expr_col.html",
    "title": "expr_col",
    "section": "",
    "text": "expr_col(column_name)\nCreate a column expression for use in conjointly() validation.\nThis function returns a ColumnExpression object that supports operations like &gt;, &lt;, +, etc. for use in conjointly() validation expressions."
  },
  {
    "objectID": "reference/expr_col.html#parameters",
    "href": "reference/expr_col.html#parameters",
    "title": "expr_col",
    "section": "Parameters",
    "text": "Parameters\n\ncolumn_name : str\n\nThe name of the column to reference."
  },
  {
    "objectID": "reference/expr_col.html#returns",
    "href": "reference/expr_col.html#returns",
    "title": "expr_col",
    "section": "Returns",
    "text": "Returns\n\n : ColumnExpression\n\nA column expression that can be used in comparisons and operations."
  },
  {
    "objectID": "reference/expr_col.html#examples",
    "href": "reference/expr_col.html#examples",
    "title": "expr_col",
    "section": "Examples",
    "text": "Examples\nLet’s say we have a table with three columns: a, b, and c. We want to validate that:\n\nThe values in column a are greater than 2.\nThe values in column b are less than 7.\nThe sum of columns a and b is less than the values in column c.\n\nWe can use the expr_col() function to create a column expression for each of these conditions.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 7, 1, 3, 9, 4],\n        \"b\": [6, 3, 0, 5, 8, 2],\n        \"c\": [10, 4, 8, 9, 10, 5],\n    }\n)\n\n# Using expr_col() to create backend-agnostic validation expressions\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pb.expr_col(\"a\") &gt; 2,\n        lambda df: pb.expr_col(\"b\") &lt; 7,\n        lambda df: pb.expr_col(\"a\") + pb.expr_col(\"b\") &lt; pb.expr_col(\"c\")\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    10.17\n    50.83\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe above code creates a validation object that checks the specified conditions using the expr_col() function. The resulting validation table will show whether each condition was satisfied for each row in the table."
  },
  {
    "objectID": "reference/expr_col.html#see-also",
    "href": "reference/expr_col.html#see-also",
    "title": "expr_col",
    "section": "See Also",
    "text": "See Also\nThe conjointly() validation method, which is where this function should be used."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html",
    "href": "reference/Validate.col_vals_between.html",
    "title": "Validate.col_vals_between",
    "section": "",
    "text": "Validate.col_vals_between(\n    columns,\n    left,\n    right,\n    inclusive=(True, True),\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo column data lie between two specified values or data in other columns?\nThe col_vals_between() validation method checks whether column values in a table fall within a range. The range is specified with three arguments: left=, right=, and inclusive=. The left= and right= values specify the lower and upper bounds. These bounds can be specified as literal values or as column names provided within col(). The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#parameters",
    "href": "reference/Validate.col_vals_between.html#parameters",
    "title": "Validate.col_vals_between",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nleft : float | int | Column\n\nThe lower bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\nright : float | int | Column\n\nThe upper bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\ninclusive : tuple[bool, bool] = (True, True)\n\nA tuple of two boolean values indicating whether the comparison should be inclusive. The position of the boolean values correspond to the left= and right= values, respectively. By default, both values are True.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#returns",
    "href": "reference/Validate.col_vals_between.html#returns",
    "title": "Validate.col_vals_between",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#what-can-be-used-in-left-and-right",
    "href": "reference/Validate.col_vals_between.html#what-can-be-used-in-left-and-right",
    "title": "Validate.col_vals_between",
    "section": "What Can Be Used in left= and right=?",
    "text": "What Can Be Used in left= and right=?\nThe left= and right= arguments both allow for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column in the target table\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value within left= and right=. There is flexibility in how you provide the date or datetime values for the bounds; they can be:\n\nstring-based dates or datetimes (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\ndate or datetime objects using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in either left= or right= (or both), it must be specified within col(). This facilitates column-to-column comparisons and, crucially, the columns being compared to either/both of the bounds must be of the same type as the column data (e.g., all numeric, all dates, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#preprocessing",
    "href": "reference/Validate.col_vals_between.html#preprocessing",
    "title": "Validate.col_vals_between",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and left=col(...)/right=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#segmentation",
    "href": "reference/Validate.col_vals_between.html#segmentation",
    "title": "Validate.col_vals_between",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#thresholds",
    "href": "reference/Validate.col_vals_between.html#thresholds",
    "title": "Validate.col_vals_between",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#examples",
    "href": "reference/Validate.col_vals_between.html#examples",
    "title": "Validate.col_vals_between",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [2, 3, 2, 4, 3, 4],\n        \"b\": [5, 6, 1, 6, 8, 5],\n        \"c\": [9, 8, 8, 7, 7, 8],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    2\n    5\n    9\n  \n  \n    2\n    3\n    6\n    8\n  \n  \n    3\n    2\n    1\n    8\n  \n  \n    4\n    4\n    6\n    7\n  \n  \n    5\n    3\n    8\n    7\n  \n  \n    6\n    4\n    5\n    8\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all between the fixed boundary values of 1 and 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_between(columns=\"a\", left=1, right=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    a\n    [1, 5]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_between(). All test units passed, and there are no failing test units.\nAside from checking a column against two literal values representing the lower and upper bounds, we can also provide column names to the left= and/or right= arguments (by using the helper function col(). In this way, we can perform three additional comparison types:\n\nleft=column, right=column\nleft=literal, right=column\nleft=column, right=literal\n\nFor the next example, we’ll use col_vals_between() to check whether the values in column b are between than corresponding values in columns a (lower bound) and c (upper bound).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_between(columns=\"b\", left=pb.col(\"a\"), right=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    b\n    [a, c]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 2: b is 1 but the bounds are 2 (a) and 8 (c).\nRow 4: b is 8 but the bounds are 3 (a) and 7 (c)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html",
    "href": "reference/Validate.col_vals_gt.html",
    "title": "Validate.col_vals_gt",
    "section": "",
    "text": "Validate.col_vals_gt(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data greater than a fixed value or data in another column?\nThe col_vals_gt() validation method checks whether column values in a table are greater than a specified value= (the exact comparison used in this function is col_val &gt; value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#parameters",
    "href": "reference/Validate.col_vals_gt.html#parameters",
    "title": "Validate.col_vals_gt",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#returns",
    "href": "reference/Validate.col_vals_gt.html#returns",
    "title": "Validate.col_vals_gt",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_gt.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_gt",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#preprocessing",
    "href": "reference/Validate.col_vals_gt.html#preprocessing",
    "title": "Validate.col_vals_gt",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#segmentation",
    "href": "reference/Validate.col_vals_gt.html#segmentation",
    "title": "Validate.col_vals_gt",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#thresholds",
    "href": "reference/Validate.col_vals_gt.html#thresholds",
    "title": "Validate.col_vals_gt",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#examples",
    "href": "reference/Validate.col_vals_gt.html#examples",
    "title": "Validate.col_vals_gt",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 6, 5],\n        \"b\": [1, 2, 1, 2, 2, 2],\n        \"c\": [2, 1, 2, 2, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    2\n    1\n  \n  \n    3\n    5\n    1\n    2\n  \n  \n    4\n    7\n    2\n    2\n  \n  \n    5\n    6\n    2\n    3\n  \n  \n    6\n    5\n    2\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all greater than the value of 4. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=4)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_gt(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_gt() to check whether the values in column c are greater than values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"c\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 1: c is 1 and b is 2.\nRow 3: c is 2 and b is 2."
  },
  {
    "objectID": "reference/Validate.interrogate.html",
    "href": "reference/Validate.interrogate.html",
    "title": "Validate.interrogate",
    "section": "",
    "text": "Validate.interrogate(\n    collect_extracts=True,\n    collect_tbl_checked=True,\n    get_first_n=None,\n    sample_n=None,\n    sample_frac=None,\n    extract_limit=500,\n)\nExecute each validation step against the table and store the results.\nWhen a validation plan has been set with a series of validation steps, the interrogation process through interrogate() should then be invoked. Interrogation will evaluate each validation step against the table and store the results.\nThe interrogation process will collect extracts of failing rows if the collect_extracts= option is set to True (the default). We can control the number of rows collected using the get_first_n=, sample_n=, and sample_frac= options. The extract_limit= option will enforce a hard limit on the number of rows collected when collect_extracts=True.\nAfter interrogation is complete, the Validate object will have gathered information, and we can use methods like n_passed(), f_failed(), etc., to understand how the table performed against the validation plan. A visual representation of the validation results can be viewed by printing the Validate object; this will display the validation table in an HTML viewing environment."
  },
  {
    "objectID": "reference/Validate.interrogate.html#parameters",
    "href": "reference/Validate.interrogate.html#parameters",
    "title": "Validate.interrogate",
    "section": "Parameters",
    "text": "Parameters\n\ncollect_extracts : bool = True\n\nAn option to collect rows of the input table that didn’t pass a particular validation step. The default is True and further options (i.e., get_first_n=, sample_*=) allow for fine control of how these rows are collected.\n\ncollect_tbl_checked : bool = True\n\nThe processed data frames produced by executing the validation steps is collected and stored in the Validate object if collect_tbl_checked=True. This information is necessary for some methods (e.g., get_sundered_data()), but it can potentially make the object grow to a large size. To opt out of attaching this data, set this to False.\n\nget_first_n : int | None = None\n\nIf the option to collect rows where test units is chosen, there is the option here to collect the first n rows. Supply an integer number of rows to extract from the top of subset table containing non-passing rows (the ordering of data from the original table is retained).\n\nsample_n : int | None = None\n\nIf the option to collect non-passing rows is chosen, this option allows for the sampling of n rows. Supply an integer number of rows to sample from the subset table. If n happens to be greater than the number of non-passing rows, then all such rows will be returned.\n\nsample_frac : int | float | None = None\n\nIf the option to collect non-passing rows is chosen, this option allows for the sampling of a fraction of those rows. Provide a number in the range of 0 and 1. The number of rows to return could be very large, however, the extract_limit= option will apply a hard limit to the returned rows.\n\nextract_limit : int = 500\n\nA value that limits the possible number of rows returned when extracting non-passing rows. The default is 500 rows. This limit is applied after any sampling or limiting options are applied. If the number of rows to be returned is greater than this limit, then the number of rows returned will be limited to this value. This is useful for preventing the collection of too many rows when the number of non-passing rows is very large."
  },
  {
    "objectID": "reference/Validate.interrogate.html#returns",
    "href": "reference/Validate.interrogate.html#returns",
    "title": "Validate.interrogate",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the results of the interrogation."
  },
  {
    "objectID": "reference/Validate.interrogate.html#examples",
    "href": "reference/Validate.interrogate.html#examples",
    "title": "Validate.interrogate",
    "section": "Examples",
    "text": "Examples\nLet’s use a built-in dataset (\"game_revenue\") to demonstrate some of the options of the interrogation process. A series of validation steps will populate our validation plan. After setting up the plan, the next step is to interrogate the table and see how well it aligns with our expectations. We’ll use the get_first_n= option so that any extracts of failing rows are limited to the first n rows.\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"game_revenue\"))\n    .col_vals_lt(columns=\"item_revenue\", value=200)\n    .col_vals_gt(columns=\"item_revenue\", value=0)\n    .col_vals_gt(columns=\"session_duration\", value=5)\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n)\n\nvalidation.interrogate(get_first_n=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:17:45Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    item_revenue\n    200\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:17:45 UTC&lt; 1 s2025-05-23 02:17:45 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that step 3 (checking for session_duration greater than 5) has 18 failing test units. This means that 18 rows in the table are problematic. We’d like to see the rows that failed this validation step and we can do that with the get_data_extracts() method.\n\npb.preview(validation.get_data_extracts(i=3, frame=True))\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows10Columns12\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    620\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:25:18+00:00\n    iap\n    offer4\n    17.991\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    621\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:26:24+00:00\n    iap\n    offer5\n    26.09\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    622\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:28:36+00:00\n    ad\n    ad_15sec\n    0.53\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n\n\n\n\n\n\n        \n\n\nThe get_data_extracts() method will return a Polars DataFrame here with the first 10 rows that failed the validation step (we passed that into the preview() function for a better display). There are actually 18 rows that failed but we limited the collection of extracts with get_first_n=10."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html",
    "href": "reference/Validate.col_vals_null.html",
    "title": "Validate.col_vals_null",
    "section": "",
    "text": "Validate.col_vals_null(\n    columns,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether values in a column are NULL.\nThe col_vals_null() validation method checks whether column values in a table are NULL. This validation will operate over the number of test units that is equal to the number of rows in the table."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#parameters",
    "href": "reference/Validate.col_vals_null.html#parameters",
    "title": "Validate.col_vals_null",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#returns",
    "href": "reference/Validate.col_vals_null.html#returns",
    "title": "Validate.col_vals_null",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#preprocessing",
    "href": "reference/Validate.col_vals_null.html#preprocessing",
    "title": "Validate.col_vals_null",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#segmentation",
    "href": "reference/Validate.col_vals_null.html#segmentation",
    "title": "Validate.col_vals_null",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#thresholds",
    "href": "reference/Validate.col_vals_null.html#thresholds",
    "title": "Validate.col_vals_null",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#examples",
    "href": "reference/Validate.col_vals_null.html#examples",
    "title": "Validate.col_vals_null",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [None, None, None, None],\n        \"b\": [None, 2, None, 9],\n    }\n).with_columns(pl.col(\"a\").cast(pl.Int64))\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    None\n    None\n  \n  \n    2\n    None\n    2\n  \n  \n    3\n    None\n    None\n  \n  \n    4\n    None\n    9\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all Null values. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_null(columns=\"a\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_null(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_null(columns=\"b\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the two non-Null values in column b."
  },
  {
    "objectID": "reference/Validate.specially.html",
    "href": "reference/Validate.specially.html",
    "title": "Validate.specially",
    "section": "",
    "text": "Validate.specially(\n    expr,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nPerform a specialized validation with customized logic.\nThe specially() validation method allows for the creation of specialized validation expressions that can be used to validate specific conditions or logic in the data. This method provides maximum flexibility by accepting a custom callable that encapsulates your validation logic.\nThe callable function can have one of two signatures:\nThe second form is particularly useful for environment validations that don’t need to inspect the data table.\nThe callable function must ultimately return one of:\nThe validation will operate over the number of test units that is equal to the number of rows in the data table (if returning a table with boolean values). If returning a scalar boolean value, the validation will operate over a single test unit. For a return of a list of boolean values, the length of the list constitutes the number of test units."
  },
  {
    "objectID": "reference/Validate.specially.html#parameters",
    "href": "reference/Validate.specially.html#parameters",
    "title": "Validate.specially",
    "section": "Parameters",
    "text": "Parameters\n\nexpr : Callable\n\nA callable function that defines the specialized validation logic. This function should: (1) accept the target data table as its single argument (though it may ignore it), or (2) take no parameters at all (for environment validations). The function must ultimately return boolean values representing validation results. Design your function to incorporate any custom parameters directly within the function itself using closure variables or default parameters.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.specially.html#returns",
    "href": "reference/Validate.specially.html#returns",
    "title": "Validate.specially",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.specially.html#preprocessing",
    "href": "reference/Validate.specially.html#preprocessing",
    "title": "Validate.specially",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.specially.html#thresholds",
    "href": "reference/Validate.specially.html#thresholds",
    "title": "Validate.specially",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.specially.html#examples",
    "href": "reference/Validate.specially.html#examples",
    "title": "Validate.specially",
    "section": "Examples",
    "text": "Examples\nThe specially() method offers maximum flexibility for validation, allowing you to create custom validation logic that fits your specific needs. The following examples demonstrate different patterns and use cases for this powerful validation approach.\n\nSimple validation with direct table access\nThis example shows the most straightforward use case where we create a function that directly checks if the sum of two columns is positive.\n\nimport pointblank as pb\nimport polars as pl\n\nsimple_tbl = pl.DataFrame({\n    \"a\": [5, 7, 1, 3, 9, 4],\n    \"b\": [6, 3, 0, 5, 8, 2]\n})\n\n# Simple function that validates directly on the table\ndef validate_sum_positive(data):\n    return data.select(pl.col(\"a\") + pl.col(\"b\") &gt; 0)\n\n(\n    pb.Validate(data=simple_tbl)\n    .specially(expr=validate_sum_positive)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    specially\n    \n        \n            \n            \n                \n            \n        \n    \n\n        \n        \n            specially()\n        \n        \n        \n    \n    EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe function returns a Polars DataFrame with a single boolean column indicating whether the sum of columns a and b is positive for each row. Each row in the resulting DataFrame is a distinct test unit. This pattern works well for simple validations where you don’t need configurable parameters.\n\n\nAdvanced validation with closure variables for parameters\nWhen you need to make your validation configurable, you can use the function factory pattern (also known as closures) to create parameterized validations:\n\n# Create a parameterized validation function using closures\ndef make_column_ratio_validator(col1, col2, min_ratio):\n    def validate_column_ratio(data):\n        return data.select((pl.col(col1) / pl.col(col2)) &gt; min_ratio)\n    return validate_column_ratio\n\n(\n    pb.Validate(data=simple_tbl)\n    .specially(\n        expr=make_column_ratio_validator(col1=\"a\", col2=\"b\", min_ratio=0.5)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    specially\n    \n        \n            \n            \n                \n            \n        \n    \n\n        \n        \n            specially()\n        \n        \n        \n    \n    EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis approach allows you to create reusable validation functions that can be configured with different parameters without modifying the function itself.\n\n\nValidation function returning a list of booleans\nThis example demonstrates how to create a validation function that returns a list of boolean values, where each element represents a separate test unit:\n\nimport pointblank as pb\nimport polars as pl\nimport random\n\n# Create sample data\ntransaction_tbl = pl.DataFrame({\n    \"transaction_id\": [f\"TX{i:04d}\" for i in range(1, 11)],\n    \"amount\": [120.50, 85.25, 50.00, 240.75, 35.20, 150.00, 85.25, 65.00, 210.75, 90.50],\n    \"category\": [\"food\", \"shopping\", \"entertainment\", \"travel\", \"utilities\",\n                \"food\", \"shopping\", \"entertainment\", \"travel\", \"utilities\"]\n})\n\n# Define a validation function that returns a list of booleans\ndef validate_transaction_rules(data):\n    # Create a list to store individual test results\n    test_results = []\n\n    # Check each row individually against multiple business rules\n    for row in data.iter_rows(named=True):\n        # Rule: transaction IDs must start with \"TX\" and be 6 chars long\n        valid_id = row[\"transaction_id\"].startswith(\"TX\") and len(row[\"transaction_id\"]) == 6\n\n        # Rule: Amounts must be appropriate for their category\n        valid_amount = True\n        if row[\"category\"] == \"food\" and (row[\"amount\"] &lt; 10 or row[\"amount\"] &gt; 200):\n            valid_amount = False\n        elif row[\"category\"] == \"utilities\" and (row[\"amount\"] &lt; 20 or row[\"amount\"] &gt; 300):\n            valid_amount = False\n        elif row[\"category\"] == \"entertainment\" and row[\"amount\"] &gt; 100:\n            valid_amount = False\n\n        # A transaction passes if it satisfies both rules\n        test_results.append(valid_id and valid_amount)\n\n    return test_results\n\n(\n    pb.Validate(data=transaction_tbl)\n    .specially(\n        expr=validate_transaction_rules,\n        brief=\"Validate transaction IDs and amounts by category.\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    specially\n    \n        \n            \n            \n                \n            \n        \n    \n\n        \n        \n            specially()\n        \n        Validate transaction IDs and amounts by category.\n\n        \n    \n    EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    10\n    101.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis example shows how to create a validation function that applies multiple business rules to each row and returns a list of boolean results. Each boolean in the list represents a separate test unit, and a test unit passes only if all rules are satisfied for a given row.\nThe function iterates through each row in the data table, checking:\n\nif transaction IDs follow the required format\nif transaction amounts are appropriate for their respective categories\n\nThis approach is powerful when you need to apply complex, conditional logic that can’t be easily expressed using the built-in validation functions.\n\n\nTable-level validation returning a single boolean\nSometimes you need to validate properties of the entire table rather than row-by-row. In these cases, your function can return a single boolean value:\n\ndef validate_table_properties(data):\n    # Check if table has at least one row with column 'a' &gt; 10\n    has_large_values = data.filter(pl.col(\"a\") &gt; 10).height &gt; 0\n\n    # Check if mean of column 'b' is positive\n    has_positive_mean = data.select(pl.mean(\"b\")).item() &gt; 0\n\n    # Return a single boolean for the entire table\n    return has_large_values and has_positive_mean\n\n(\n    pb.Validate(data=simple_tbl)\n    .specially(expr=validate_table_properties)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    specially\n    \n        \n            \n            \n                \n            \n        \n    \n\n        \n        \n            specially()\n        \n        \n        \n    \n    EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis example demonstrates how to perform multiple checks on the table as a whole and combine them into a single validation result.\n\n\nEnvironment validation that doesn’t use the data table\nThe specially() validation method can even be used to validate aspects of your environment that are completely independent of the data:\n\ndef validate_pointblank_version():\n    try:\n        import importlib.metadata\n        version = importlib.metadata.version(\"pointblank\")\n        version_parts = version.split(\".\")\n\n        # Get major and minor components regardless of how many parts there are\n        major = int(version_parts[0])\n        minor = int(version_parts[1])\n\n        # Check both major and minor components for version `0.9+`\n        return (major &gt; 0) or (major == 0 and minor &gt;= 9)\n\n    except Exception as e:\n        # More specific error handling could be added here\n        print(f\"Version check failed: {e}\")\n        return False\n\n(\n    pb.Validate(data=simple_tbl)\n    .specially(\n        expr=validate_pointblank_version,\n        brief=\"Check Pointblank version `&gt;=0.9.0`.\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    specially\n    \n        \n            \n            \n                \n            \n        \n    \n\n        \n        \n            specially()\n        \n        Check Pointblank version &gt;=0.9.0.\n\n        \n    \n    EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis pattern shows how to validate external dependencies or environment conditions as part of your validation workflow. Notice that the function doesn’t take any parameters at all, which makes it cleaner when the validation doesn’t need to access the data table.\nBy combining these patterns, you can create sophisticated validation workflows that address virtually any data quality requirement in your organization."
  },
  {
    "objectID": "reference/config.html",
    "href": "reference/config.html",
    "title": "config",
    "section": "",
    "text": "config(\n    report_incl_header=True,\n    report_incl_footer=True,\n    preview_incl_header=True,\n)\nConfiguration settings for the Pointblank library.\n\n\n\nreport_incl_header : bool = True\n\nThis controls whether the header should be present in the validation table report. The header contains the table name, label information, and might contain global failure threshold levels (if set).\n\nreport_incl_footer : bool = True\n\nShould the footer of the validation table report be displayed? The footer contains the starting and ending times of the interrogation.\n\npreview_incl_header : bool = True\n\nWhether the header should be present in any preview table (generated via the preview() function).\n\n\n\n\n\n\n : PointblankConfig\n\nA PointblankConfig object with the specified configuration settings."
  },
  {
    "objectID": "reference/config.html#parameters",
    "href": "reference/config.html#parameters",
    "title": "config",
    "section": "",
    "text": "report_incl_header : bool = True\n\nThis controls whether the header should be present in the validation table report. The header contains the table name, label information, and might contain global failure threshold levels (if set).\n\nreport_incl_footer : bool = True\n\nShould the footer of the validation table report be displayed? The footer contains the starting and ending times of the interrogation.\n\npreview_incl_header : bool = True\n\nWhether the header should be present in any preview table (generated via the preview() function)."
  },
  {
    "objectID": "reference/config.html#returns",
    "href": "reference/config.html#returns",
    "title": "config",
    "section": "",
    "text": ": PointblankConfig\n\nA PointblankConfig object with the specified configuration settings."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html",
    "href": "reference/Validate.col_vals_ne.html",
    "title": "Validate.col_vals_ne",
    "section": "",
    "text": "Validate.col_vals_ne(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data not equal to a fixed value or data in another column?\nThe col_vals_ne() validation method checks whether column values in a table are not equal to a specified value= (the exact comparison used in this function is col_val != value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#parameters",
    "href": "reference/Validate.col_vals_ne.html#parameters",
    "title": "Validate.col_vals_ne",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#returns",
    "href": "reference/Validate.col_vals_ne.html#returns",
    "title": "Validate.col_vals_ne",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_ne.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_ne",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#preprocessing",
    "href": "reference/Validate.col_vals_ne.html#preprocessing",
    "title": "Validate.col_vals_ne",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#segmentation",
    "href": "reference/Validate.col_vals_ne.html#segmentation",
    "title": "Validate.col_vals_ne",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#thresholds",
    "href": "reference/Validate.col_vals_ne.html#thresholds",
    "title": "Validate.col_vals_ne",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#examples",
    "href": "reference/Validate.col_vals_ne.html#examples",
    "title": "Validate.col_vals_ne",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 5, 5, 5, 5, 5],\n        \"b\": [5, 6, 3, 6, 5, 8],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    5\n    6\n  \n  \n    3\n    5\n    3\n  \n  \n    4\n    5\n    6\n  \n  \n    5\n    5\n    5\n  \n  \n    6\n    5\n    8\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are not equal to the value of 3. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ne(columns=\"a\", value=3)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_ne\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ne()\n        \n        \n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_ne(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_ne() to check whether the values in column a aren’t equal to the values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ne(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_ne\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ne()\n        \n        \n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are in rows 0 and 4, where a is 5 and b is 5 in both cases (i.e., they are equal to each other)."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html",
    "href": "reference/Validate.get_data_extracts.html",
    "title": "Validate.get_data_extracts",
    "section": "",
    "text": "Validate.get_data_extracts(i=None, frame=False)\nGet the rows that failed for each validation step.\nAfter the interrogate() method has been called, the get_data_extracts() method can be used to extract the rows that failed in each row-based validation step (e.g., col_vals_gt(), etc.). The method returns a dictionary of tables containing the rows that failed in every row-based validation function. If frame=True and i= is a scalar, the value is conveniently returned as a table (forgoing the dictionary structure)."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#parameters",
    "href": "reference/Validate.get_data_extracts.html#parameters",
    "title": "Validate.get_data_extracts",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the failed rows are obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nframe : bool = False\n\nIf True and i= is a scalar, return the value as a DataFrame instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#returns",
    "href": "reference/Validate.get_data_extracts.html#returns",
    "title": "Validate.get_data_extracts",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, FrameT | None] | FrameT | None\n\nA dictionary of tables containing the rows that failed in every row-based validation step or a DataFrame."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#validation-methods-that-are-row-based",
    "href": "reference/Validate.get_data_extracts.html#validation-methods-that-are-row-based",
    "title": "Validate.get_data_extracts",
    "section": "Validation Methods that are Row-Based",
    "text": "Validation Methods that are Row-Based\nThe following validation methods are row-based and will have rows extracted when there are failing test units.\n\ncol_vals_gt()\ncol_vals_ge()\ncol_vals_lt()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\nrows_distinct()\n\nAn extracted row means that a test unit failed for that row in the validation step. The extracted rows are a subset of the original table and are useful for further analysis or for understanding the nature of the failing test units."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#examples",
    "href": "reference/Validate.get_data_extracts.html#examples",
    "title": "Validate.get_data_extracts",
    "section": "Examples",
    "text": "Examples\nLet’s perform a series of validation steps on a Polars DataFrame. We’ll use the col_vals_gt() in the first step, col_vals_lt() in the second step, and col_vals_ge() in the third step. The interrogate() method executes the validation; then, we can extract the rows that failed for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 3, 6, 1],\n        \"b\": [1, 2, 1, 5, 2, 6],\n        \"c\": [3, 7, 2, 6, 3, 1],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=4)\n    .col_vals_lt(columns=\"c\", value=5)\n    .col_vals_ge(columns=\"b\", value=1)\n    .interrogate()\n)\n\nvalidation.get_data_extracts()\n\n{1: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n │ 6         ┆ 1   ┆ 6   ┆ 1   │\n └───────────┴─────┴─────┴─────┘,\n 2: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 2         ┆ 6   ┆ 2   ┆ 7   │\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n └───────────┴─────┴─────┴─────┘,\n 3: shape: (0, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n └───────────┴─────┴─────┴─────┘}\n\n\nThe get_data_extracts() method returns a dictionary of tables, where each table contains a subset of rows from the table. These are the rows that failed for each validation step.\nIn the first step, thecol_vals_gt() method was used to check if the values in column a were greater than 4. The extracted table shows the rows where this condition was not met; look at the a column: all values are less than 4.\nIn the second step, the col_vals_lt() method was used to check if the values in column c were less than 5. In the extracted two-row table, we see that the values in column c are greater than 5.\nThe third step (col_vals_ge()) checked if the values in column b were greater than or equal to 1. There were no failing test units, so the extracted table is empty (i.e., has columns but no rows).\nThe i= argument can be used to narrow down the extraction to one or more steps. For example, to extract the rows that failed in the first step only:\n\nvalidation.get_data_extracts(i=1)\n\n{1: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n │ 6         ┆ 1   ┆ 6   ┆ 1   │\n └───────────┴─────┴─────┴─────┘}\n\n\nNote that the first validation step is indexed at 1 (not 0). This 1-based indexing is in place here to match the step numbers reported in the validation table. What we get back is still a dictionary, but it only contains one table (the one for the first step).\nIf you want to get the extracted table as a DataFrame, set frame=True and provide a scalar value for i. For example, to get the extracted table for the second step as a DataFrame:\n\npb.preview(validation.get_data_extracts(i=2, frame=True))\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    2\n    6\n    2\n    7\n  \n  \n    4\n    3\n    5\n    6\n  \n\n\n\n\n\n\n        \n\n\nThe extracted table is now a DataFrame, which can serve as a more convenient format for further analysis or visualization. We further used the preview() function to show the DataFrame in an HTML view."
  },
  {
    "objectID": "reference/Validate.n_passed.html",
    "href": "reference/Validate.n_passed.html",
    "title": "Validate.n_passed",
    "section": "",
    "text": "Validate.n_passed(i=None, scalar=False)\nProvides a dictionary of the number of test units that passed for each validation step.\nThe n_passed() method provides the number of test units that passed for each validation step. This is the number of test units that passed in the the validation step. It is always some integer value between 0 and the total number of test units.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of passing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the n_passed() method (i.e., n - n_failed)."
  },
  {
    "objectID": "reference/Validate.n_passed.html#parameters",
    "href": "reference/Validate.n_passed.html#parameters",
    "title": "Validate.n_passed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of passing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n_passed.html#returns",
    "href": "reference/Validate.n_passed.html#returns",
    "title": "Validate.n_passed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of passing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n_passed.html#examples",
    "href": "reference/Validate.n_passed.html#examples",
    "title": "Validate.n_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps and, as it turns out, all of them will have failing test units. After interrogation, the n_passed() method is used to determine the number of passing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12],\n        \"b\": [9, 8, 10, 5, 10],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.n_passed()\n\n{1: 4, 2: 3, 3: 4}\n\n\nThe returned dictionary shows that all validation steps had no passing test units (each value was less than 5, which is the total number of test units for each step).\nIf we wanted to check the number of passing test units for a single validation step, we can provide the step number. Also, we could forego the dictionary and get a scalar value by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.n_passed(i=1)\n\n{1: 4}\n\n\nThe returned value of 4 is the number of passing test units for the first validation step."
  },
  {
    "objectID": "reference/Validate.critical.html",
    "href": "reference/Validate.critical.html",
    "title": "Validate.critical",
    "section": "",
    "text": "Validate.critical(i=None, scalar=False)\nGet the ‘critical’ level status for each validation step.\nThe ‘critical’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the notification level. Otherwise, the status is False.\nThe ascribed name of ‘critical’ is semantic and is thus simply a status indicator that could be used to trigger some action to be take. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the notification status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.critical.html#parameters",
    "href": "reference/Validate.critical.html#parameters",
    "title": "Validate.critical",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the notification status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.critical.html#returns",
    "href": "reference/Validate.critical.html#returns",
    "title": "Validate.critical",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the notification status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.critical.html#examples",
    "href": "reference/Validate.critical.html#examples",
    "title": "Validate.critical",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have many failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the critical() method is used to determine the ‘critical’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [2, 4, 4, 7, 2, 3, 8],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.critical()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘critical’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘critical’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘critical’ level.\nWe can also visually inspect the ‘critical’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:17:07PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    20.29\n    50.71\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:17:07 UTC&lt; 1 s2025-05-23 02:17:07 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there are filled gray, yellow, and red circles in the first step (far right side, in the W, E, and C columns) indicating that the ‘warning’, ‘error’, and ‘critical’ thresholds were met. The other steps have empty gray, yellow, and red circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘critical’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.critical(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had the ‘critical’ threshold met."
  },
  {
    "objectID": "reference/first_n.html",
    "href": "reference/first_n.html",
    "title": "first_n",
    "section": "",
    "text": "first_n(n, offset=0)\nSelect the first n columns in the column list.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The first_n() selector function can be used to select n columns positioned at the start of the column list. So if the set of table columns consists of\n[rev_01, rev_02, profit_01, profit_02, age]\nand you want to validate the first two columns, you can use columns=first_n(2). This will select the rev_01 and rev_02 columns and a validation step will be created for each.\nThe offset= parameter can be used to skip a certain number of columns from the start of the column list. So if you want to select the third and fourth columns, you can use columns=first_n(2, offset=2)."
  },
  {
    "objectID": "reference/first_n.html#parameters",
    "href": "reference/first_n.html#parameters",
    "title": "first_n",
    "section": "Parameters",
    "text": "Parameters\n\nn : int\n\nThe number of columns to select from the start of the column list. Should be a positive integer value. If n is greater than the number of columns in the table, all columns will be selected.\n\noffset : int = 0\n\nThe offset from the start of the column list. The default is 0. If offset is greater than the number of columns in the table, no columns will be selected."
  },
  {
    "objectID": "reference/first_n.html#returns",
    "href": "reference/first_n.html#returns",
    "title": "first_n",
    "section": "Returns",
    "text": "Returns\n\n : FirstN\n\nA FirstN object, which can be used to select the first n columns."
  },
  {
    "objectID": "reference/first_n.html#relevant-validation-methods-where-first_n-can-be-used",
    "href": "reference/first_n.html#relevant-validation-methods-where-first_n-can-be-used",
    "title": "first_n",
    "section": "Relevant Validation Methods where first_n() can be Used",
    "text": "Relevant Validation Methods where first_n() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe first_n() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/first_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/first_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "first_n",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe first_n() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names starting with “rev” along with the first two columns, you can use the first_n() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(first_n(2) | starts_with(\"rev\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/first_n.html#examples",
    "href": "reference/first_n.html#examples",
    "title": "first_n",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns paid_2021, paid_2022, paid_2023, paid_2024, and name and we’d like to validate that the values in the first four columns are greater than 10. We can use the first_n() column selector function to specify that the first four columns in the table are the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.first_n(4), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps. The values in all those columns were all greater than 10.\nWe can also use the first_n() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select the first four columns but also omit those columns that end with \"2023\", we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.first_n(4) - pb.ends_with(\"2023\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get three validation steps, one for paid_2021, paid_2022, and paid_2024."
  },
  {
    "objectID": "reference/Validate.get_step_report.html",
    "href": "reference/Validate.get_step_report.html",
    "title": "Validate.get_step_report",
    "section": "",
    "text": "Validate.get_step_report(i, columns_subset=None, header=':default:', limit=10)\nGet a detailed report for a single validation step.\nThe get_step_report() method returns a report of what went well—or what failed spectacularly—for a given validation step. The report includes a summary of the validation step and a detailed breakdown of the interrogation results. The report is presented as a GT table object, which can be displayed in a notebook or exported to an HTML file."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#parameters",
    "href": "reference/Validate.get_step_report.html#parameters",
    "title": "Validate.get_step_report",
    "section": "Parameters",
    "text": "Parameters\n\ni : int\n\nThe step number for which to get the report.\n\ncolumns_subset : str | list[str] | Column | None = None\n\nThe columns to display in a step report that shows errors in the input table. By default all columns are shown (None). If a subset of columns is desired, we can provide a list of column names, a string with a single column name, a Column object, or a ColumnSelector object. The last two options allow for more flexible column selection using column selector functions. Errors are raised if the column names provided don’t match any columns in the table (when provided as a string or list of strings) or if column selector expressions don’t resolve to any columns.\n\nheader : str = ':default:'\n\nOptions for customizing the header of the step report. The default is the \":default:\" value which produces a header with a standard title and set of details underneath. Aside from this default, free text can be provided for the header. This will be interpreted as Markdown text and transformed internally to HTML. You can provide one of two templating elements: {title} and {details}. The default header has the template \"{title}{details}\" so you can easily start from that and modify as you see fit. If you don’t want a header at all, you can set header=None to remove it entirely.\n\nlimit : int | None = 10\n\nThe number of rows to display for those validation steps that check values in rows (the col_vals_*() validation steps). The default is 10 rows and the limit can be removed entirely by setting limit=None."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#returns",
    "href": "reference/Validate.get_step_report.html#returns",
    "title": "Validate.get_step_report",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT table object that represents the detailed report for the validation step."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#types-of-step-reports",
    "href": "reference/Validate.get_step_report.html#types-of-step-reports",
    "title": "Validate.get_step_report",
    "section": "Types of Step Reports",
    "text": "Types of Step Reports\nThe get_step_report() method produces a report based on the type of validation step. The following row-based validation methods will produce a report that shows the rows of the data that failed because of failing test units within one or more columns failed:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_regex()\ncol_vals_null()\ncol_vals_not_null()\nrows_complete()\nconjointly()\n\nThe rows_distinct() validation step will produce a report that shows duplicate rows (or duplicate values in one or a set of columns as defined in that method’s columns_subset= parameter.\nThe col_schema_match() validation step will produce a report that shows the schema of the data table and the schema of the validation step. The report will indicate whether the schemas match or not."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#examples",
    "href": "reference/Validate.get_step_report.html#examples",
    "title": "Validate.get_step_report",
    "section": "Examples",
    "text": "Examples\nLet’s create a validation plan with a few validation steps and interrogate the data. With that, we’ll have a look at the validation reporting table for the entire collection of steps and what went well or what failed.\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Example for the get_step_report() method\",\n        thresholds=(1, 0.20, 0.40)\n    )\n    .col_vals_lt(columns=\"d\", value=3500)\n    .col_vals_between(columns=\"c\", left=1, right=8)\n    .col_vals_gt(columns=\"a\", value=3)\n    .col_vals_regex(columns=\"b\", pattern=r\"[0-9]-[a-z]{3}-[0-9]{3}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    3500\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    c\n    [1, 8]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    [0-9]-[a-z]{3}-[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThere were four validation steps performed, where the first three steps had failing test units and the last step had no failures. Let’s get a detailed report for the first step by using the get_step_report() method.\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION d &lt; 35002 / 13 TEST UNIT FAILURES IN COLUMN 6 EXTRACT OF ALL 2 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    \n    3892.4\n    False\n    mid\n  \n\n\n\n\n\n\n        \n\n\nThe report for the first step is displayed. The report includes a summary of the validation step and a detailed breakdown of the interrogation results. The report provides details on what the validation step was checking, the extent to which the test units failed, and a table that shows the failing rows of the data with the column of interest highlighted.\nThe second and third steps also had failing test units. Reports for those steps can be viewed by using get_step_report(i=2) and get_step_report(i=3) respectively.\nThe final step did not have any failing test units. A report for the final step can still be viewed by using get_step_report(i=4). The report will indicate that every test unit passed and a prview of the target table will be provided.\n\nvalidation.get_step_report(i=4)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 4 ✓ASSERTION b matches regex [0-9]-[a-z]{3}-[0-9]{3}13 TEST UNITS ALL PASSED IN COLUMN 4PREVIEW OF TARGET TABLE:\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIf you’d like to trim down the number of columns shown in the report, you can provide a subset of columns to display. For example, if you only want to see the columns a, b, and c, you can provide those column names as a list.\n\nvalidation.get_step_report(i=1, columns_subset=[\"a\", \"b\", \"c\"])\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION d &lt; 35002 / 13 TEST UNIT FAILURES IN COLUMN 6 (NOT SHOWN)EXTRACT OF ALL 2 ROWS :\n  \n\n  \n  aint64\n  bobject\n  cfloat64\n\n\n\n  \n    2\n    3\n    5-egh-163\n    8.0\n  \n  \n    4\n    2\n    5-jdo-903\n    \n  \n\n\n\n\n\n\n        \n\n\nIf you’d like to increase or reduce the maximum number of rows shown in the report, you can provide a different value for the limit parameter. For example, if you’d like to see only up to 5 rows, you can set limit=5.\n\nvalidation.get_step_report(i=3, limit=5)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 3ASSERTION a &gt; 37 / 13 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF FIRST 5 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    \n    3892.4\n    False\n    mid\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n\n\n\n\n\n\n        \n\n\nStep 3 actually had 7 failing test units, but only the first 5 rows are shown in the step report because of the limit=5 parameter."
  },
  {
    "objectID": "reference/Validate.html",
    "href": "reference/Validate.html",
    "title": "Validate",
    "section": "",
    "text": "Validate(\n    self,\n    data,\n    tbl_name=None,\n    label=None,\n    thresholds=None,\n    actions=None,\n    final_actions=None,\n    brief=None,\n    lang=None,\n    locale=None,\n)\nWorkflow for defining a set of validations on a table and interrogating for results.\nThe Validate class is used for defining a set of validation steps on a table and interrogating the table with the validation plan. This class is the main entry point for the data quality reporting workflow. The overall aim of this workflow is to generate comprehensive reporting information to assess the level of data quality for a target table.\nWe can supply as many validation steps as needed, and having a large number of them should increase the validation coverage for a given table. The validation methods (e.g., col_vals_gt(), col_vals_between(), etc.) translate to discrete validation steps, where each step will be sequentially numbered (useful when viewing the reporting data). This process of calling validation methods is known as developing a validation plan.\nThe validation methods, when called, are merely instructions up to the point the concluding interrogate() method is called. That kicks off the process of acting on the validation plan by querying the target table getting reporting results for each step. Once the interrogation process is complete, we can say that the workflow now has reporting information. We can then extract useful information from the reporting data to understand the quality of the table. Printing the Validate object (or using the get_tabular_report() method) will return a table with the results of the interrogation and get_sundered_data() allows for the splitting of the table based on passing and failing rows."
  },
  {
    "objectID": "reference/Validate.html#parameters",
    "href": "reference/Validate.html#parameters",
    "title": "Validate",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to validate, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str | None = None\n\nAn optional name to assign to the input table object. If no value is provided, a name will be generated based on whatever information is available. This table name will be displayed in the header area of the tabular report.\n\nlabel : str | None = None\n\nAn optional label for the validation plan. If no value is provided, a label will be generated based on the current system date and time. Markdown can be used here to make the label more visually appealing (it will appear in the header area of the tabular report).\n\nthresholds : int | float | bool | tuple | dict | Thresholds | None = None\n\nGenerate threshold failure levels so that all validation steps can report and react accordingly when exceeding the set levels. The thresholds are set at the global level and can be overridden at the validation step level (each validation step has its own thresholds= parameter). The default is None, which means that no thresholds will be set. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nThe actions to take when validation steps meet or exceed any set threshold levels. These actions are paired with the threshold levels and are executed during the interrogation process when there are exceedances. The actions are executed right after each step is evaluated. Such actions should be provided in the form of an Actions object. If None then no global actions will be set. View the Actions section for information on how to set actions.\n\nfinal_actions : FinalActions | None = None\n\nThe actions to take when the validation process is complete and the final results are available. This is useful for sending notifications or reporting the overall status of the validation process. The final actions are executed after all validation steps have been processed and the results have been collected. The final actions are not tied to any threshold levels, they are executed regardless of the validation results. Such actions should be provided in the form of a FinalActions object. If None then no finalizing actions will be set. Please see the Actions section for information on how to set final actions.\n\nbrief : str | bool | None = None\n\nA global setting for briefs, which are optional brief descriptions for validation steps (they be displayed in the reporting table). For such a global setting, templating elements like \"{step}\" (to insert the step number) or \"{auto}\" (to include an automatically generated brief) are useful. If True then each brief will be automatically generated. If None (the default) then briefs aren’t globally set.\n\nlang : str | None = None\n\nThe language to use for various reporting elements. By default, None will select English (\"en\") as the but other options include French (\"fr\"), German (\"de\"), Italian (\"it\"), Spanish (\"es\"), and several more. Have a look at the Reporting Languages section for the full list of supported languages and information on how the language setting is utilized.\n\nlocale : str | None = None\n\nAn optional locale ID to use for formatting values in the reporting table according the locale’s rules. Examples include \"en-US\" for English (United States) and \"fr-FR\" for French (France). More simply, this can be a language identifier without a designation of territory, like \"es\" for Spanish."
  },
  {
    "objectID": "reference/Validate.html#returns",
    "href": "reference/Validate.html#returns",
    "title": "Validate",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nA Validate object with the table and validations to be performed."
  },
  {
    "objectID": "reference/Validate.html#supported-input-table-types",
    "href": "reference/Validate.html#supported-input-table-types",
    "title": "Validate",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nMicrosoft SQL Server table (\"mssql\")*\nSnowflake table (\"snowflake\")*\nDatabricks table (\"databricks\")*\nPySpark table (\"pyspark\")*\nBigQuery table (\"bigquery\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, the use of Validate with such tables requires the Ibis library v9.5.0 and above to be installed. If the input table is a Polars or Pandas DataFrame, the Ibis library is not required."
  },
  {
    "objectID": "reference/Validate.html#thresholds",
    "href": "reference/Validate.html#thresholds",
    "title": "Validate",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for all validation steps. They are set here at the global level but can be overridden at the validation step level (each validation step has its own local thresholds= parameter).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units for a validation step exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.html#actions",
    "href": "reference/Validate.html#actions",
    "title": "Validate",
    "section": "Actions",
    "text": "Actions\nThe actions= and final_actions= parameters provide mechanisms to respond to validation results. These actions can be used to notify users of validation failures, log issues, or trigger other processes when problems are detected.\nStep Actions\nThe actions= parameter allows you to define actions that are triggered when validation steps exceed specific threshold levels (warning, error, or critical). These actions are executed during the interrogation process, right after each step is evaluated.\nStep actions should be provided using the Actions class, which lets you specify different actions for different severity levels:\n# Define an action that logs a message when warning threshold is exceeded\ndef log_warning():\n    metadata = pb.get_action_metadata()\n    print(f\"WARNING: Step {metadata['step']} failed with type {metadata['type']}\")\n\n# Define actions for different threshold levels\nactions = pb.Actions(\n    warning = log_warning,\n    error = lambda: send_email(\"Error in validation\"),\n    critical = \"CRITICAL FAILURE DETECTED\"\n)\n\n# Use in Validate\nvalidation = pb.Validate(\n    data=my_data,\n    actions=actions  # Global actions for all steps\n)\nYou can also provide step-specific actions in individual validation methods:\nvalidation.col_vals_gt(\n    columns=\"revenue\",\n    value=0,\n    actions=pb.Actions(warning=log_warning)  # Only applies to this step\n)\nStep actions have access to step-specific context through the get_action_metadata() function, which provides details about the current validation step that triggered the action.\nFinal Actions\nThe final_actions= parameter lets you define actions that execute after all validation steps have completed. These are useful for providing summaries, sending notifications based on overall validation status, or performing cleanup operations.\nFinal actions should be provided using the FinalActions class:\ndef send_report():\n    summary = pb.get_validation_summary()\n    if summary[\"status\"] == \"CRITICAL\":\n        send_alert_email(\n            subject=f\"CRITICAL validation failures in {summary['table_name']}\",\n            body=f\"{summary['critical_steps']} steps failed with critical severity.\"\n        )\n\nvalidation = pb.Validate(\n    data=my_data,\n    final_actions=pb.FinalActions(send_report)\n)\nFinal actions have access to validation-wide summary information through the get_validation_summary() function, which provides a comprehensive overview of the entire validation process.\nThe combination of step actions and final actions provides a flexible system for responding to data quality issues at both the individual step level and the overall validation level."
  },
  {
    "objectID": "reference/Validate.html#reporting-languages",
    "href": "reference/Validate.html#reporting-languages",
    "title": "Validate",
    "section": "Reporting Languages",
    "text": "Reporting Languages\nVarious pieces of reporting in Pointblank can be localized to a specific language. This is done by setting the lang= parameter in Validate. Any of the following languages can be used (just provide the language code):\n\nEnglish (\"en\")\nFrench (\"fr\")\nGerman (\"de\")\nItalian (\"it\")\nSpanish (\"es\")\nPortuguese (\"pt\")\nDutch (\"nl\")\nSwedish (\"sv\")\nDanish (\"da\")\nNorwegian Bokmål (\"nb\")\nIcelandic (\"is\")\nFinnish (\"fi\")\nPolish (\"pl\")\nCzech (\"cs\")\nRomanian (\"ro\")\nGreek (\"el\")\nRussian (\"ru\")\nTurkish (\"tr\")\nArabic (\"ar\")\nHindi (\"hi\")\nSimplified Chinese (\"zh-Hans\")\nTraditional Chinese (\"zh-Hant\")\nJapanese (\"ja\")\nKorean (\"ko\")\nVietnamese (\"vi\")\n\nAutomatically generated briefs (produced by using brief=True or brief=\"...{auto}...\") will be written in the selected language. The language setting will also used when generating the validation report table through get_tabular_report() (or printing the Validate object in a notebook environment)."
  },
  {
    "objectID": "reference/Validate.html#examples",
    "href": "reference/Validate.html#examples",
    "title": "Validate",
    "section": "Examples",
    "text": "Examples\n\nCreating a validation plan and interrogating\nLet’s walk through a data quality analysis of an extremely small table. It’s actually called \"small_table\" and it’s accessible through the load_dataset() function.\n\nimport pointblank as pb\n\n# Load the small_table dataset\nsmall_table = pb.load_dataset()\n\n# Preview the table\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nWe ought to think about what’s tolerable in terms of data quality so let’s designate proportional failure thresholds to the ‘warning’, ‘error’, and ‘critical’ states. This can be done by using the Thresholds class.\n\nthresholds = pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n\nNow, we use the Validate class and give it the thresholds object (which serves as a default for all validation steps but can be overridden). The static thresholds provided in thresholds= will make the reporting a bit more useful. We also need to provide a target table and we’ll use small_table for this.\n\nvalidation = (\n    pb.Validate(\n        data=small_table,\n        tbl_name=\"small_table\",\n        label=\"`Validate` example.\",\n        thresholds=thresholds\n    )\n)\n\nThen, as with any Validate object, we can add steps to the validation plan by using as many validation methods as we want. To conclude the process (and actually query the data table), we use the interrogate() method.\n\nvalidation = (\n    validation\n    .col_vals_gt(columns=\"d\", value=100)\n    .col_vals_le(columns=\"c\", value=5)\n    .col_vals_between(columns=\"c\", left=3, right=10, na_pass=True)\n    .col_vals_regex(columns=\"b\", pattern=r\"[0-9]-[a-z]{3}-[0-9]{3}\")\n    .col_exists(columns=[\"date\", \"date_time\"])\n    .interrogate()\n)\n\nThe validation object can be printed as a reporting table.\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    `Validate` example.Polarssmall_tableWARNING0.1ERROR0.25CRITICAL0.35\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    c\n    [3, 10]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    [0-9]-[a-z]{3}-[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:16:47 UTC&lt; 1 s2025-05-23 02:16:47 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe report could be further customized by using the get_tabular_report() method, which contains options for modifying the display of the table.\n\n\nAdding briefs\nBriefs are short descriptions of the validation steps. While they can be set for each step individually, they can also be set globally. The global setting is done by using the brief= argument in Validate. The global setting can be as simple as True to have automatically-generated briefs for each step. Alternatively, we can use templating elements like \"{step}\" (to insert the step number) or \"{auto}\" (to include an automatically generated brief). Here’s an example of a global setting for briefs:\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(),\n        tbl_name=\"small_table\",\n        label=\"Validation example with briefs\",\n        brief=\"Step {step}: {auto}\",\n    )\n    .col_vals_gt(columns=\"d\", value=100)\n    .col_vals_between(columns=\"c\", left=3, right=10, na_pass=True)\n    .col_vals_regex(\n        columns=\"b\",\n        pattern=r\"[0-9]-[a-z]{3}-[0-9]{3}\",\n        brief=\"Regex check for column {col}\"\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Validation example with briefsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Step 1: Expect that values in d should be &gt; 100.\n\n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        Step 2: Expect that values in c should be between 3 and 10.\n\n        \n    c\n    [3, 10]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        Regex check for column b\n\n        \n    b\n    [0-9]-[a-z]{3}-[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:16:48 UTC&lt; 1 s2025-05-23 02:16:48 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe see the text of the briefs appear in the STEP column of the reporting table. Furthermore, the global brief’s template (\"Step {step}: {auto}\") is applied to all steps except for the final step, where the step-level brief= argument provided an override.\nIf you should want to cancel the globally-defined brief for one or more validation steps, you can set brief=False in those particular steps.\n\n\nPost-interrogation methods\nThe Validate class has a number of post-interrogation methods that can be used to extract useful information from the validation results. For example, the get_data_extracts() method can be used to get the data extracts for each validation step.\n\nvalidation.get_data_extracts()\n\n{1: shape: (0, 9)\n ┌───────────┬──────────────┬──────┬─────┬───┬─────┬─────┬──────┬─────┐\n │ _row_num_ ┆ date_time    ┆ date ┆ a   ┆ … ┆ c   ┆ d   ┆ e    ┆ f   │\n │ ---       ┆ ---          ┆ ---  ┆ --- ┆   ┆ --- ┆ --- ┆ ---  ┆ --- │\n │ u32       ┆ datetime[μs] ┆ date ┆ i64 ┆   ┆ i64 ┆ f64 ┆ bool ┆ str │\n ╞═══════════╪══════════════╪══════╪═════╪═══╪═════╪═════╪══════╪═════╡\n └───────────┴──────────────┴──────┴─────┴───┴─────┴─────┴──────┴─────┘,\n 2: shape: (1, 9)\n ┌───────────┬─────────────────────┬────────────┬─────┬───┬─────┬─────────┬───────┬─────┐\n │ _row_num_ ┆ date_time           ┆ date       ┆ a   ┆ … ┆ c   ┆ d       ┆ e     ┆ f   │\n │ ---       ┆ ---                 ┆ ---        ┆ --- ┆   ┆ --- ┆ ---     ┆ ---   ┆ --- │\n │ u32       ┆ datetime[μs]        ┆ date       ┆ i64 ┆   ┆ i64 ┆ f64     ┆ bool  ┆ str │\n ╞═══════════╪═════════════════════╪════════════╪═════╪═══╪═════╪═════════╪═══════╪═════╡\n │ 8         ┆ 2016-01-17 11:27:00 ┆ 2016-01-17 ┆ 4   ┆ … ┆ 2   ┆ 1035.64 ┆ false ┆ low │\n └───────────┴─────────────────────┴────────────┴─────┴───┴─────┴─────────┴───────┴─────┘,\n 3: shape: (0, 9)\n ┌───────────┬──────────────┬──────┬─────┬───┬─────┬─────┬──────┬─────┐\n │ _row_num_ ┆ date_time    ┆ date ┆ a   ┆ … ┆ c   ┆ d   ┆ e    ┆ f   │\n │ ---       ┆ ---          ┆ ---  ┆ --- ┆   ┆ --- ┆ --- ┆ ---  ┆ --- │\n │ u32       ┆ datetime[μs] ┆ date ┆ i64 ┆   ┆ i64 ┆ f64 ┆ bool ┆ str │\n ╞═══════════╪══════════════╪══════╪═════╪═══╪═════╪═════╪══════╪═════╡\n └───────────┴──────────────┴──────┴─────┴───┴─────┴─────┴──────┴─────┘}\n\n\nWe can also view step reports for each validation step using the get_step_report() method. This method adapts to the type of validation step and shows the relevant information for a step’s validation.\n\nvalidation.get_step_report(i=2)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION 3 ≤ c ≤ 101 / 13 TEST UNIT FAILURES IN COLUMN 5 EXTRACT OF ALL 1 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\nThe Validate class also has a method for getting the sundered data, which is the data that passed or failed the validation steps. This can be done using the get_sundered_data() method.\n\npb.preview(validation.get_sundered_data())\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows12Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    8\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    11\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    12\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThe sundered data is a DataFrame that contains the rows that passed or failed the validation. The default behavior is to return the rows that failed the validation, as shown above."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html",
    "href": "reference/Validate.rows_distinct.html",
    "title": "Validate.rows_distinct",
    "section": "",
    "text": "Validate.rows_distinct(\n    columns_subset=None,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether rows in the table are distinct.\nThe rows_distinct() method checks whether rows in the table are distinct. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#parameters",
    "href": "reference/Validate.rows_distinct.html#parameters",
    "title": "Validate.rows_distinct",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns_subset : str | list[str] | None = None\n\nA single column or a list of columns to use as a subset for the distinct comparison. If None, then all columns in the table will be used for the comparison. If multiple columns are supplied, the distinct comparison will be made over the combination of values in those columns.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#returns",
    "href": "reference/Validate.rows_distinct.html#returns",
    "title": "Validate.rows_distinct",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#preprocessing",
    "href": "reference/Validate.rows_distinct.html#preprocessing",
    "title": "Validate.rows_distinct",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns_subset= that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#segmentation",
    "href": "reference/Validate.rows_distinct.html#segmentation",
    "title": "Validate.rows_distinct",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#thresholds",
    "href": "reference/Validate.rows_distinct.html#thresholds",
    "title": "Validate.rows_distinct",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#examples",
    "href": "reference/Validate.rows_distinct.html#examples",
    "title": "Validate.rows_distinct",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three string columns (col_1, col_2, and col_3). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"col_1\": [\"a\", \"b\", \"c\", \"d\"],\n        \"col_2\": [\"a\", \"a\", \"c\", \"d\"],\n        \"col_3\": [\"a\", \"a\", \"d\", \"e\"],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  col_1String\n  col_2String\n  col_3String\n\n\n\n  \n    1\n    a\n    a\n    a\n  \n  \n    2\n    b\n    a\n    a\n  \n  \n    3\n    c\n    c\n    d\n  \n  \n    4\n    d\n    d\n    e\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the rows in the table are distinct with rows_distinct(). We’ll determine if this validation had any failing test units (there are four test units, one for each row). A failing test units means that a given row is not distinct from every other row.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom this validation table we see that there are no failing test units. All rows in the table are distinct from one another.\nWe can also use a subset of columns to determine distinctness. Let’s specify the subset using columns col_2 and col_3 for the next validation.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_distinct(columns_subset=[\"col_2\", \"col_3\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    col_2, col_3\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The first and second rows are duplicated when considering only the values in columns col_2 and col_3. There’s only one set of duplicates but there are two failing test units since each row is compared to all others."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html",
    "href": "reference/Validate.col_vals_regex.html",
    "title": "Validate.col_vals_regex",
    "section": "",
    "text": "Validate.col_vals_regex(\n    columns,\n    pattern,\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values match a regular expression pattern.\nThe col_vals_regex() validation method checks whether column values in a table correspond to a pattern= matching expression. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#parameters",
    "href": "reference/Validate.col_vals_regex.html#parameters",
    "title": "Validate.col_vals_regex",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npattern : str\n\nA regular expression pattern to compare against.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#returns",
    "href": "reference/Validate.col_vals_regex.html#returns",
    "title": "Validate.col_vals_regex",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#preprocessing",
    "href": "reference/Validate.col_vals_regex.html#preprocessing",
    "title": "Validate.col_vals_regex",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#segmentation",
    "href": "reference/Validate.col_vals_regex.html#segmentation",
    "title": "Validate.col_vals_regex",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#thresholds",
    "href": "reference/Validate.col_vals_regex.html#thresholds",
    "title": "Validate.col_vals_regex",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#examples",
    "href": "reference/Validate.col_vals_regex.html#examples",
    "title": "Validate.col_vals_regex",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two string columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"rb-0343\", \"ra-0232\", \"ry-0954\", \"rc-1343\"],\n        \"b\": [\"ra-0628\", \"ra-583\", \"rya-0826\", \"rb-0735\"],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bString\n\n\n\n  \n    1\n    rb-0343\n    ra-0628\n  \n  \n    2\n    ra-0232\n    ra-583\n  \n  \n    3\n    ry-0954\n    rya-0826\n  \n  \n    4\n    rc-1343\n    rb-0735\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that all of the values in column a match a particular regex pattern. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=\"a\", pattern=r\"r[a-z]-[0-9]{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    a\n    r[a-z]-[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_regex(). All test units passed, and there are no failing test units.\nNow, let’s use the same regex for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=\"b\", pattern=r\"r[a-z]-[0-9]{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    r[a-z]-[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the string values of rows 1 and 2 in column b."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html",
    "href": "reference/Validate.col_vals_in_set.html",
    "title": "Validate.col_vals_in_set",
    "section": "",
    "text": "Validate.col_vals_in_set(\n    columns,\n    set,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values are in a set of values.\nThe col_vals_in_set() validation method checks whether column values in a table are part of a specified set= of values. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#parameters",
    "href": "reference/Validate.col_vals_in_set.html#parameters",
    "title": "Validate.col_vals_in_set",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nset : Collection[Any]\n\nA list of values to compare against.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#returns",
    "href": "reference/Validate.col_vals_in_set.html#returns",
    "title": "Validate.col_vals_in_set",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#preprocessing",
    "href": "reference/Validate.col_vals_in_set.html#preprocessing",
    "title": "Validate.col_vals_in_set",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#segmentation",
    "href": "reference/Validate.col_vals_in_set.html#segmentation",
    "title": "Validate.col_vals_in_set",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#thresholds",
    "href": "reference/Validate.col_vals_in_set.html#thresholds",
    "title": "Validate.col_vals_in_set",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#examples",
    "href": "reference/Validate.col_vals_in_set.html#examples",
    "title": "Validate.col_vals_in_set",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 2, 4, 6, 2, 5],\n        \"b\": [5, 8, 2, 6, 5, 1],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    2\n    8\n  \n  \n    3\n    4\n    2\n  \n  \n    4\n    6\n    6\n  \n  \n    5\n    2\n    5\n  \n  \n    6\n    5\n    1\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all in the set of [2, 3, 4, 5, 6]. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_in_set(columns=\"a\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    a\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_in_set(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_in_set(columns=\"b\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    b\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the column b values of 8 and 1, which are not in the set of [2, 3, 4, 5, 6]."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html",
    "href": "reference/Validate.get_tabular_report.html",
    "title": "Validate.get_tabular_report",
    "section": "",
    "text": "Validate.get_tabular_report(\n    title=':default:',\n    incl_header=None,\n    incl_footer=None,\n)\nValidation report as a GT table.\nThe get_tabular_report() method returns a GT table object that represents the validation report. This validation table provides a summary of the validation results, including the validation steps, the number of test units, the number of failing test units, and the fraction of failing test units. The table also includes status indicators for the ‘warning’, ‘error’, and ‘critical’ levels.\nYou could simply display the validation table without the use of the get_tabular_report() method. However, the method provides a way to customize the title of the report. In the future this method may provide additional options for customizing the report."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#parameters",
    "href": "reference/Validate.get_tabular_report.html#parameters",
    "title": "Validate.get_tabular_report",
    "section": "Parameters",
    "text": "Parameters\n\ntitle : str | None = ':default:'\n\nOptions for customizing the title of the report. The default is the \":default:\" value which produces a generic title. Another option is \":tbl_name:\", and that presents the name of the table as the title for the report. If no title is wanted, then \":none:\" can be used. Aside from keyword options, text can be provided for the title. This will be interpreted as Markdown text and transformed internally to HTML."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#returns",
    "href": "reference/Validate.get_tabular_report.html#returns",
    "title": "Validate.get_tabular_report",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT table object that represents the validation report."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#examples",
    "href": "reference/Validate.get_tabular_report.html#examples",
    "title": "Validate.get_tabular_report",
    "section": "Examples",
    "text": "Examples\nLet’s create a Validate object with a few validation steps and then interrogate the data table to see how it performs against the validation plan. We can then generate a tabular report to get a summary of the results.\n\nimport pointblank as pb\nimport polars as pl\n\n# Create a Polars DataFrame\ntbl_pl = pl.DataFrame({\"x\": [1, 2, 3, 4], \"y\": [4, 5, 6, 7]})\n\n# Validate data using Polars DataFrame\nvalidation = (\n    pb.Validate(data=tbl_pl, tbl_name=\"tbl_xy\", thresholds=(2, 3, 4))\n    .col_vals_gt(columns=\"x\", value=1)\n    .col_vals_lt(columns=\"x\", value=3)\n    .col_vals_le(columns=\"y\", value=7)\n    .interrogate()\n)\n\n# Look at the validation table\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:16:27Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:16:27 UTC&lt; 1 s2025-05-23 02:16:27 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe validation table is displayed with a default title (‘Validation Report’). We can use the get_tabular_report() method to customize the title of the report. For example, we can set the title to the name of the table by using the title=\":tbl_name:\" option. This will use the string provided in the tbl_name= argument of the Validate object.\n\nvalidation.get_tabular_report(title=\":tbl_name:\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    tbl_xy\n  \n  \n    2025-05-23|02:16:27Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:16:27 UTC&lt; 1 s2025-05-23 02:16:27 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe title of the report is now set to the name of the table, which is ‘tbl_xy’. This can be useful if you have multiple tables and want to keep track of which table the validation report is for.\nAlternatively, you can provide your own title for the report.\n\nvalidation.get_tabular_report(title=\"Report for Table XY\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Table XY\n\n  \n  \n    2025-05-23|02:16:27Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:16:27 UTC&lt; 1 s2025-05-23 02:16:27 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe title of the report is now set to ‘Report for Table XY’. This can be useful if you want to provide a more descriptive title for the report."
  },
  {
    "objectID": "reference/send_slack_notification.html",
    "href": "reference/send_slack_notification.html",
    "title": "send_slack_notification",
    "section": "",
    "text": "send_slack_notification(\n    webhook_url=None,\n    step_msg=None,\n    summary_msg=None,\n    debug=False,\n)\nCreate a Slack notification function using a webhook URL.\nThis function can be used in two ways:\n\nWith Actions to notify about individual validation step failures\nWith FinalActions to provide a summary notification after all validation steps have undergone interrogation\n\nThe function creates a callable that sends notifications through a Slack webhook. Message formatting can be customized using templates for both individual steps and summary reports.\n\n\n\nwebhook_url : str | None = None\n\nThe Slack webhook URL. If None (and debug=True), a dry run is performed (see the Offline Testing section below for information on this).\n\nstep_msg : str | None = None\n\nTemplate string for step notifications. Some of the available variables include: \"{step}\", \"{column}\", \"{value}\", \"{type}\", \"{time}\", \"{level}\", etc. See the Available Template Variables for Step Notifications section below for more details. If not provided, a default step message template will be used.\n\nsummary_msg : str | None = None\n\nTemplate string for summary notifications. Some of the available variables are: \"{n_steps}\", \"{n_passing_steps}\", \"{n_failing_steps}\", \"{all_passed}\", \"{highest_severity}\", etc. See the Available Template Variables for Summary Notifications section below for more details. If not provided, a default summary message template will be used.\n\ndebug : bool = False\n\nPrint debug information if True. This includes the message content and the response from Slack. This is useful for testing and debugging the notification function. If webhook_url is None, the function will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly.\n\n\n\n\n\n\n : Callable\n\nA function that sends notifications to Slack.\n\n\n\n\n\nWhen creating a custom template for validation step alerts (step_msg=), the following templating strings can be used:\n\n\"{step}\": The step number.\n\"{column}\": The column name.\n\"{value}\": The value being compared (only available in certain validation steps).\n\"{type}\": The assertion type (e.g., \"col_vals_gt\", etc.).\n\"{level}\": The severity level (\"warning\", \"error\", or \"critical\").\n\"{level_num}\": The severity level as a numeric value (30, 40, or 50).\n\"{autobrief}\": A localized and brief statement of the expectation for the step.\n\"{failure_text}\": Localized text that explains how the validation step failed.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to construct a step_msg= template:\nstep_msg = '''🚨 *Validation Step Alert*\n• Step Number: {step}\n• Column: {column}\n• Test Type: {type}\n• Value Tested: {value}\n• Severity: {level} (level {level_num})\n• Brief: {autobrief}\n• Details: {failure_text}\n• Time: {time}'''\nThis template will be filled with the relevant information when a validation step fails. The placeholders will be replaced with actual values when the Slack notification is sent.\n\n\n\nWhen creating a custom template for a validation summary (summary_msg=), the following templating strings can be used:\n\n\"{n_steps}\": The total number of validation steps.\n\"{n_passing_steps}\": The number of validation steps where all test units passed.\n\"{n_failing_steps}\": The number of validation steps that had some failing test units.\n\"{n_warning_steps}\": The number of steps that exceeded a ‘warning’ threshold.\n\"{n_error_steps}\": The number of steps that exceeded an ‘error’ threshold.\n\"{n_critical_steps}\": The number of steps that exceeded a ‘critical’ threshold.\n\"{all_passed}\": Whether or not every validation step had no failing test units.\n\"{highest_severity}\": The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\n\"{tbl_row_count}\": The number of rows in the target table.\n\"{tbl_column_count}\": The number of columns in the target table.\n\"{tbl_name}\": The name of the target table.\n\"{validation_duration}\": The duration of the validation in seconds.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to put together a summary_msg= template:\nsummary_msg = '''📊 *Validation Summary Report*\n*Overview*\n• Status: {highest_severity}\n• All Passed: {all_passed}\n• Total Steps: {n_steps}\n\n*Step Results*\n• Passing Steps: {n_passing_steps}\n• Failing Steps: {n_failing_steps}\n• Warning Level: {n_warning_steps}\n• Error Level: {n_error_steps}\n• Critical Level: {n_critical_steps}\n\n*Table Info*\n• Table Name: {tbl_name}\n• Row Count: {tbl_row_count}\n• Column Count: {tbl_column_count}\n\n*Timing*\n• Duration: {validation_duration}s\n• Completed: {time}'''\nThis template will be filled with the relevant information when the validation summary is generated. The placeholders will be replaced with actual values when the Slack notification is sent.\n\n\n\nIf you want to test the function without sending actual notifications, you can leave the webhook_url= as None and set debug=True. This will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly. Furthermore, the function could be run globally (i.e., outside of the context of a validation plan) to show the message templates with all possible variables. Here’s an example of how to do this:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None,  # Leave as None for dry run\n    debug=True,  # Enable debug mode to print message previews\n)\n# Call the function to see the message previews\nnotify_slack()\nThis will print the step and summary message previews to the console, allowing you to see how the templates will look when filled with actual data. You can then adjust your templates as needed before using them in a real validation plan.\nWhen step_msg= and summary_msg= are not provided, the function will use default templates. However, you can customize the templates to include additional information or change the format to better suit your needs. Iterating on the templates can help you create more informative and visually appealing messages. Here’s an example of that:\nimport pointblank as pb\n\n# Create a Slack notification function with custom templates\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None, # Leave as None for dry run\n    step_msg='''*Data Validation Alert*\n    • Type: {type}\n    • Level: {level}\n    • Step: {step}\n    • Column: {column}\n    • Time: {time}''',\n    summary_msg='''*Data Validation Summary*\n    • Highest Severity: {highest_severity}\n    • Total Steps: {n_steps}\n    • Failed Steps: {n_failing_steps}\n    • Time: {time}''',\n    debug=True,  # Enable debug mode to print message previews\n)\nThese templates will be used with sample data when the function is called. The combination of webhook_url=None and debug=True allows you to test your custom templates without having to send actual notifications to Slack.\n\n\n\nWhen using an action with one or more validation steps, you typically provide callables that fire when a matched threshold of failed test units is exceeded. The callable can be a function or a lambda. The send_slack_notification() function creates a callable that sends a Slack notification when the validation step fails. Here is how it can be set up to work for multiple validation steps by using of Actions:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation\nBy placing the notify_slack() function in the Validate(actions=Actions(critical=)) argument, you can ensure that the notification is sent whenever the ‘critical’ threshold is reached (as set here, when 15% or more of the test units fail). The notification will include information about the validation step that triggered the alert.\nWhen using a FinalActions object, the notification will be sent after all validation steps have been completed. This is useful for providing a summary of the validation process. Here is an example of how to set up a summary notification:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this case, the same notify_slack() function is used, but it is placed in Validate(final_actions=FinalActions()). This results in the summary notification being sent after all validation steps are completed, regardless of whether any steps failed or not.\nThis simplicity is possible because the send_slack_notification() function creates a callable that can be used in both contexts. The function will automatically determine whether to send a step notification or a summary notification based on the context in which it is called.\nWe can customize the message templates for both step and summary notifications. In that way, it’s possible to create a more informative and visually appealing message. For example, we can use Markdown formatting to make the message more readable and visually appealing. Here is an example of how to customize the templates:\nimport pointblank as pb\n# Create a Slack notification function\n\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\",\n    step_msg='''\n    🚨 *Validation Step Alert*\n    • Step Number: {step}\n    • Column: {column}\n    • Test Type: {type}\n    • Value Tested: {value}\n    • Severity: {level} (level {level_num})\n    • Brief: {autobrief}\n    • Details: {failure_text}\n    • Time: {time}''',\n    summary_msg='''\n    📊 *Validation Summary Report*\n    *Overview*\n    • Status: {highest_severity}\n    • All Passed: {all_passed}\n    • Total Steps: {n_steps}\n\n    *Step Results*\n    • Passing Steps: {n_passing_steps}\n    • Failing Steps: {n_failing_steps}\n    • Warning Level: {n_warning_steps}\n    • Error Level: {n_error_steps}\n    • Critical Level: {n_critical_steps}\n\n    *Table Info*\n    • Table Name: {tbl_name}\n    • Row Count: {tbl_row_count}\n    • Column Count: {tbl_column_count}\n\n    *Timing*\n    • Duration: {validation_duration}s\n    • Completed: {time}''',\n)\n\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(default=notify_slack),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this example, we have customized the templates for both step and summary notifications. The step notification includes details about the validation step, including the step number, column name, test type, value tested, severity level, brief description, and time of the notification. The summary notification includes an overview of the validation process, including the status, number of steps, passing and failing steps, table information, and timing details."
  },
  {
    "objectID": "reference/send_slack_notification.html#parameters",
    "href": "reference/send_slack_notification.html#parameters",
    "title": "send_slack_notification",
    "section": "",
    "text": "webhook_url : str | None = None\n\nThe Slack webhook URL. If None (and debug=True), a dry run is performed (see the Offline Testing section below for information on this).\n\nstep_msg : str | None = None\n\nTemplate string for step notifications. Some of the available variables include: \"{step}\", \"{column}\", \"{value}\", \"{type}\", \"{time}\", \"{level}\", etc. See the Available Template Variables for Step Notifications section below for more details. If not provided, a default step message template will be used.\n\nsummary_msg : str | None = None\n\nTemplate string for summary notifications. Some of the available variables are: \"{n_steps}\", \"{n_passing_steps}\", \"{n_failing_steps}\", \"{all_passed}\", \"{highest_severity}\", etc. See the Available Template Variables for Summary Notifications section below for more details. If not provided, a default summary message template will be used.\n\ndebug : bool = False\n\nPrint debug information if True. This includes the message content and the response from Slack. This is useful for testing and debugging the notification function. If webhook_url is None, the function will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly."
  },
  {
    "objectID": "reference/send_slack_notification.html#returns",
    "href": "reference/send_slack_notification.html#returns",
    "title": "send_slack_notification",
    "section": "",
    "text": ": Callable\n\nA function that sends notifications to Slack."
  },
  {
    "objectID": "reference/send_slack_notification.html#available-template-variables-for-step-notifications",
    "href": "reference/send_slack_notification.html#available-template-variables-for-step-notifications",
    "title": "send_slack_notification",
    "section": "",
    "text": "When creating a custom template for validation step alerts (step_msg=), the following templating strings can be used:\n\n\"{step}\": The step number.\n\"{column}\": The column name.\n\"{value}\": The value being compared (only available in certain validation steps).\n\"{type}\": The assertion type (e.g., \"col_vals_gt\", etc.).\n\"{level}\": The severity level (\"warning\", \"error\", or \"critical\").\n\"{level_num}\": The severity level as a numeric value (30, 40, or 50).\n\"{autobrief}\": A localized and brief statement of the expectation for the step.\n\"{failure_text}\": Localized text that explains how the validation step failed.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to construct a step_msg= template:\nstep_msg = '''🚨 *Validation Step Alert*\n• Step Number: {step}\n• Column: {column}\n• Test Type: {type}\n• Value Tested: {value}\n• Severity: {level} (level {level_num})\n• Brief: {autobrief}\n• Details: {failure_text}\n• Time: {time}'''\nThis template will be filled with the relevant information when a validation step fails. The placeholders will be replaced with actual values when the Slack notification is sent."
  },
  {
    "objectID": "reference/send_slack_notification.html#available-template-variables-for-summary-notifications",
    "href": "reference/send_slack_notification.html#available-template-variables-for-summary-notifications",
    "title": "send_slack_notification",
    "section": "",
    "text": "When creating a custom template for a validation summary (summary_msg=), the following templating strings can be used:\n\n\"{n_steps}\": The total number of validation steps.\n\"{n_passing_steps}\": The number of validation steps where all test units passed.\n\"{n_failing_steps}\": The number of validation steps that had some failing test units.\n\"{n_warning_steps}\": The number of steps that exceeded a ‘warning’ threshold.\n\"{n_error_steps}\": The number of steps that exceeded an ‘error’ threshold.\n\"{n_critical_steps}\": The number of steps that exceeded a ‘critical’ threshold.\n\"{all_passed}\": Whether or not every validation step had no failing test units.\n\"{highest_severity}\": The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\n\"{tbl_row_count}\": The number of rows in the target table.\n\"{tbl_column_count}\": The number of columns in the target table.\n\"{tbl_name}\": The name of the target table.\n\"{validation_duration}\": The duration of the validation in seconds.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to put together a summary_msg= template:\nsummary_msg = '''📊 *Validation Summary Report*\n*Overview*\n• Status: {highest_severity}\n• All Passed: {all_passed}\n• Total Steps: {n_steps}\n\n*Step Results*\n• Passing Steps: {n_passing_steps}\n• Failing Steps: {n_failing_steps}\n• Warning Level: {n_warning_steps}\n• Error Level: {n_error_steps}\n• Critical Level: {n_critical_steps}\n\n*Table Info*\n• Table Name: {tbl_name}\n• Row Count: {tbl_row_count}\n• Column Count: {tbl_column_count}\n\n*Timing*\n• Duration: {validation_duration}s\n• Completed: {time}'''\nThis template will be filled with the relevant information when the validation summary is generated. The placeholders will be replaced with actual values when the Slack notification is sent."
  },
  {
    "objectID": "reference/send_slack_notification.html#offline-testing",
    "href": "reference/send_slack_notification.html#offline-testing",
    "title": "send_slack_notification",
    "section": "",
    "text": "If you want to test the function without sending actual notifications, you can leave the webhook_url= as None and set debug=True. This will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly. Furthermore, the function could be run globally (i.e., outside of the context of a validation plan) to show the message templates with all possible variables. Here’s an example of how to do this:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None,  # Leave as None for dry run\n    debug=True,  # Enable debug mode to print message previews\n)\n# Call the function to see the message previews\nnotify_slack()\nThis will print the step and summary message previews to the console, allowing you to see how the templates will look when filled with actual data. You can then adjust your templates as needed before using them in a real validation plan.\nWhen step_msg= and summary_msg= are not provided, the function will use default templates. However, you can customize the templates to include additional information or change the format to better suit your needs. Iterating on the templates can help you create more informative and visually appealing messages. Here’s an example of that:\nimport pointblank as pb\n\n# Create a Slack notification function with custom templates\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None, # Leave as None for dry run\n    step_msg='''*Data Validation Alert*\n    • Type: {type}\n    • Level: {level}\n    • Step: {step}\n    • Column: {column}\n    • Time: {time}''',\n    summary_msg='''*Data Validation Summary*\n    • Highest Severity: {highest_severity}\n    • Total Steps: {n_steps}\n    • Failed Steps: {n_failing_steps}\n    • Time: {time}''',\n    debug=True,  # Enable debug mode to print message previews\n)\nThese templates will be used with sample data when the function is called. The combination of webhook_url=None and debug=True allows you to test your custom templates without having to send actual notifications to Slack."
  },
  {
    "objectID": "reference/send_slack_notification.html#examples",
    "href": "reference/send_slack_notification.html#examples",
    "title": "send_slack_notification",
    "section": "",
    "text": "When using an action with one or more validation steps, you typically provide callables that fire when a matched threshold of failed test units is exceeded. The callable can be a function or a lambda. The send_slack_notification() function creates a callable that sends a Slack notification when the validation step fails. Here is how it can be set up to work for multiple validation steps by using of Actions:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation\nBy placing the notify_slack() function in the Validate(actions=Actions(critical=)) argument, you can ensure that the notification is sent whenever the ‘critical’ threshold is reached (as set here, when 15% or more of the test units fail). The notification will include information about the validation step that triggered the alert.\nWhen using a FinalActions object, the notification will be sent after all validation steps have been completed. This is useful for providing a summary of the validation process. Here is an example of how to set up a summary notification:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this case, the same notify_slack() function is used, but it is placed in Validate(final_actions=FinalActions()). This results in the summary notification being sent after all validation steps are completed, regardless of whether any steps failed or not.\nThis simplicity is possible because the send_slack_notification() function creates a callable that can be used in both contexts. The function will automatically determine whether to send a step notification or a summary notification based on the context in which it is called.\nWe can customize the message templates for both step and summary notifications. In that way, it’s possible to create a more informative and visually appealing message. For example, we can use Markdown formatting to make the message more readable and visually appealing. Here is an example of how to customize the templates:\nimport pointblank as pb\n# Create a Slack notification function\n\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\",\n    step_msg='''\n    🚨 *Validation Step Alert*\n    • Step Number: {step}\n    • Column: {column}\n    • Test Type: {type}\n    • Value Tested: {value}\n    • Severity: {level} (level {level_num})\n    • Brief: {autobrief}\n    • Details: {failure_text}\n    • Time: {time}''',\n    summary_msg='''\n    📊 *Validation Summary Report*\n    *Overview*\n    • Status: {highest_severity}\n    • All Passed: {all_passed}\n    • Total Steps: {n_steps}\n\n    *Step Results*\n    • Passing Steps: {n_passing_steps}\n    • Failing Steps: {n_failing_steps}\n    • Warning Level: {n_warning_steps}\n    • Error Level: {n_error_steps}\n    • Critical Level: {n_critical_steps}\n\n    *Table Info*\n    • Table Name: {tbl_name}\n    • Row Count: {tbl_row_count}\n    • Column Count: {tbl_column_count}\n\n    *Timing*\n    • Duration: {validation_duration}s\n    • Completed: {time}''',\n)\n\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(default=notify_slack),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this example, we have customized the templates for both step and summary notifications. The step notification includes details about the validation step, including the step number, column name, test type, value tested, severity level, brief description, and time of the notification. The summary notification includes an overview of the validation process, including the status, number of steps, passing and failing steps, table information, and timing details."
  },
  {
    "objectID": "reference/DataScan.html",
    "href": "reference/DataScan.html",
    "title": "DataScan",
    "section": "",
    "text": "DataScan(self, data, tbl_name=None)\nGet a summary of a dataset.\nThe DataScan class provides a way to get a summary of a dataset. The summary includes the following information:\n\nthe name of the table (if provided)\nthe type of the table (e.g., \"polars\", \"pandas\", etc.)\nthe number of rows and columns in the table\ncolumn-level information, including:\n\nthe column name\nthe column type\nmeasures of missingness and distinctness\nmeasures of negative, zero, and positive values (for numerical columns)\na sample of the data (the first 5 values)\nstatistics (if the column contains numbers, strings, or datetimes)\n\n\nTo obtain a dictionary representation of the summary, you can use the to_dict() method. To get a JSON representation of the summary, you can use the to_json() method. To save the JSON text to a file, the save_to_json() method could be used.\n\n\n\n\n\n\nWarning\n\n\n\nThe DataScan() class is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\ndata : FrameT | Any\n\nThe data to scan and summarize.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name.\n\n\n\n\n\nFor each column, the following measures are provided:\n\nn_missing_values: the number of missing values in the column\nf_missing_values: the fraction of missing values in the column\nn_unique_values: the number of unique values in the column\nf_unique_values: the fraction of unique values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset.\n\n\n\nFor numerical columns, the following measures are provided:\n\nn_negative_values: the number of negative values in the column\nf_negative_values: the fraction of negative values in the column\nn_zero_values: the number of zero values in the column\nf_zero_values: the fraction of zero values in the column\nn_positive_values: the number of positive values in the column\nf_positive_values: the fraction of positive values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset.\n\n\n\nFor numerical and string columns, several statistical measures are provided. Please note that for string columms, the statistics are based on the lengths of the strings in the column.\nThe following descriptive statistics are provided:\n\nmean: the mean of the column\nstd_dev: the standard deviation of the column\n\nAdditionally, the following quantiles are provided:\n\nmin: the minimum value in the column\np05: the 5th percentile of the column\nq_1: the first quartile of the column\nmed: the median of the column\nq_3: the third quartile of the column\np95: the 95th percentile of the column\nmax: the maximum value in the column\niqr: the interquartile range of the column\n\n\n\n\nFor date/datetime columns, the following statistics are provided:\n\nmin: the minimum date/datetime in the column\nmax: the maximum date/datetime in the column\n\n\n\n\n\n : DataScan\n\nA DataScan object."
  },
  {
    "objectID": "reference/DataScan.html#parameters",
    "href": "reference/DataScan.html#parameters",
    "title": "DataScan",
    "section": "",
    "text": "data : FrameT | Any\n\nThe data to scan and summarize.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name."
  },
  {
    "objectID": "reference/DataScan.html#measures-of-missingness-and-distinctness",
    "href": "reference/DataScan.html#measures-of-missingness-and-distinctness",
    "title": "DataScan",
    "section": "",
    "text": "For each column, the following measures are provided:\n\nn_missing_values: the number of missing values in the column\nf_missing_values: the fraction of missing values in the column\nn_unique_values: the number of unique values in the column\nf_unique_values: the fraction of unique values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset."
  },
  {
    "objectID": "reference/DataScan.html#counts-and-fractions-of-negative-zero-and-positive-values",
    "href": "reference/DataScan.html#counts-and-fractions-of-negative-zero-and-positive-values",
    "title": "DataScan",
    "section": "",
    "text": "For numerical columns, the following measures are provided:\n\nn_negative_values: the number of negative values in the column\nf_negative_values: the fraction of negative values in the column\nn_zero_values: the number of zero values in the column\nf_zero_values: the fraction of zero values in the column\nn_positive_values: the number of positive values in the column\nf_positive_values: the fraction of positive values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset."
  },
  {
    "objectID": "reference/DataScan.html#statistics-for-numerical-and-string-columns",
    "href": "reference/DataScan.html#statistics-for-numerical-and-string-columns",
    "title": "DataScan",
    "section": "",
    "text": "For numerical and string columns, several statistical measures are provided. Please note that for string columms, the statistics are based on the lengths of the strings in the column.\nThe following descriptive statistics are provided:\n\nmean: the mean of the column\nstd_dev: the standard deviation of the column\n\nAdditionally, the following quantiles are provided:\n\nmin: the minimum value in the column\np05: the 5th percentile of the column\nq_1: the first quartile of the column\nmed: the median of the column\nq_3: the third quartile of the column\np95: the 95th percentile of the column\nmax: the maximum value in the column\niqr: the interquartile range of the column"
  },
  {
    "objectID": "reference/DataScan.html#statistics-for-date-and-datetime-columns",
    "href": "reference/DataScan.html#statistics-for-date-and-datetime-columns",
    "title": "DataScan",
    "section": "",
    "text": "For date/datetime columns, the following statistics are provided:\n\nmin: the minimum date/datetime in the column\nmax: the maximum date/datetime in the column"
  },
  {
    "objectID": "reference/DataScan.html#returns",
    "href": "reference/DataScan.html#returns",
    "title": "DataScan",
    "section": "",
    "text": ": DataScan\n\nA DataScan object."
  },
  {
    "objectID": "reference/Validate.assert_passing.html",
    "href": "reference/Validate.assert_passing.html",
    "title": "Validate.assert_passing",
    "section": "",
    "text": "Validate.assert_passing()\nRaise an AssertionError if all tests are not passing.\nThe assert_passing() method will raise an AssertionError if a test does not pass. This method simply wraps all_passed for more ready use in test suites. The step number and assertion made is printed in the AssertionError message if a failure occurs, ensuring some details are preserved.\nIf the validation has not yet been interrogated, this method will automatically call interrogate() with default parameters before checking for passing tests."
  },
  {
    "objectID": "reference/Validate.assert_passing.html#raises",
    "href": "reference/Validate.assert_passing.html#raises",
    "title": "Validate.assert_passing",
    "section": "Raises",
    "text": "Raises\n\n: AssertionError\n\nIf any validation step has failing test units."
  },
  {
    "objectID": "reference/Validate.assert_passing.html#examples",
    "href": "reference/Validate.assert_passing.html#examples",
    "title": "Validate.assert_passing",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the second step will have a failing test unit (the value 10 isn’t less than 9). The assert_passing() method is used to assert that all validation steps passed perfectly, automatically performing the interrogation if needed.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n    \"a\": [1, 2, 9, 5],\n    \"b\": [5, 6, 10, 3],\n    \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_vals_lt(columns=\"b\", value=9) # this assertion is false\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n)\n\n# No need to call [`interrogate()`](`pointblank.Validate.interrogate`) explicitly\nvalidation.assert_passing()\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/tmp/ipykernel_3194/2424908189.py in ?()\n     16     .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n     17 )\n     18 \n     19 # No need to call [`interrogate()`](`pointblank.Validate.interrogate`) explicitly\n---&gt; 20 validation.assert_passing()\n\n/opt/hostedtoolcache/Python/3.10.17/x64/lib/python3.10/site-packages/pointblank/validate.py in ?(self)\n   8852             ]\n   8853             msg = \"The following assertions failed:\\n\" + \"\\n\".join(\n   8854                 [f\"- Step {i + 1}: {autobrief}\" for i, autobrief in failed_steps]\n   8855             )\n-&gt; 8856             raise AssertionError(msg)\n\nAssertionError: The following assertions failed:\n- Step 2: Expect that values in `b` should be &lt; `9`."
  },
  {
    "objectID": "reference/FinalActions.html",
    "href": "reference/FinalActions.html",
    "title": "FinalActions",
    "section": "",
    "text": "FinalActions(self, *args)\nDefine actions to be taken after validation is complete.\nFinal actions are executed after all validation steps have been completed. They provide a mechanism to respond to the overall validation results, such as sending alerts when critical failures are detected or generating summary reports.\n\n\n\n*actions : \n\nOne or more actions to execute after validation. An action can be (1) a callable function that will be executed with no arguments, or (2) a string message that will be printed to the console.\n\n\n\n\n\n\n : FinalActions\n\nAn FinalActions object. This can be used when using the Validate class (to set final actions for the validation workflow).\n\n\n\n\n\nFinal actions can be defined in two different ways:\n\nString: A message to be displayed when the validation is complete.\nCallable: A function that is called when the validation is complete.\n\nThe actions are executed at the end of the validation workflow. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of validation completion. Several strings and callables can be provided to the FinalActions class, and they will be executed in the order they are provided.\n\n\n\nWhen creating a callable function to be used as a final action, you can use the get_validation_summary() function to retrieve the summary of the validation results. This summary contains information about the validation workflow, including the number of test units, the number of failing test units, and the threshold levels that were exceeded. You can use this information to craft your final action message or to take specific actions based on the validation results.\n\n\n\nFinal actions provide a powerful way to respond to the overall results of a validation workflow. They’re especially useful for sending notifications, generating reports, or taking corrective actions based on the complete validation outcome.\nThe following example shows how to create a final action that checks for critical failures and sends an alert:\nimport pointblank as pb\n\ndef send_alert():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        print(f\"ALERT: Critical validation failures found in {summary['table_name']}\")\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_alert)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nIn this example, the send_alert() function is defined to check the validation summary for critical failures. If any are found, an alert message is printed to the console. The function is passed to the FinalActions class, which ensures it will be executed after all validation steps are complete. Note that we used the get_validation_summary() function to retrieve the summary of the validation results to help craft the alert message.\nMultiple final actions can be provided in a sequence. They will be executed in the order they are specified after all validation steps have completed:\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(\n            \"Validation complete.\",  # a string message\n            send_alert,              # a callable function\n            generate_report          # another callable function\n        )\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\n\n\n\nThe get_validation_summary() function, which can be used to retrieve the summary of the validation results."
  },
  {
    "objectID": "reference/FinalActions.html#parameters",
    "href": "reference/FinalActions.html#parameters",
    "title": "FinalActions",
    "section": "",
    "text": "*actions : \n\nOne or more actions to execute after validation. An action can be (1) a callable function that will be executed with no arguments, or (2) a string message that will be printed to the console."
  },
  {
    "objectID": "reference/FinalActions.html#returns",
    "href": "reference/FinalActions.html#returns",
    "title": "FinalActions",
    "section": "",
    "text": ": FinalActions\n\nAn FinalActions object. This can be used when using the Validate class (to set final actions for the validation workflow)."
  },
  {
    "objectID": "reference/FinalActions.html#types-of-actions",
    "href": "reference/FinalActions.html#types-of-actions",
    "title": "FinalActions",
    "section": "",
    "text": "Final actions can be defined in two different ways:\n\nString: A message to be displayed when the validation is complete.\nCallable: A function that is called when the validation is complete.\n\nThe actions are executed at the end of the validation workflow. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of validation completion. Several strings and callables can be provided to the FinalActions class, and they will be executed in the order they are provided."
  },
  {
    "objectID": "reference/FinalActions.html#crafting-callables-with-get_validation_summary",
    "href": "reference/FinalActions.html#crafting-callables-with-get_validation_summary",
    "title": "FinalActions",
    "section": "",
    "text": "When creating a callable function to be used as a final action, you can use the get_validation_summary() function to retrieve the summary of the validation results. This summary contains information about the validation workflow, including the number of test units, the number of failing test units, and the threshold levels that were exceeded. You can use this information to craft your final action message or to take specific actions based on the validation results."
  },
  {
    "objectID": "reference/FinalActions.html#examples",
    "href": "reference/FinalActions.html#examples",
    "title": "FinalActions",
    "section": "",
    "text": "Final actions provide a powerful way to respond to the overall results of a validation workflow. They’re especially useful for sending notifications, generating reports, or taking corrective actions based on the complete validation outcome.\nThe following example shows how to create a final action that checks for critical failures and sends an alert:\nimport pointblank as pb\n\ndef send_alert():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        print(f\"ALERT: Critical validation failures found in {summary['table_name']}\")\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_alert)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nIn this example, the send_alert() function is defined to check the validation summary for critical failures. If any are found, an alert message is printed to the console. The function is passed to the FinalActions class, which ensures it will be executed after all validation steps are complete. Note that we used the get_validation_summary() function to retrieve the summary of the validation results to help craft the alert message.\nMultiple final actions can be provided in a sequence. They will be executed in the order they are specified after all validation steps have completed:\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(\n            \"Validation complete.\",  # a string message\n            send_alert,              # a callable function\n            generate_report          # another callable function\n        )\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)"
  },
  {
    "objectID": "reference/FinalActions.html#see-also",
    "href": "reference/FinalActions.html#see-also",
    "title": "FinalActions",
    "section": "",
    "text": "The get_validation_summary() function, which can be used to retrieve the summary of the validation results."
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "load_dataset(dataset='small_table', tbl_type='polars')\nLoad a dataset hosted in the library as specified table type.\nThe Pointblank library includes several datasets that can be loaded using the load_dataset() function. The datasets can be loaded as a Polars DataFrame, a Pandas DataFrame, or as a DuckDB table (which uses the Ibis library backend). These datasets are used throughout the documentation’s examples to demonstrate the functionality of the library. They’re also useful for experimenting with the library and trying out different validation scenarios."
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\ndataset : Literal['small_table', 'game_revenue', 'nycflights', 'global_sales'] = 'small_table'\n\nThe name of the dataset to load. Current options are \"small_table\", \"game_revenue\", \"nycflights\", and \"global_sales\".\n\ntbl_type : Literal['polars', 'pandas', 'duckdb'] = 'polars'\n\nThe type of table to generate from the dataset. The named options are \"polars\", \"pandas\", and \"duckdb\"."
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n : FrameT | Any\n\nThe dataset for the Validate object. This could be a Polars DataFrame, a Pandas DataFrame, or a DuckDB table as an Ibis table."
  },
  {
    "objectID": "reference/load_dataset.html#included-datasets",
    "href": "reference/load_dataset.html#included-datasets",
    "title": "load_dataset",
    "section": "Included Datasets",
    "text": "Included Datasets\nThere are three included datasets that can be loaded using the load_dataset() function:\n\n\"small_table\": A small dataset with 13 rows and 8 columns. This dataset is useful for testing and demonstration purposes.\n\"game_revenue\": A dataset with 2000 rows and 11 columns. Provides revenue data for a game development company. For the particular game, there are records of player sessions, the items they purchased, ads viewed, and the revenue generated.\n\"nycflights\": A dataset with 336,776 rows and 18 columns. This dataset provides information about flights departing from New York City airports (JFK, LGA, or EWR) in 2013.\n\"global_sales\": A dataset with 50,000 rows and 20 columns. Provides information about global sales of products across different regions and countries."
  },
  {
    "objectID": "reference/load_dataset.html#supported-dataframe-types",
    "href": "reference/load_dataset.html#supported-dataframe-types",
    "title": "load_dataset",
    "section": "Supported DataFrame Types",
    "text": "Supported DataFrame Types\nThe tbl_type= parameter can be set to one of the following:\n\n\"polars\": A Polars DataFrame.\n\"pandas\": A Pandas DataFrame.\n\"duckdb\": An Ibis table for a DuckDB database."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\nLoad the \"small_table\" dataset as a Polars DataFrame by calling load_dataset() with dataset=\"small_table\" and tbl_type=\"polars\":\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nNote that the \"small_table\" dataset is a Polars DataFrame and using the preview() function will display the table in an HTML viewing environment.\nThe \"game_revenue\" dataset can be loaded as a Pandas DataFrame by specifying the dataset name and setting tbl_type=\"pandas\":\n\ngame_revenue = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"pandas\")\n\npb.preview(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  player_idobject\n  session_idobject\n  session_startdatetime64[ns, UTC]\n  timedatetime64[ns, UTC]\n  item_typeobject\n  item_nameobject\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydatetime64[ns]\n  acquisitionobject\n  countryobject\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11 00:00:00\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11 00:00:00\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10 00:00:00\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10 00:00:00\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14 00:00:00\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nThe \"game_revenue\" dataset is a more real-world dataset with a mix of data types, and it’s significantly larger than the small_table dataset at 2000 rows and 11 columns.\nThe \"nycflights\" dataset can be loaded as a DuckDB table by specifying the dataset name and setting tbl_type=\"duckdb\":\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\npb.preview(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows336,776Columns18\n  \n\n  \n  yearint64\n  monthint64\n  dayint64\n  dep_timeint64\n  sched_dep_timeint64\n  dep_delayint64\n  arr_timeint64\n  sched_arr_timeint64\n  arr_delayint64\n  carrierstring\n  flightint64\n  tailnumstring\n  originstring\n  deststring\n  air_timeint64\n  distanceint64\n  hourint64\n  minuteint64\n\n\n\n  \n    1\n    2013\n    1\n    1\n    517\n    515\n    2\n    830\n    819\n    11\n    UA\n    1545\n    N14228\n    EWR\n    IAH\n    227\n    1400\n    5\n    15\n  \n  \n    2\n    2013\n    1\n    1\n    533\n    529\n    4\n    850\n    830\n    20\n    UA\n    1714\n    N24211\n    LGA\n    IAH\n    227\n    1416\n    5\n    29\n  \n  \n    3\n    2013\n    1\n    1\n    542\n    540\n    2\n    923\n    850\n    33\n    AA\n    1141\n    N619AA\n    JFK\n    MIA\n    160\n    1089\n    5\n    40\n  \n  \n    4\n    2013\n    1\n    1\n    544\n    545\n    -1\n    1004\n    1022\n    -18\n    B6\n    725\n    N804JB\n    JFK\n    BQN\n    183\n    1576\n    5\n    45\n  \n  \n    5\n    2013\n    1\n    1\n    554\n    600\n    -6\n    812\n    837\n    -25\n    DL\n    461\n    N668DN\n    LGA\n    ATL\n    116\n    762\n    6\n    0\n  \n  \n    336772\n    2013\n    9\n    30\n    NULL\n    1455\n    NULL\n    NULL\n    1634\n    NULL\n    9E\n    3393\n    NULL\n    JFK\n    DCA\n    NULL\n    213\n    14\n    55\n  \n  \n    336773\n    2013\n    9\n    30\n    NULL\n    2200\n    NULL\n    NULL\n    2312\n    NULL\n    9E\n    3525\n    NULL\n    LGA\n    SYR\n    NULL\n    198\n    22\n    0\n  \n  \n    336774\n    2013\n    9\n    30\n    NULL\n    1210\n    NULL\n    NULL\n    1330\n    NULL\n    MQ\n    3461\n    N535MQ\n    LGA\n    BNA\n    NULL\n    764\n    12\n    10\n  \n  \n    336775\n    2013\n    9\n    30\n    NULL\n    1159\n    NULL\n    NULL\n    1344\n    NULL\n    MQ\n    3572\n    N511MQ\n    LGA\n    CLE\n    NULL\n    419\n    11\n    59\n  \n  \n    336776\n    2013\n    9\n    30\n    NULL\n    840\n    NULL\n    NULL\n    1020\n    NULL\n    MQ\n    3531\n    N839MQ\n    LGA\n    RDU\n    NULL\n    431\n    8\n    40\n  \n\n\n\n\n\n\n        \n\n\nThe \"nycflights\" dataset is a large dataset with 336,776 rows and 18 columns. This dataset is truly a real-world dataset and provides information about flights originating from New York City airports in 2013.\nFinally, the \"global_sales\" dataset can be loaded as a Polars table by specifying the dataset name. Since tbl_type= is set to \"polars\" by default, we don’t need to specify it:\n\nglobal_sales = pb.load_dataset(dataset=\"global_sales\")\n\npb.preview(global_sales)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows50,000Columns20\n  \n\n  \n  product_idString\n  product_categoryString\n  customer_idString\n  customer_segmentString\n  regionString\n  countryString\n  cityString\n  timestampDatetime\n  quarterString\n  monthInt64\n  yearInt64\n  priceFloat64\n  quantityInt64\n  statusString\n  emailString\n  revenueFloat64\n  taxFloat64\n  totalFloat64\n  payment_methodString\n  sales_channelString\n\n\n\n  \n    1\n    98b70df0\n    Manufacturing\n    cf3b13c7\n    Government\n    Asia Pacific\n    Australia\n    Melbourne\n    2021-12-25 19:00:00\n    2021-Q4\n    12\n    2021\n    186.0\n    7\n    returned\n    user1651@test.org\n    1302.0\n    127.45\n    1429.45\n    Apple Pay\n    Partner\n  \n  \n    2\n    9d09fef5\n    Manufacturing\n    08b5db12\n    Consumer\n    Europe\n    France\n    Nice\n    2022-06-12 17:25:00\n    2022-Q2\n    6\n    2022\n    137.03\n    8\n    returned\n    user5200@company.io\n    1096.24\n    222.52\n    1318.76\n    PayPal\n    Distributor\n  \n  \n    3\n    8ac6b077\n    Retail\n    41079b2e\n    Consumer\n    Europe\n    France\n    Toulouse\n    2023-05-06 09:09:00\n    2023-Q2\n    5\n    2023\n    330.08\n    4\n    shipped\n    user9180@mockdata.com\n    1320.32\n    260.89\n    1581.21\n    PayPal\n    Phone\n  \n  \n    4\n    13d2df9d\n    Healthcare\n    b421eece\n    Consumer\n    North America\n    USA\n    Miami\n    2023-10-11 16:53:00\n    2023-Q4\n    10\n    2023\n    420.09\n    3\n    shipped\n    user1636@example.com\n    1260.27\n    103.99\n    1364.26\n    Bank Transfer\n    Phone\n  \n  \n    5\n    98b70df0\n    Manufacturing\n    5906a04f\n    SMB\n    North America\n    Canada\n    Calgary\n    2022-05-05 01:53:00\n    2022-Q2\n    5\n    2022\n    187.77\n    3\n    delivered\n    user9971@mockdata.com\n    563.31\n    75.73\n    639.04\n    Credit Card\n    Phone\n  \n  \n    49996\n    53a36468\n    Finance\n    966a8bbe\n    Government\n    Asia Pacific\n    Australia\n    Melbourne\n    2023-11-04 14:45:00\n    2023-Q4\n    11\n    2023\n    198.18\n    1\n    pending\n    user8593@test.org\n    198.18\n    18.3\n    216.48\n    Google Pay\n    Partner\n  \n  \n    49997\n    a42fd1ff\n    Healthcare\n    ff8933e4\n    SMB\n    Asia Pacific\n    Japan\n    Kyoto\n    2023-04-27 17:27:00\n    2023-Q2\n    4\n    2023\n    419.72\n    2\n    returned\n    user5448@company.io\n    839.44\n    90.49\n    929.93\n    Google Pay\n    Partner\n  \n  \n    49998\n    bbf158d2\n    Technology\n    f0c0af3f\n    Enterprise\n    North America\n    USA\n    Los Angeles\n    2021-04-24 23:15:00\n    2021-Q2\n    4\n    2021\n    302.52\n    1\n    pending\n    user1463@test.org\n    302.52\n    21.68\n    324.2\n    Bank Transfer\n    Online\n  \n  \n    49999\n    2a0866de\n    Healthcare\n    5b27ba59\n    SMB\n    Europe\n    France\n    Nice\n    2023-12-30 19:44:00\n    2023-Q4\n    12\n    2023\n    433.82\n    5\n    pending\n    user4167@test.org\n    2169.1\n    448.87\n    2617.97\n    Credit Card\n    Online\n  \n  \n    50000\n    6260f67c\n    Technology\n    482c1d84\n    Consumer\n    Asia Pacific\n    Japan\n    Kyoto\n    2021-12-05 09:49:00\n    2021-Q4\n    12\n    2021\n    400.31\n    8\n    returned\n    user4238@example.com\n    3202.48\n    339.84\n    3542.32\n    Apple Pay\n    Distributor\n  \n\n\n\n\n\n\n        \n\n\nThe \"global_sales\" dataset is a large dataset with 50,000 rows and 20 columns. Each record describes the sales of a particular product to a customer located in one of three global regions: North America, Europe, or Asia."
  },
  {
    "objectID": "reference/everything.html",
    "href": "reference/everything.html",
    "title": "everything",
    "section": "",
    "text": "everything()\nSelect all columns.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The everything() selector function can be used to select every column in the table. If you have a table with six columns and they’re all suitable for a specific type of validation, you can use columns=everything()) and all six columns will be selected for validation."
  },
  {
    "objectID": "reference/everything.html#returns",
    "href": "reference/everything.html#returns",
    "title": "everything",
    "section": "Returns",
    "text": "Returns\n\n : Everything\n\nAn Everything object, which can be used to select all columns."
  },
  {
    "objectID": "reference/everything.html#relevant-validation-methods-where-everything-can-be-used",
    "href": "reference/everything.html#relevant-validation-methods-where-everything-can-be-used",
    "title": "everything",
    "section": "Relevant Validation Methods where everything() can be Used",
    "text": "Relevant Validation Methods where everything() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe everything() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/everything.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/everything.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "everything",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe everything() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names except those having starting with “id_”, you can use the everything() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(everything() - starts_with(\"id_\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/everything.html#examples",
    "href": "reference/everything.html#examples",
    "title": "everything",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with several numeric columns and we’d like to validate that all these columns have less than 1000. We can use the everything() column selector function to select all columns for validation.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.everything(), value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2023_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2023_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps, one each column in the table. The values in every column were all lower than 1000.\nWe can also use the everything() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select every column except those that begin with \"2023\" we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.col(pb.everything() - pb.starts_with(\"2023\")), value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2024_hours and one for 2024_pay_total."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html",
    "href": "reference/Validate.col_vals_outside.html",
    "title": "Validate.col_vals_outside",
    "section": "",
    "text": "Validate.col_vals_outside(\n    columns,\n    left,\n    right,\n    inclusive=(True, True),\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo column data lie outside of two specified values or data in other columns?\nThe col_vals_between() validation method checks whether column values in a table do not fall within a certain range. The range is specified with three arguments: left=, right=, and inclusive=. The left= and right= values specify the lower and upper bounds. These bounds can be specified as literal values or as column names provided within col(). The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#parameters",
    "href": "reference/Validate.col_vals_outside.html#parameters",
    "title": "Validate.col_vals_outside",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nleft : float | int | Column\n\nThe lower bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\nright : float | int | Column\n\nThe upper bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\ninclusive : tuple[bool, bool] = (True, True)\n\nA tuple of two boolean values indicating whether the comparison should be inclusive. The position of the boolean values correspond to the left= and right= values, respectively. By default, both values are True.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#returns",
    "href": "reference/Validate.col_vals_outside.html#returns",
    "title": "Validate.col_vals_outside",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#what-can-be-used-in-left-and-right",
    "href": "reference/Validate.col_vals_outside.html#what-can-be-used-in-left-and-right",
    "title": "Validate.col_vals_outside",
    "section": "What Can Be Used in left= and right=?",
    "text": "What Can Be Used in left= and right=?\nThe left= and right= arguments both allow for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column in the target table\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value within left= and right=. There is flexibility in how you provide the date or datetime values for the bounds; they can be:\n\nstring-based dates or datetimes (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\ndate or datetime objects using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in either left= or right= (or both), it must be specified within col(). This facilitates column-to-column comparisons and, crucially, the columns being compared to either/both of the bounds must be of the same type as the column data (e.g., all numeric, all dates, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#preprocessing",
    "href": "reference/Validate.col_vals_outside.html#preprocessing",
    "title": "Validate.col_vals_outside",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and left=col(...)/right=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#segmentation",
    "href": "reference/Validate.col_vals_outside.html#segmentation",
    "title": "Validate.col_vals_outside",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#thresholds",
    "href": "reference/Validate.col_vals_outside.html#thresholds",
    "title": "Validate.col_vals_outside",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#examples",
    "href": "reference/Validate.col_vals_outside.html#examples",
    "title": "Validate.col_vals_outside",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 5, 5],\n        \"b\": [2, 3, 6, 4, 3, 6],\n        \"c\": [9, 8, 8, 9, 9, 7],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    2\n    9\n  \n  \n    2\n    6\n    3\n    8\n  \n  \n    3\n    5\n    6\n    8\n  \n  \n    4\n    7\n    4\n    9\n  \n  \n    5\n    5\n    3\n    9\n  \n  \n    6\n    5\n    6\n    7\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all outside the fixed boundary values of 1 and 4. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_outside(columns=\"a\", left=1, right=4)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_outside\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_outside()\n        \n        \n        \n    a\n    [1, 4]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_outside(). All test units passed, and there are no failing test units.\nAside from checking a column against two literal values representing the lower and upper bounds, we can also provide column names to the left= and/or right= arguments (by using the helper function col(). In this way, we can perform three additional comparison types:\n\nleft=column, right=column\nleft=literal, right=column\nleft=column, right=literal\n\nFor the next example, we’ll use col_vals_outside() to check whether the values in column b are outside of the range formed by the corresponding values in columns a (lower bound) and c (upper bound).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_outside(columns=\"b\", left=pb.col(\"a\"), right=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_outside\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_outside()\n        \n        \n        \n    b\n    [a, c]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 2: b is 6 and the bounds are 5 (a) and 8 (c).\nRow 5: b is 6 and the bounds are 5 (a) and 7 (c)."
  },
  {
    "objectID": "reference/Validate.col_exists.html",
    "href": "reference/Validate.col_exists.html",
    "title": "Validate.col_exists",
    "section": "",
    "text": "Validate.col_exists(\n    columns,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether one or more columns exist in the table.\nThe col_exists() method checks whether one or more columns exist in the target table. The only requirement is specification of the column names. Each validation step or expectation will operate over a single test unit, which is whether the column exists or not."
  },
  {
    "objectID": "reference/Validate.col_exists.html#parameters",
    "href": "reference/Validate.col_exists.html#parameters",
    "title": "Validate.col_exists",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_exists.html#returns",
    "href": "reference/Validate.col_exists.html#returns",
    "title": "Validate.col_exists",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_exists.html#thresholds",
    "href": "reference/Validate.col_exists.html#thresholds",
    "title": "Validate.col_exists",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_exists.html#examples",
    "href": "reference/Validate.col_exists.html#examples",
    "title": "Validate.col_exists",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with a string columns (a) and a numeric column (b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bInt64\n\n\n\n  \n    1\n    apple\n    1\n  \n  \n    2\n    banana\n    6\n  \n  \n    3\n    cherry\n    3\n  \n  \n    4\n    date\n    5\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the columns a and b actually exist in the table. We’ll determine if this validation had any failing test units (each validation will have a single test unit).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_exists(columns=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows two entries (one check per column) generated by the col_exists() validation step. Both steps passed since both columns provided in columns= are present in the table.\nNow, let’s check for the existence of a different set of columns.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_exists(columns=[\"b\", \"c\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports one passing validation step (the check for column b) and one failing validation step (the check for column c, which doesn’t exist)."
  },
  {
    "objectID": "reference/Validate.row_count_match.html",
    "href": "reference/Validate.row_count_match.html",
    "title": "Validate.row_count_match",
    "section": "",
    "text": "Validate.row_count_match(\n    count,\n    tol=0,\n    inverse=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether the row count of the table matches a specified count.\nThe row_count_match() method checks whether the row count of the target table matches a specified count. This validation will operate over a single test unit, which is whether the row count matches the specified count.\nWe also have the option to invert the validation step by setting inverse=True. This will make the expectation that the row count of the target table does not match the specified count."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#parameters",
    "href": "reference/Validate.row_count_match.html#parameters",
    "title": "Validate.row_count_match",
    "section": "Parameters",
    "text": "Parameters\n\ncount : int | FrameT | Any\n\nThe expected row count of the table. This can be an integer value, a Polars or Pandas DataFrame object, or an Ibis backend table. If a DataFrame/table is provided, the row count of that object will be used as the expected count.\n\ntol : Tolerance = 0\n\nThe tolerance allowable for the row count match. This can be specified as a single numeric value (integer or float) or as a tuple of two integers representing the lower and upper bounds of the tolerance range. If a single integer value (greater than 1) is provided, it represents the absolute bounds of the tolerance, ie. plus or minus the value. If a float value (between 0-1) is provided, it represents the relative tolerance, ie. plus or minus the relative percentage of the target. If a tuple is provided, it represents the lower and upper absolute bounds of the tolerance range. See the examples for more.\n\ninverse : bool = False\n\nShould the validation step be inverted? If True, then the expectation is that the row count of the target table should not match the specified count= value.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#returns",
    "href": "reference/Validate.row_count_match.html#returns",
    "title": "Validate.row_count_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#preprocessing",
    "href": "reference/Validate.row_count_match.html#preprocessing",
    "title": "Validate.row_count_match",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#thresholds",
    "href": "reference/Validate.row_count_match.html#thresholds",
    "title": "Validate.row_count_match",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#examples",
    "href": "reference/Validate.row_count_match.html#examples",
    "title": "Validate.row_count_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use the built in dataset \"small_table\". The table can be obtained by calling load_dataset(\"small_table\").\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(\"small_table\")\n\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the number of rows in the table matches a fixed value. In this case, we will use the value 13 as the expected row count.\n\nvalidation = (\n    pb.Validate(data=small_table)\n    .row_count_match(count=13)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the expectation value of 13 matches the actual count of rows in the target table. So, the single test unit passed.\nLet’s modify our example to show the different ways we can allow some tolerance to our validation by using the tol argument.\n\nsmaller_small_table = small_table.sample(n = 12) # within the lower bound\nvalidation = (\n    pb.Validate(data=smaller_small_table)\n    .row_count_match(count=13,tol=(2, 0)) # minus 2 but plus 0, ie. 11-13\n    .interrogate()\n)\n\nvalidation\n\nvalidation = (\n    pb.Validate(data=smaller_small_table)\n    .row_count_match(count=13,tol=.05) # .05% tolerance of 13\n    .interrogate()\n)\n\neven_smaller_table = small_table.sample(n = 2)\nvalidation = (\n    pb.Validate(data=even_smaller_table)\n    .row_count_match(count=13,tol=5) # plus or minus 5; this test will fail\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —"
  },
  {
    "objectID": "reference/contains.html",
    "href": "reference/contains.html",
    "title": "contains",
    "section": "",
    "text": "contains(text, case_sensitive=False)\nSelect columns that contain specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The contains() selector function can be used to select one or more columns that contain some specified text. So if the set of table columns consists of\n[profit, conv_first, conv_last, highest_conv, age]\nand you want to validate columns that have \"conv\" in the name, you can use columns=contains(\"conv\"). This will select the conv_first, conv_last, and highest_conv columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using contains() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/contains.html#parameters",
    "href": "reference/contains.html#parameters",
    "title": "contains",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should contain.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/contains.html#returns",
    "href": "reference/contains.html#returns",
    "title": "contains",
    "section": "Returns",
    "text": "Returns\n\n : Contains\n\nA Contains object, which can be used to select columns that contain the specified text."
  },
  {
    "objectID": "reference/contains.html#relevant-validation-methods-where-contains-can-be-used",
    "href": "reference/contains.html#relevant-validation-methods-where-contains-can-be-used",
    "title": "contains",
    "section": "Relevant Validation Methods where contains() can be Used",
    "text": "Relevant Validation Methods where contains() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe contains() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/contains.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/contains.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "contains",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe contains() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that have the text \"_n\" and start with \"item\", you can use the contains() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(contains(\"_n\") & starts_with(\"item\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/contains.html#examples",
    "href": "reference/contains.html#examples",
    "title": "contains",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, 2021_pay_total, 2022_pay_total, and person_id and we’d like to validate that the values in columns having \"pay\" in the name are greater than 10. We can use the contains() column selector function to specify the column names that contain \"pay\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2021_pay_total\": [16.32, 16.25, 15.75],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.contains(\"pay\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2021_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2022_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2021_pay_total and one for 2022_pay_total. The values in both columns were all greater than 10.\nWe can also use the contains() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that contain \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.contains(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2023_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2024_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay_total and one for 2024_pay_total."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html",
    "href": "reference/Validate.col_vals_eq.html",
    "title": "Validate.col_vals_eq",
    "section": "",
    "text": "Validate.col_vals_eq(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data equal to a fixed value or data in another column?\nThe col_vals_eq() validation method checks whether column values in a table are equal to a specified value= (the exact comparison used in this function is col_val == value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#parameters",
    "href": "reference/Validate.col_vals_eq.html#parameters",
    "title": "Validate.col_vals_eq",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#returns",
    "href": "reference/Validate.col_vals_eq.html#returns",
    "title": "Validate.col_vals_eq",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_eq.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_eq",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#preprocessing",
    "href": "reference/Validate.col_vals_eq.html#preprocessing",
    "title": "Validate.col_vals_eq",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#segmentation",
    "href": "reference/Validate.col_vals_eq.html#segmentation",
    "title": "Validate.col_vals_eq",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#thresholds",
    "href": "reference/Validate.col_vals_eq.html#thresholds",
    "title": "Validate.col_vals_eq",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#examples",
    "href": "reference/Validate.col_vals_eq.html#examples",
    "title": "Validate.col_vals_eq",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 5, 5, 5, 5, 5],\n        \"b\": [5, 5, 5, 6, 5, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    5\n    5\n  \n  \n    3\n    5\n    5\n  \n  \n    4\n    5\n    6\n  \n  \n    5\n    5\n    5\n  \n  \n    6\n    5\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all equal to the value of 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_eq(columns=\"a\", value=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_eq\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_eq()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_eq(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_eq() to check whether the values in column a are equal to the values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_eq(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_eq\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_eq()\n        \n        \n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 3: a is 5 and b is 6.\nRow 5: a is 5 and b is 4."
  },
  {
    "objectID": "reference/starts_with.html",
    "href": "reference/starts_with.html",
    "title": "starts_with",
    "section": "",
    "text": "starts_with(text, case_sensitive=False)\nSelect columns that start with specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The starts_with() selector function can be used to select one or more columns that start with some specified text. So if the set of table columns consists of\n[name_first, name_last, age, address]\nand you want to validate columns that start with \"name\", you can use columns=starts_with(\"name\"). This will select the name_first and name_last columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using starts_with() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/starts_with.html#parameters",
    "href": "reference/starts_with.html#parameters",
    "title": "starts_with",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should start with.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/starts_with.html#returns",
    "href": "reference/starts_with.html#returns",
    "title": "starts_with",
    "section": "Returns",
    "text": "Returns\n\n : StartsWith\n\nA StartsWith object, which can be used to select columns that start with the specified text."
  },
  {
    "objectID": "reference/starts_with.html#relevant-validation-methods-where-starts_with-can-be-used",
    "href": "reference/starts_with.html#relevant-validation-methods-where-starts_with-can-be-used",
    "title": "starts_with",
    "section": "Relevant Validation Methods where starts_with() can be Used",
    "text": "Relevant Validation Methods where starts_with() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe starts_with() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/starts_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/starts_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "starts_with",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe starts_with() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that start with \"a\" and end with \"e\", you can use the starts_with() and ends_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(starts_with(\"a\") & ends_with(\"e\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/starts_with.html#examples",
    "href": "reference/starts_with.html#examples",
    "title": "starts_with",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, paid_2021, paid_2022, and person_id and we’d like to validate that the values in columns that start with \"paid\" are greater than 10. We can use the starts_with() column selector function to specify the columns that start with \"paid\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [16.32, 16.25, 15.75],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.starts_with(\"paid\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for paid_2021 and one for paid_2022. The values in both columns were all greater than 10.\nWe can also use the starts_with() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that start with \"paid\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.starts_with(\"paid\") & pb.matches(\"23|24\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for paid_2023 and one for paid_2024."
  },
  {
    "objectID": "reference/Validate.error.html",
    "href": "reference/Validate.error.html",
    "title": "Validate.error",
    "section": "",
    "text": "Validate.error(i=None, scalar=False)\nGet the ‘error’ level status for each validation step.\nThe ‘error’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the ‘error’ level. Otherwise, the status is False.\nThe ascribed name of ‘error’ is semantic and does not imply that the validation process is halted, it is simply a status indicator that could be used to trigger some action to be taken. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the ‘error’ status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.error.html#parameters",
    "href": "reference/Validate.error.html#parameters",
    "title": "Validate.error",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the ‘error’ status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.error.html#returns",
    "href": "reference/Validate.error.html#returns",
    "title": "Validate.error",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the ‘error’ status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.error.html#examples",
    "href": "reference/Validate.error.html#examples",
    "title": "Validate.error",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have some failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the error() method is used to determine the ‘error’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [3, 4, 9, 7, 2, 3, 8],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.error()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘error’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘error’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘error’ level.\nWe can also visually inspect the ‘error’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:15:15PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    30.43\n    40.57\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:15:15 UTC&lt; 1 s2025-05-23 02:15:15 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there are filled gray and yellow circles in the first step (far right side, in the W and E columns) indicating that the ‘warning’ and ‘error’ thresholds were met. The other steps have empty gray and yellow circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘error’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.error(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had the ‘error’ threshold met."
  },
  {
    "objectID": "reference/Validate.warning.html",
    "href": "reference/Validate.warning.html",
    "title": "Validate.warning",
    "section": "",
    "text": "Validate.warning(i=None, scalar=False)\nGet the ‘warning’ level status for each validation step.\nThe ‘warning’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the ‘warning’ level. Otherwise, the status is False.\nThe ascribed name of ‘warning’ is semantic and does not imply that a warning message is generated, it is simply a status indicator that could be used to trigger some action to be taken. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the ‘warning’ status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.warning.html#parameters",
    "href": "reference/Validate.warning.html#parameters",
    "title": "Validate.warning",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the ‘warning’ status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.warning.html#returns",
    "href": "reference/Validate.warning.html#returns",
    "title": "Validate.warning",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the ‘warning’ status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.warning.html#examples",
    "href": "reference/Validate.warning.html#examples",
    "title": "Validate.warning",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have some failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the warning() method is used to determine the ‘warning’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.warning()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘warning’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘warning’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘warning’ level.\nWe can also visually inspect the ‘warning’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:15:22PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    50.71\n    20.29\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:15:22 UTC&lt; 1 s2025-05-23 02:15:22 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there’s a filled gray circle in the first step (look to the far right side, in the W column) indicating that the ‘warning’ threshold was met. The other steps have empty gray circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘warning’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.warning(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had met the ‘warning’ threshold."
  },
  {
    "objectID": "reference/missing_vals_tbl.html",
    "href": "reference/missing_vals_tbl.html",
    "title": "missing_vals_tbl",
    "section": "",
    "text": "missing_vals_tbl(data)\nDisplay a table that shows the missing values in the input table.\nThe missing_vals_tbl() function generates a table that shows the missing values in the input table. The table is displayed using the Great Tables API, which allows for further customization of the table’s appearance if so desired."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#parameters",
    "href": "reference/missing_vals_tbl.html#parameters",
    "title": "missing_vals_tbl",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to display the missing values. This could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#returns",
    "href": "reference/missing_vals_tbl.html#returns",
    "title": "missing_vals_tbl",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the table of missing values in the input table."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#supported-input-table-types",
    "href": "reference/missing_vals_tbl.html#supported-input-table-types",
    "title": "missing_vals_tbl",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nMicrosoft SQL Server table (\"mssql\")*\nSnowflake table (\"snowflake\")*\nDatabricks table (\"databricks\")*\nPySpark table (\"pyspark\")*\nBigQuery table (\"bigquery\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using missing_vals_tbl() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#the-missing-values-table",
    "href": "reference/missing_vals_tbl.html#the-missing-values-table",
    "title": "missing_vals_tbl",
    "section": "The Missing Values Table",
    "text": "The Missing Values Table\nThe missing values table shows the proportion of missing values in each column of the input table. The table is divided into sectors, with each sector representing a range of rows in the table. The proportion of missing values in each sector is calculated for each column. The table is displayed using the Great Tables API, which allows for further customization of the table’s appearance.\nTo ensure that the table can scale to tables with many columns, each row in the reporting table represents a column in the input table. There are 10 sectors shown in the table, where the first sector represents the first 10% of the rows, the second sector represents the next 10% of the rows, and so on. Any sectors that are light blue indicate that there are no missing values in that sector. If there are missing values, the proportion of missing values is shown by a gray color (light gray for low proportions, dark gray to black for very high proportions)."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#examples",
    "href": "reference/missing_vals_tbl.html#examples",
    "title": "missing_vals_tbl",
    "section": "Examples",
    "text": "Examples\nThe missing_vals_tbl() function is useful for quickly identifying columns with missing values in a table. Here’s an example using the nycflights dataset (loaded as a Polars DataFrame using the load_dataset() function):\n\nimport pointblank as pb\n\nnycflights = pb.load_dataset(\"nycflights\", tbl_type=\"polars\")\n\npb.missing_vals_tbl(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values   46,595 in total\n  \n  \n    PolarsRows336,776Columns18\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    year\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    month\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    day\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    carrier\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    flight\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    tailnum\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    origin\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dest\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    air_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    distance\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    hour\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    minute\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 3367733678 – 6735467355 – 101031101032 – 134708134709 – 168385168386 – 202062202063 – 235739235740 – 269416269417 – 303093303094 – 336776\n  \n\n\n\n\n\n\n        \n\n\nThe table shows the proportion of missing values in each column of the nycflights dataset. The table is divided into sectors, with each sector representing a range of rows in the table (with around 34,000 rows per sector). The proportion of missing values in each sector is calculated for each column. The various shades of gray indicate the proportion of missing values in each sector. Many columns have no missing values at all, and those sectors are colored light blue."
  },
  {
    "objectID": "reference/Validate.above_threshold.html",
    "href": "reference/Validate.above_threshold.html",
    "title": "Validate.above_threshold",
    "section": "",
    "text": "Validate.above_threshold(level='warning', i=None)\nCheck if any validation steps exceed a specified threshold level.\nThe above_threshold() method checks whether validation steps exceed a given threshold level. This provides a non-exception-based alternative to assert_below_threshold() for conditional workflow control based on validation results.\nThis method is useful in scenarios where you want to check if any validation steps failed beyond a certain threshold without raising an exception, allowing for more flexible programmatic responses to validation issues."
  },
  {
    "objectID": "reference/Validate.above_threshold.html#parameters",
    "href": "reference/Validate.above_threshold.html#parameters",
    "title": "Validate.above_threshold",
    "section": "Parameters",
    "text": "Parameters\n\nlevel : str = 'warning'\n\nThe threshold level to check against. Valid options are: \"warning\" (the least severe threshold level), \"error\" (the middle severity threshold level), and \"critical\" (the most severe threshold level). The default is \"warning\".\n\ni : int | None = None\n\nSpecific validation step number(s) to check. If a single integer, checks only that step. If a list of integers, checks all specified steps. If None (the default), checks all validation steps. Step numbers are 1-based (first step is 1, not 0)."
  },
  {
    "objectID": "reference/Validate.above_threshold.html#returns",
    "href": "reference/Validate.above_threshold.html#returns",
    "title": "Validate.above_threshold",
    "section": "Returns",
    "text": "Returns\n\n : bool\n\nTrue if any of the specified validation steps exceed the given threshold level, False otherwise."
  },
  {
    "objectID": "reference/Validate.above_threshold.html#raises",
    "href": "reference/Validate.above_threshold.html#raises",
    "title": "Validate.above_threshold",
    "section": "Raises",
    "text": "Raises\n\n: ValueError\n\nIf an invalid threshold level is provided."
  },
  {
    "objectID": "reference/Validate.above_threshold.html#examples",
    "href": "reference/Validate.above_threshold.html#examples",
    "title": "Validate.above_threshold",
    "section": "Examples",
    "text": "Examples\nBelow are some examples of how to use the above_threshold() method. First, we’ll create a simple Polars DataFrame with a single column (values).\n\nimport polars as pl\n\ntbl = pl.DataFrame({\n    \"values\": [1, 2, 3, 4, 5, 0, -1]\n})\n\nThen a validation plan will be created with thresholds (warning=0.1, error=0.2, critical=0.3). After interrogating, we display the validation report table:\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(0.1, 0.2, 0.3))\n    .col_vals_gt(columns=\"values\", value=0)\n    .col_vals_lt(columns=\"values\", value=10)\n    .col_vals_between(columns=\"values\", left=0, right=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    values\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    50.71\n    20.29\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    values\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #AAAAAA\n    3\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    values\n    [0, 5]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    60.86\n    10.14\n    ●\n    ○\n    ○\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nLet’s check if any steps exceed the ‘warning’ threshold with the above_threshold() method. A message will be printed if that’s the case:\n\nif validation.above_threshold(level=\"warning\"):\n    print(\"Some steps have exceeded the warning threshold\")\n\nSome steps have exceeded the warning threshold\n\n\nCheck if only steps 2 and 3 exceed the ‘error’ threshold through use of the i= argument:\n\nif validation.above_threshold(level=\"error\", i=[2, 3]):\n    print(\"Steps 2 and/or 3 have exceeded the error threshold\")\n\nYou can use this in a workflow to conditionally trigger processes. Here’s a snippet of how you might use this in a function:\ndef process_data(validation_obj):\n    # Only continue processing if validation passes critical thresholds\n    if not validation_obj.above_threshold(level=\"critical\"):\n        # Continue with processing\n        print(\"Data meets critical quality thresholds, proceeding...\")\n        return True\n    else:\n        # Log failure and stop processing\n        print(\"Data fails critical quality checks, aborting...\")\n        return False\nNote that this is just a suggestion for how to implement conditional workflow processes. You should adapt this pattern to your specific requirements, which might include different threshold levels, custom logging mechanisms, or integration with your organization’s data pipelines and notification systems."
  },
  {
    "objectID": "reference/Validate.above_threshold.html#see-also",
    "href": "reference/Validate.above_threshold.html#see-also",
    "title": "Validate.above_threshold",
    "section": "See Also",
    "text": "See Also\n\nassert_below_threshold(): a similar method that raises an exception if thresholds are exceeded\nwarning(): get the ‘warning’ status for each validation step\nerror(): get the ‘error’ status for each validation step\ncritical(): get the ‘critical’ status for each validation step"
  },
  {
    "objectID": "reference/Schema.html",
    "href": "reference/Schema.html",
    "title": "Schema",
    "section": "",
    "text": "Schema(self, columns=None, tbl=None, **kwargs)\nDefinition of a schema object.\nThe schema object defines the structure of a table. Once it is defined, the object can be used in a validation workflow, using Validate and its methods, to ensure that the structure of a table matches the expected schema. The validation method that works with the schema object is called col_schema_match().\nA schema for a table can be constructed with the Schema class in a number of ways:\nThe schema object can also be constructed by providing a DataFrame or Ibis table object (using the tbl= parameter) and the schema will be collected from either type of object. The schema object can be printed to display the column names and dtypes. Note that if tbl= is provided then there shouldn’t be any other inputs provided through either columns= or **kwargs."
  },
  {
    "objectID": "reference/Schema.html#parameters",
    "href": "reference/Schema.html#parameters",
    "title": "Schema",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | list[tuple[str, str]] | list[tuple[str]] | dict[str, str] | None = None\n\nA list of strings (representing column names), a list of tuples (for column names and column dtypes), or a dictionary containing column and dtype information. If any of these inputs are provided here, it will take precedence over any column arguments provided via **kwargs.\n\ntbl : any | None = None\n\nA DataFrame (Polars or Pandas) or an Ibis table object from which the schema will be collected. Read the Supported Input Table Types section for details on the supported table types.\n\n****kwargs** :  = {}\n\nIndividual column arguments that are in the form of column=dtype or column=[dtype1, dtype2, ...]. These will be ignored if the columns= parameter is not None."
  },
  {
    "objectID": "reference/Schema.html#returns",
    "href": "reference/Schema.html#returns",
    "title": "Schema",
    "section": "Returns",
    "text": "Returns\n\n : Schema\n\nA schema object."
  },
  {
    "objectID": "reference/Schema.html#supported-input-table-types",
    "href": "reference/Schema.html#supported-input-table-types",
    "title": "Schema",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe tbl= parameter, if used, can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using Schema(tbl=) with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/Schema.html#additional-notes-on-schema-construction",
    "href": "reference/Schema.html#additional-notes-on-schema-construction",
    "title": "Schema",
    "section": "Additional Notes on Schema Construction",
    "text": "Additional Notes on Schema Construction\nWhile there is flexibility in how a schema can be constructed, there is the potential for some confusion. So let’s go through each of the methods of constructing a schema in more detail and single out some important points.\nWhen providing a list of column names to columns=, a col_schema_match() validation step will only check the column names. Any arguments pertaining to dtypes will be ignored.\nWhen using a list of tuples in columns=, the tuples could contain the column name and dtype or just the column name. This construction allows for more flexibility in constructing the schema as some columns will be checked for dtypes and others will not. This method is the only way to have mixed checks of column names and dtypes in col_schema_match().\nWhen providing a dictionary to columns=, the keys are the column names and the values are the dtypes. This method of input is useful in those cases where you might already have a dictionary of column names and dtypes that you want to use as the schema.\nIf using individual column arguments in the form of keyword arguments, the column names are the keyword arguments and the dtypes are the values. This method emphasizes readability and is perhaps more convenient when manually constructing a schema with a small number of columns.\nFinally, multiple dtypes can be provided for a single column by providing a list or tuple of dtypes in place of a scalar string value. Having multiple dtypes for a column allows for the dtype check via col_schema_match() to make multiple attempts at matching the column dtype. Should any of the dtypes match the column dtype, that part of the schema check will pass. Here are some examples of how you could provide single and multiple dtypes for a column:\n# list of tuples\nschema_1 = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", [\"Float64\", \"Int64\"])])\n\n# dictionary\nschema_2 = pb.Schema(columns={\"name\": \"String\", \"age\": [\"Float64\", \"Int64\"]})\n\n# keyword arguments\nschema_3 = pb.Schema(name=\"String\", age=[\"Float64\", \"Int64\"])\nAll of the above examples will construct the same schema object."
  },
  {
    "objectID": "reference/Schema.html#examples",
    "href": "reference/Schema.html#examples",
    "title": "Schema",
    "section": "Examples",
    "text": "Examples\nA schema can be constructed via the Schema class in multiple ways. Let’s use the following Polars DataFrame as a basis for constructing a schema:\n\nimport pointblank as pb\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"height\": [5.6, 6.0, 5.8]\n})\n\nYou could provide Schema(columns=) a list of tuples containing column names and data types:\n\nschema = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", \"Int64\"), (\"height\", \"Float64\")])\n\nAlternatively, a dictionary containing column names and dtypes also works:\n\nschema = pb.Schema(columns={\"name\": \"String\", \"age\": \"Int64\", \"height\": \"Float64\"})\n\nAnother input method involves using individual column arguments in the form of keyword arguments:\n\nschema = pb.Schema(name=\"String\", age=\"Int64\", height=\"Float64\")\n\nFinally, could also provide a DataFrame (Polars and Pandas) or an Ibis table object to tbl= and the schema will be collected:\nschema = pb.Schema(tbl=df)\nWhichever method you choose, you can verify the schema inputs by printing the schema object:\n\nprint(schema)\n\nPointblank Schema\n  name: String\n  age: Int64\n  height: Float64\n\n\nThe Schema object can be used to validate the structure of a table against the schema. The relevant Validate method for this is col_schema_match(). In a validation workflow, you’ll have a target table (defined at the beginning of the workflow) and you might want to ensure that your expectations of the table structure are met. The col_schema_match() method works with a Schema object to validate the structure of the table. Here’s an example of how you could use col_schema_match() in a validation workflow:\n\n# Define the schema\nschema = pb.Schema(name=\"String\", age=\"Int64\", height=\"Float64\")\n\n# Define a validation that checks the schema against the table (`df`)\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\n# Display the validation results\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe col_schema_match() validation method will validate the structure of the table against the schema during interrogation. If the structure of the table does not match the schema, the single test unit will fail. In this case, the defined schema matched the structure of the table, so the validation passed.\nWe can also choose to check only the column names of the target table. This can be done by providing a simplified Schema object, which is given a list of column names:\n\nschema = pb.Schema(columns=[\"name\", \"age\", \"height\"])\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the schema only checks the column names of the table against the schema during interrogation. If the column names of the table do not match the schema, the single test unit will fail. In this case, the defined schema matched the column names of the table, so the validation passed.\nIf you wanted to check column names and dtypes only for a subset of columns (and just the column names for the rest), you could use a list of mixed one- or two-item tuples in columns=:\n\nschema = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", ), (\"height\", )])\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nNot specifying a dtype for a column (as is the case for the age and height columns in the above example) will only check the column name.\nThere may also be the case where you want to check the column names and specify multiple dtypes for a column to have several attempts at matching the dtype. This can be done by providing a list of dtypes where there would normally be a single dtype:\n\nschema = pb.Schema(\n  columns=[(\"name\", \"String\"), (\"age\", [\"Float64\", \"Int64\"]), (\"height\", \"Float64\")]\n)\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFor the age column, the schema will check for both Float64 and Int64 dtypes. If either of these dtypes is found in the column, the portion of the schema check will succeed."
  },
  {
    "objectID": "reference/Schema.html#see-also",
    "href": "reference/Schema.html#see-also",
    "title": "Schema",
    "section": "See Also",
    "text": "See Also\nThe col_schema_match() validation method, where a Schema object is used in a validation workflow."
  },
  {
    "objectID": "reference/col.html",
    "href": "reference/col.html",
    "title": "col",
    "section": "",
    "text": "col(exprs)\nHelper function for referencing a column in the input table.\nMany of the validation methods (i.e., col_vals_*() methods) in Pointblank have a value= argument. These validations are comparisons between column values and a literal value, or, between column values and adjacent values in another column. The col() helper function is used to specify that it is a column being referenced, not a literal value.\nThe col() doesn’t check that the column exists in the input table. It acts to signal that the value being compared is a column value. During validation (i.e., when interrogate() is called), Pointblank will then check that the column exists in the input table.\nFor creating expressions to use with the conjointly() validation method, use the expr_col() function instead."
  },
  {
    "objectID": "reference/col.html#parameters",
    "href": "reference/col.html#parameters",
    "title": "col",
    "section": "Parameters",
    "text": "Parameters\n\nexprs : str | ColumnSelector | ColumnSelectorNarwhals\n\nEither the name of a single column in the target table, provided as a string, or, an expression involving column selector functions (e.g., starts_with(\"a\"), ends_with(\"e\") \\| starts_with(\"a\"), etc.)."
  },
  {
    "objectID": "reference/col.html#returns",
    "href": "reference/col.html#returns",
    "title": "col",
    "section": "Returns",
    "text": "Returns\n\n : Column | ColumnLiteral | ColumnSelectorNarwhals:\n\nA column object or expression representing the column reference."
  },
  {
    "objectID": "reference/col.html#usage-with-the-columns-argument",
    "href": "reference/col.html#usage-with-the-columns-argument",
    "title": "col",
    "section": "Usage with the columns= Argument",
    "text": "Usage with the columns= Argument\nThe col() function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nIf specifying a single column with certainty (you have the exact name), col() is not necessary since you can just pass the column name as a string (though it is still valid to use col(\"column_name\"), if preferred). However, if you want to select columns based on complex logic involving multiple column selector functions (e.g., columns that start with \"a\" but don’t end with \"e\"), you need to use col() to wrap expressions involving column selector functions and logical operators such as &, |, -, and ~.\nHere is an example of such usage with the col_vals_gt() validation method:\ncol_vals_gt(columns=col(starts_with(\"a\") & ~ends_with(\"e\")), value=10)\nIf using only a single column selector function, you can pass the function directly to the columns= argument of the validation method, or, you can use col() to wrap the function (either is valid though the first is more concise). Here is an example of that simpler usage:\ncol_vals_gt(columns=starts_with(\"a\"), value=10)"
  },
  {
    "objectID": "reference/col.html#usage-with-the-value-left-and-right-arguments",
    "href": "reference/col.html#usage-with-the-value-left-and-right-arguments",
    "title": "col",
    "section": "Usage with the value=, left=, and right= Arguments",
    "text": "Usage with the value=, left=, and right= Arguments\nThe col() function can be used in the value= argument of the following validation methods\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\n\nand in the left= and right= arguments (either or both) of these two validation methods\n\ncol_vals_between()\ncol_vals_outside()\n\nYou cannot use column selector functions such as starts_with() in either of the value=, left=, or right= arguments since there would be no guarantee that a single column will be resolved from the target table with this approach. The col() function is used to signal that the value being compared is a column value and not a literal value."
  },
  {
    "objectID": "reference/col.html#available-selectors",
    "href": "reference/col.html#available-selectors",
    "title": "col",
    "section": "Available Selectors",
    "text": "Available Selectors\nThere is a collection of selectors available in pointblank, allowing you to select columns based on attributes of column names and positions. The selectors are:\n\nstarts_with()\nends_with()\ncontains()\nmatches()\neverything()\nfirst_n()\nlast_n()\n\nAlternatively, we support selectors from the Narwhals library! Those selectors can additionally take advantage of the data types of the columns. The selectors are:\n\nboolean()\nby_dtype()\ncategorical()\nmatches()\nnumeric()\nstring()\n\nHave a look at the Narwhals API documentation on selectors for more information."
  },
  {
    "objectID": "reference/col.html#examples",
    "href": "reference/col.html#examples",
    "title": "col",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns a and b and we’d like to validate that the values in column a are greater than the values in column b. We can use the col() helper function to reference the comparison column when creating the validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 6, 5],\n        \"b\": [4, 2, 3, 3, 4, 3],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom results of the validation table it can be seen that values in a were greater than values in b for every row (or test unit). Using value=pb.col(\"b\") specified that the greater-than comparison is across columns, not with a fixed literal value.\nIf you want to select an arbitrary set of columns upon which to base a validation, you can use column selector functions (e.g., starts_with(), ends_with(), etc.) to specify columns in the columns= argument of a validation method. Let’s use the starts_with() column selector function to select columns that start with \"paid\" and validate that the values in those columns are greater than 10.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [16.32, 16.25, 15.75],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.starts_with(\"paid\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the starts_with() column selector function. This is not strictly necessary when using a single column selector function, so columns=pb.starts_with(\"paid\") would be equivalent usage here. However, the use of col() is required when using multiple column selector functions with logical operators. Here is an example of that more complex usage:\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.starts_with(\"paid\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the starts_with() and matches() column selector functions, combined with the & operator. This is necessary to specify the set of columns that start with \"paid\" and match the text \"2023\" or \"2024\".\nIf you’d like to take advantage of Narwhals selectors, that’s also possible. Here is an example of using the numeric() column selector function to select all numeric columns for validation, checking that their values are greater than 0.\n\nimport narwhals.selectors as ncs\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=pb.col(ncs.numeric()), value=0)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    hours_2022\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    hours_2023\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    hours_2024\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    paid_2022\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    paid_2023\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    paid_2024\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the numeric() column selector function from Narwhals. As with the other selectors, this is not strictly necessary when using a single column selector, so columns=ncs.numeric() would also be fine here.\nNarwhals selectors can also use operators to combine multiple selectors. Here is an example of using the numeric() and matches() selectors together to select all numeric columns that fit a specific pattern.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_status\": [\"ft\", \"ft\", \"pt\"],\n        \"2023_status\": [\"ft\", \"pt\", \"ft\"],\n        \"2024_status\": [\"ft\", \"pt\", \"ft\"],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.col(ncs.numeric() & ncs.matches(\"2023|2024\")), value=30)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2023_pay_total\n    30\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_pay_total\n    30\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the numeric() and matches() column selector functions from Narwhals, combined with the & operator. This is necessary to specify the set of columns that are numeric and match the text \"2023\" or \"2024\"."
  },
  {
    "objectID": "reference/col.html#see-also",
    "href": "reference/col.html#see-also",
    "title": "col",
    "section": "See Also",
    "text": "See Also\nCreate a column expression for use in conjointly() validation with the expr_col() function."
  },
  {
    "objectID": "reference/Thresholds.html",
    "href": "reference/Thresholds.html",
    "title": "Thresholds",
    "section": "",
    "text": "Thresholds(self, warning=None, error=None, critical=None)\nDefinition of threshold values.\nThresholds are used to set limits on the number of failing test units at different levels. The levels are ‘warning’, ‘error’, and ‘critical’. These levels correspond to different levels of severity when a threshold is reached. The threshold values can be set as absolute counts or as fractions of the total number of test units. When a threshold is reached, an action can be taken (e.g., displaying a message or calling a function) if there is an associated action defined for that level (defined through the Actions class)."
  },
  {
    "objectID": "reference/Thresholds.html#parameters",
    "href": "reference/Thresholds.html#parameters",
    "title": "Thresholds",
    "section": "Parameters",
    "text": "Parameters\n\nwarning : int | float | bool | None = None\n\nThe threshold for the ‘warning’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1.\n\nerror : int | float | bool | None = None\n\nThe threshold for the ‘error’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1.\n\ncritical : int | float | bool | None = None\n\nThe threshold for the ‘critical’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1."
  },
  {
    "objectID": "reference/Thresholds.html#returns",
    "href": "reference/Thresholds.html#returns",
    "title": "Thresholds",
    "section": "Returns",
    "text": "Returns\n\n : Thresholds\n\nA Thresholds object. This can be used when using the Validate class (to set thresholds globally) or when defining validation steps like col_vals_gt() (so that threshold values are scoped to individual validation steps, overriding any global thresholds)."
  },
  {
    "objectID": "reference/Thresholds.html#examples",
    "href": "reference/Thresholds.html#examples",
    "title": "Thresholds",
    "section": "Examples",
    "text": "Examples\nIn a data validation workflow, you can set thresholds for the number of failing test units at different levels. For example, you can set a threshold for the ‘warning’ level when the number of failing test units exceeds 10% of the total number of test units:\n\nthresholds_1 = pb.Thresholds(warning=0.1)\n\nYou can also set thresholds for the ‘error’ and ‘critical’ levels:\n\nthresholds_2 = pb.Thresholds(warning=0.1, error=0.2, critical=0.05)\n\nThresholds can also be set as absolute counts. Here’s an example where the ‘warning’ level is set to 5 failing test units:\n\nthresholds_3 = pb.Thresholds(warning=5)\n\nThe thresholds object can be used to set global thresholds for all validation steps. Or, you can set thresholds for individual validation steps, which will override the global thresholds. Here’s a data validation workflow example where we set global thresholds and then override with different thresholds at the col_vals_gt() step:\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        label=\"Example Validation\",\n        thresholds=pb.Thresholds(warning=0.1, error=0.2, critical=0.3)\n    )\n    .col_vals_not_null(columns=[\"c\", \"d\"])\n    .col_vals_gt(columns=\"a\", value=3, thresholds=pb.Thresholds(warning=5))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolarsWARNING0.1ERROR0.2CRITICAL0.3\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    d\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #AAAAAA\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    ●\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nAs can be seen, the last step (col_vals_gt()) has its own thresholds, which override the global thresholds set at the beginning of the validation workflow (in the Validate class)."
  },
  {
    "objectID": "reference/get_validation_summary.html",
    "href": "reference/get_validation_summary.html",
    "title": "get_validation_summary",
    "section": "",
    "text": "get_validation_summary()\nAccess validation summary information when authoring final actions.\nThis function provides a convenient way to access summary information about the validation process within a final action. It returns a dictionary with key metrics from the validation process. This function can only be used within callables crafted for the FinalActions class.\n\n\n\n : dict | None\n\nA dictionary containing validation metrics. If called outside of an final action context, this function will return None.\n\n\n\n\n\nThe summary dictionary contains the following fields:\n\nn_steps (int): The total number of validation steps.\nn_passing_steps (int): The number of validation steps where all test units passed.\nn_failing_steps (int): The number of validation steps that had some failing test units.\nn_warning_steps (int): The number of steps that exceeded a ‘warning’ threshold.\nn_error_steps (int): The number of steps that exceeded an ‘error’ threshold.\nn_critical_steps (int): The number of steps that exceeded a ‘critical’ threshold.\nlist_passing_steps (list[int]): List of step numbers where all test units passed.\nlist_failing_steps (list[int]): List of step numbers for steps having failing test units.\ndict_n (dict): The number of test units for each validation step.\ndict_n_passed (dict): The number of test units that passed for each validation step.\ndict_n_failed (dict): The number of test units that failed for each validation step.\ndict_f_passed (dict): The fraction of test units that passed for each validation step.\ndict_f_failed (dict): The fraction of test units that failed for each validation step.\ndict_warning (dict): The ‘warning’ level status for each validation step.\ndict_error (dict): The ‘error’ level status for each validation step.\ndict_critical (dict): The ‘critical’ level status for each validation step.\nall_passed (bool): Whether or not every validation step had no failing test units.\nhighest_severity (str): The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\ntbl_row_count (int): The number of rows in the target table.\ntbl_column_count (int): The number of columns in the target table.\ntbl_name (str): The name of the target table.\nvalidation_duration (float): The duration of the validation in seconds.\n\nNote that the summary dictionary is only available within the context of a final action. If called outside of a final action (i.e., when no final action is being executed), this function will return None.\n\n\n\nFinal actions are executed after the completion of all validation steps. They provide an opportunity to take appropriate actions based on the overall validation results. Here’s an example of a final action function (send_report()) that sends an alert when critical validation failures are detected:\nimport pointblank as pb\n\ndef send_report():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        # Send an alert email\n        send_alert_email(\n            subject=f\"CRITICAL validation failures in {summary['tbl_name']}\",\n            body=f\"{summary['n_critical_steps']} steps failed with critical severity.\"\n        )\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_report)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nNote that send_alert_email() in the example above is a placeholder function that would be implemented by the user to send email alerts. This function is not provided by the Pointblank package.\nThe get_validation_summary() function can also be used to create custom reporting for validation results:\ndef log_validation_results():\n    summary = pb.get_validation_summary()\n\n    print(f\"Validation completed with status: {summary['highest_severity'].upper()}\")\n    print(f\"Steps: {summary['n_steps']} total\")\n    print(f\"  - {summary['n_passing_steps']} passing, {summary['n_failing_steps']} failing\")\n    print(\n        f\"  - Severity: {summary['n_warning_steps']} warnings, \"\n        f\"{summary['n_error_steps']} errors, \"\n        f\"{summary['n_critical_steps']} critical\"\n    )\n\n    if summary['highest_severity'] in [\"error\", \"critical\"]:\n        print(\"⚠️ Action required: Please review failing validation steps!\")\nFinal actions work well with both simple logging and more complex notification systems, allowing you to integrate validation results into your broader data quality workflows.\n\n\n\nHave a look at FinalActions for more information on how to create custom actions that are executed after all validation steps have been completed."
  },
  {
    "objectID": "reference/get_validation_summary.html#returns",
    "href": "reference/get_validation_summary.html#returns",
    "title": "get_validation_summary",
    "section": "",
    "text": ": dict | None\n\nA dictionary containing validation metrics. If called outside of an final action context, this function will return None."
  },
  {
    "objectID": "reference/get_validation_summary.html#description-of-the-summary-fields",
    "href": "reference/get_validation_summary.html#description-of-the-summary-fields",
    "title": "get_validation_summary",
    "section": "",
    "text": "The summary dictionary contains the following fields:\n\nn_steps (int): The total number of validation steps.\nn_passing_steps (int): The number of validation steps where all test units passed.\nn_failing_steps (int): The number of validation steps that had some failing test units.\nn_warning_steps (int): The number of steps that exceeded a ‘warning’ threshold.\nn_error_steps (int): The number of steps that exceeded an ‘error’ threshold.\nn_critical_steps (int): The number of steps that exceeded a ‘critical’ threshold.\nlist_passing_steps (list[int]): List of step numbers where all test units passed.\nlist_failing_steps (list[int]): List of step numbers for steps having failing test units.\ndict_n (dict): The number of test units for each validation step.\ndict_n_passed (dict): The number of test units that passed for each validation step.\ndict_n_failed (dict): The number of test units that failed for each validation step.\ndict_f_passed (dict): The fraction of test units that passed for each validation step.\ndict_f_failed (dict): The fraction of test units that failed for each validation step.\ndict_warning (dict): The ‘warning’ level status for each validation step.\ndict_error (dict): The ‘error’ level status for each validation step.\ndict_critical (dict): The ‘critical’ level status for each validation step.\nall_passed (bool): Whether or not every validation step had no failing test units.\nhighest_severity (str): The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\ntbl_row_count (int): The number of rows in the target table.\ntbl_column_count (int): The number of columns in the target table.\ntbl_name (str): The name of the target table.\nvalidation_duration (float): The duration of the validation in seconds.\n\nNote that the summary dictionary is only available within the context of a final action. If called outside of a final action (i.e., when no final action is being executed), this function will return None."
  },
  {
    "objectID": "reference/get_validation_summary.html#examples",
    "href": "reference/get_validation_summary.html#examples",
    "title": "get_validation_summary",
    "section": "",
    "text": "Final actions are executed after the completion of all validation steps. They provide an opportunity to take appropriate actions based on the overall validation results. Here’s an example of a final action function (send_report()) that sends an alert when critical validation failures are detected:\nimport pointblank as pb\n\ndef send_report():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        # Send an alert email\n        send_alert_email(\n            subject=f\"CRITICAL validation failures in {summary['tbl_name']}\",\n            body=f\"{summary['n_critical_steps']} steps failed with critical severity.\"\n        )\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_report)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nNote that send_alert_email() in the example above is a placeholder function that would be implemented by the user to send email alerts. This function is not provided by the Pointblank package.\nThe get_validation_summary() function can also be used to create custom reporting for validation results:\ndef log_validation_results():\n    summary = pb.get_validation_summary()\n\n    print(f\"Validation completed with status: {summary['highest_severity'].upper()}\")\n    print(f\"Steps: {summary['n_steps']} total\")\n    print(f\"  - {summary['n_passing_steps']} passing, {summary['n_failing_steps']} failing\")\n    print(\n        f\"  - Severity: {summary['n_warning_steps']} warnings, \"\n        f\"{summary['n_error_steps']} errors, \"\n        f\"{summary['n_critical_steps']} critical\"\n    )\n\n    if summary['highest_severity'] in [\"error\", \"critical\"]:\n        print(\"⚠️ Action required: Please review failing validation steps!\")\nFinal actions work well with both simple logging and more complex notification systems, allowing you to integrate validation results into your broader data quality workflows."
  },
  {
    "objectID": "reference/get_validation_summary.html#see-also",
    "href": "reference/get_validation_summary.html#see-also",
    "title": "get_validation_summary",
    "section": "",
    "text": "Have a look at FinalActions for more information on how to create custom actions that are executed after all validation steps have been completed."
  },
  {
    "objectID": "reference/preview.html",
    "href": "reference/preview.html",
    "title": "preview",
    "section": "",
    "text": "preview(\n    data,\n    columns_subset=None,\n    n_head=5,\n    n_tail=5,\n    limit=50,\n    show_row_numbers=True,\n    max_col_width=250,\n    min_tbl_width=500,\n    incl_header=None,\n)\nDisplay a table preview that shows some rows from the top, some from the bottom.\nTo get a quick look at the data in a table, we can use the preview() function to display a preview of the table. The function shows a subset of the rows from the start and end of the table, with the number of rows from the start and end determined by the n_head= and n_tail= parameters (set to 5 by default). This function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.).\nThe view is optimized for readability, with column names and data types displayed in a compact format. The column widths are sized to fit the column names, dtypes, and column content up to a configurable maximum width of max_col_width= pixels. The table can be scrolled horizontally to view even very large datasets. Since the output is a Great Tables (GT) object, it can be further customized using the great_tables API."
  },
  {
    "objectID": "reference/preview.html#parameters",
    "href": "reference/preview.html#parameters",
    "title": "preview",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to preview, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ncolumns_subset : str | list[str] | Column | None = None\n\nThe columns to display in the table, by default None (all columns are shown). This can be a string, a list of strings, a Column object, or a ColumnSelector object. The latter two options allow for more flexible column selection using column selector functions. Errors are raised if the column names provided don’t match any columns in the table (when provided as a string or list of strings) or if column selector expressions don’t resolve to any columns.\n\nn_head : int = 5\n\nThe number of rows to show from the start of the table. Set to 5 by default.\n\nn_tail : int = 5\n\nThe number of rows to show from the end of the table. Set to 5 by default.\n\nlimit : int = 50\n\nThe limit value for the sum of n_head= and n_tail= (the total number of rows shown). If the sum of n_head= and n_tail= exceeds the limit, an error is raised. The default value is 50.\n\nshow_row_numbers : bool = True\n\nShould row numbers be shown? The numbers shown reflect the row numbers of the head and tail in the input data= table. By default, this is set to True.\n\nmax_col_width : int = 250\n\nThe maximum width of the columns (in pixels) before the text is truncated. The default value is 250 (\"250px\").\n\nmin_tbl_width : int = 500\n\nThe minimum width of the table in pixels. If the sum of the column widths is less than this value, the all columns are sized up to reach this minimum width value. The default value is 500 (\"500px\").\n\nincl_header : bool = None\n\nShould the table include a header with the table type and table dimensions? Set to True by default."
  },
  {
    "objectID": "reference/preview.html#returns",
    "href": "reference/preview.html#returns",
    "title": "preview",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the preview of the table."
  },
  {
    "objectID": "reference/preview.html#supported-input-table-types",
    "href": "reference/preview.html#supported-input-table-types",
    "title": "preview",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nMicrosoft SQL Server table (\"mssql\")*\nSnowflake table (\"snowflake\")*\nDatabricks table (\"databricks\")*\nPySpark table (\"pyspark\")*\nBigQuery table (\"bigquery\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using preview() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/preview.html#examples",
    "href": "reference/preview.html#examples",
    "title": "preview",
    "section": "Examples",
    "text": "Examples\nIt’s easy to preview a table using the preview() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table_polars = pb.load_dataset(\"small_table\")\n\npb.preview(small_table_polars)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThis table is a Polars DataFrame, but the preview() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nsmall_table_duckdb = pb.load_dataset(\"small_table\", tbl_type=\"duckdb\")\n\npb.preview(small_table_duckdb)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows13Columns8\n  \n\n  \n  date_timetimestamp\n  datedate\n  aint64\n  bstring\n  cint64\n  dfloat64\n  eboolean\n  fstring\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    NULL\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    NULL\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThe blue dividing line marks the end of the first n_head= rows and the start of the last n_tail= rows.\nWe can adjust the number of rows shown from the start and end of the table by setting the n_head= and n_tail= parameters. Let’s enlarge each of these to 10:\n\npb.preview(small_table_polars, n_head=10, n_tail=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIn the above case, the entire dataset is shown since the sum of n_head= and n_tail= is greater than the number of rows in the table (which is 13).\nThe columns_subset= parameter can be used to show only specific columns in the table. You can provide a list of column names to make the selection. Let’s try that with the \"game_revenue\" dataset as a Pandas DataFrame:\n\ngame_revenue_pandas = pb.load_dataset(\"game_revenue\", tbl_type=\"pandas\")\n\npb.preview(game_revenue_pandas, columns_subset=[\"player_id\", \"item_name\", \"item_revenue\"])\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  player_idobject\n  item_nameobject\n  item_revenuefloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    offer2\n    8.99\n  \n  \n    2\n    ECPANOIXLZHF896\n    gems3\n    22.49\n  \n  \n    3\n    ECPANOIXLZHF896\n    gold7\n    107.99\n  \n  \n    4\n    ECPANOIXLZHF896\n    ad_20sec\n    0.76\n  \n  \n    5\n    ECPANOIXLZHF896\n    ad_5sec\n    0.03\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    ad_survey\n    1.332\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    ad_survey\n    1.35\n  \n  \n    1998\n    RMOSWHJGELCI675\n    ad_5sec\n    0.03\n  \n  \n    1999\n    RMOSWHJGELCI675\n    offer5\n    26.09\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    ad_5sec\n    0.12\n  \n\n\n\n\n\n\n        \n\n\nAlternatively, we can use column selector functions like starts_with() and matches()` to select columns based on text or patterns:\n\npb.preview(game_revenue_pandas, n_head=2, n_tail=2, columns_subset=pb.starts_with(\"session\"))\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  session_idobject\n  session_startdatetime64[ns, UTC]\n  session_durationfloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    16.3\n  \n  \n    2\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    16.3\n  \n  \n    1999\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    8.4\n  \n  \n    2000\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    18.5\n  \n\n\n\n\n\n\n        \n\n\nMultiple column selector functions can be combined within col() using operators like | and &:\n\npb.preview(\n  game_revenue_pandas,\n  n_head=2,\n  n_tail=2,\n  columns_subset=pb.col(pb.starts_with(\"item\") | pb.matches(\"player\"))\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  player_idobject\n  item_typeobject\n  item_nameobject\n  item_revenuefloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    iap\n    offer2\n    8.99\n  \n  \n    2\n    ECPANOIXLZHF896\n    iap\n    gems3\n    22.49\n  \n  \n    1999\n    RMOSWHJGELCI675\n    iap\n    offer5\n    26.09\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    ad\n    ad_5sec\n    0.12"
  },
  {
    "objectID": "reference/Validate.all_passed.html",
    "href": "reference/Validate.all_passed.html",
    "title": "Validate.all_passed",
    "section": "",
    "text": "Validate.all_passed()\nDetermine if every validation step passed perfectly, with no failing test units.\nThe all_passed() method determines if every validation step passed perfectly, with no failing test units. This method is useful for quickly checking if the table passed all validation steps with flying colors. If there’s even a single failing test unit in any validation step, this method will return False.\nThis validation metric might be overly stringent for some validation plans where failing test units are generally expected (and the strategy is to monitor data quality over time). However, the value of all_passed() could be suitable for validation plans designed to ensure that every test unit passes perfectly (e.g., checks for column presence, null-checking tests, etc.)."
  },
  {
    "objectID": "reference/Validate.all_passed.html#returns",
    "href": "reference/Validate.all_passed.html#returns",
    "title": "Validate.all_passed",
    "section": "Returns",
    "text": "Returns\n\n : bool\n\nTrue if all validation steps had no failing test units, False otherwise."
  },
  {
    "objectID": "reference/Validate.all_passed.html#examples",
    "href": "reference/Validate.all_passed.html#examples",
    "title": "Validate.all_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the second step will have a failing test unit (the value 10 isn’t less than 9). After interrogation, the all_passed() method is used to determine if all validation steps passed perfectly.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 9, 5],\n        \"b\": [5, 6, 10, 3],\n        \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_vals_lt(columns=\"b\", value=9)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.all_passed()\n\nFalse\n\n\nThe returned value is False since the second validation step had a failing test unit. If it weren’t for that one failing test unit, the return value would have been True."
  },
  {
    "objectID": "reference/Validate.conjointly.html",
    "href": "reference/Validate.conjointly.html",
    "title": "Validate.conjointly",
    "section": "",
    "text": "Validate.conjointly(\n    *exprs,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nPerform multiple row-wise validations for joint validity.\nThe conjointly() validation method checks whether each row in the table passes multiple validation conditions simultaneously. This enables compound validation logic where a test unit (typically a row) must satisfy all specified conditions to pass the validation.\nThis method accepts multiple validation expressions as callables, which should return boolean expressions when applied to the data. You can use lambdas that incorporate Polars/Pandas/Ibis expressions (based on the target table type) or create more complex validation functions. The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.conjointly.html#parameters",
    "href": "reference/Validate.conjointly.html#parameters",
    "title": "Validate.conjointly",
    "section": "Parameters",
    "text": "Parameters\n\n*exprs : Callable = ()\n\nMultiple validation expressions provided as callable functions. Each callable should accept a table as its single argument and return a boolean expression or Series/Column that evaluates to boolean values for each row.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.conjointly.html#returns",
    "href": "reference/Validate.conjointly.html#returns",
    "title": "Validate.conjointly",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.conjointly.html#preprocessing",
    "href": "reference/Validate.conjointly.html#preprocessing",
    "title": "Validate.conjointly",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.conjointly.html#thresholds",
    "href": "reference/Validate.conjointly.html#thresholds",
    "title": "Validate.conjointly",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.conjointly.html#examples",
    "href": "reference/Validate.conjointly.html#examples",
    "title": "Validate.conjointly",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 7, 1, 3, 9, 4],\n        \"b\": [6, 3, 0, 5, 8, 2],\n        \"c\": [10, 4, 8, 9, 10, 5],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    6\n    10\n  \n  \n    2\n    7\n    3\n    4\n  \n  \n    3\n    1\n    0\n    8\n  \n  \n    4\n    3\n    5\n    9\n  \n  \n    5\n    9\n    8\n    10\n  \n  \n    6\n    4\n    2\n    5\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the values in each row satisfy multiple conditions simultaneously:\n\nColumn a should be greater than 2\nColumn b should be less than 7\nThe sum of a and b should be less than the value in column c\n\nWe’ll use conjointly() to check all these conditions together:\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pl.col(\"a\") &gt; 2,\n        lambda df: pl.col(\"b\") &lt; 7,\n        lambda df: pl.col(\"a\") + pl.col(\"b\") &lt; pl.col(\"c\")\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    10.17\n    50.83\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that not all rows satisfy all three conditions together. For a row to pass the conjoint validation, all three conditions must be true for that row.\nWe can also use preprocessing to filter the data before applying the conjoint validation:\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pl.col(\"a\") &gt; 2,\n        lambda df: pl.col(\"b\") &lt; 7,\n        lambda df: pl.col(\"a\") + pl.col(\"b\") &lt; pl.col(\"c\"),\n        pre=lambda df: df.filter(pl.col(\"c\") &gt; 5)\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    10.25\n    30.75\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThis allows for more complex validation scenarios where the data is first prepared and then validated against multiple conditions simultaneously.\nOr, you can use the backend-agnostic column expression helper expr_col() to write expressions that work across different table backends:\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 7, 1, 3, 9, 4],\n        \"b\": [6, 3, 0, 5, 8, 2],\n        \"c\": [10, 4, 8, 9, 10, 5],\n    }\n)\n\n# Using backend-agnostic syntax with expr_col()\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pb.expr_col(\"a\") &gt; 2,\n        lambda df: pb.expr_col(\"b\") &lt; 7,\n        lambda df: pb.expr_col(\"a\") + pb.expr_col(\"b\") &lt; pb.expr_col(\"c\")\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    10.17\n    50.83\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nUsing expr_col() allows your validation code to work consistently across Pandas, Polars, and Ibis table backends without changes, making your validation pipelines more portable."
  },
  {
    "objectID": "reference/Validate.conjointly.html#see-also",
    "href": "reference/Validate.conjointly.html#see-also",
    "title": "Validate.conjointly",
    "section": "See Also",
    "text": "See Also\nLook at the documentation of the expr_col() function for more information on how to use it with different table backends."
  },
  {
    "objectID": "reference/Actions.html",
    "href": "reference/Actions.html",
    "title": "Actions",
    "section": "",
    "text": "Actions(\n    self,\n    warning=None,\n    error=None,\n    critical=None,\n    default=None,\n    highest_only=True,\n)\nDefinition of action values.\nActions complement threshold values by defining what action should be taken when a threshold level is reached. The action can be a string or a Callable. When a string is used, it is interpreted as a message to be displayed. When a Callable is used, it will be invoked at interrogation time if the threshold level is met or exceeded.\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. These levels correspond to different levels of severity when a threshold is reached. Those thresholds can be defined using the Thresholds class or various shorthand forms. Actions don’t have to be defined for all threshold levels; if an action is not defined for a level in exceedance, no action will be taken. Likewise, there is no negative consequence (other than a no-op) for defining actions for thresholds that don’t exist (e.g., setting an action for the ‘critical’ level when no corresponding ‘critical’ threshold has been set)."
  },
  {
    "objectID": "reference/Actions.html#parameters",
    "href": "reference/Actions.html#parameters",
    "title": "Actions",
    "section": "Parameters",
    "text": "Parameters\n\nwarning : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘warning’ level. Using None means no action should be performed at the ‘warning’ level.\n\nerror : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘error’ level. Using None means no action should be performed at the ‘error’ level.\n\ncritical : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘critical’ level. Using None means no action should be performed at the ‘critical’ level.\n\ndefault : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for all threshold levels. This parameter can be used to set the same action for all threshold levels. If an action is defined for a specific threshold level, it will override the action set for all levels.\n\nhighest_only : bool = True\n\nA boolean value that, when set to True (the default), results in executing only the action for the highest threshold level that is exceeded. Useful when you want to ensure that only the most severe action is taken when multiple threshold levels are exceeded."
  },
  {
    "objectID": "reference/Actions.html#returns",
    "href": "reference/Actions.html#returns",
    "title": "Actions",
    "section": "Returns",
    "text": "Returns\n\n : Actions\n\nAn Actions object. This can be used when using the Validate class (to set actions for meeting different threshold levels globally) or when defining validation steps like col_vals_gt() (so that actions are scoped to individual validation steps, overriding any globally set actions)."
  },
  {
    "objectID": "reference/Actions.html#types-of-actions",
    "href": "reference/Actions.html#types-of-actions",
    "title": "Actions",
    "section": "Types of Actions",
    "text": "Types of Actions\nActions can be defined in different ways:\n\nString: A message to be displayed when the threshold level is met or exceeded.\nCallable: A function that is called when the threshold level is met or exceeded.\nList of Strings/Callables: Multiple messages or functions to be called when the threshold level is met or exceeded.\n\nThe actions are executed at interrogation time when the threshold level assigned to the action is exceeded by the number or proportion of failing test units. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of interrogation. If providing a list of strings or callables, each item in the list will be executed in order. Such a list can contain a mix of strings and callables."
  },
  {
    "objectID": "reference/Actions.html#string-templating",
    "href": "reference/Actions.html#string-templating",
    "title": "Actions",
    "section": "String Templating",
    "text": "String Templating\nWhen using a string as an action, you can include placeholders for the following variables:\n\n{type}: The validation step type where the action is executed (e.g., ‘col_vals_gt’, ‘col_vals_lt’, etc.)\n{level}: The threshold level where the action is executed (‘warning’, ‘error’, or ‘critical’)\n{step} or {i}: The step number in the validation workflow where the action is executed\n{col} or {column}: The column name where the action is executed\n{val} or {value}: An associated value for the validation method (e.g., the value to compare against in a ‘col_vals_gt’ validation step)\n{time}: A datetime value for when the action was executed\n\nThe first two placeholders can also be used in uppercase (e.g., {TYPE} or {LEVEL}) and the corresponding values will be displayed in uppercase. The placeholders are replaced with the actual values during interrogation.\nFor example, the string \"{LEVEL}: '{type}' threshold exceeded for column {col}.\" will be displayed as \"WARNING: 'col_vals_gt' threshold exceeded for column a.\" when the ‘warning’ threshold is exceeded in a ‘col_vals_gt’ validation step involving column a."
  },
  {
    "objectID": "reference/Actions.html#crafting-callables-with-get_action_metadata",
    "href": "reference/Actions.html#crafting-callables-with-get_action_metadata",
    "title": "Actions",
    "section": "Crafting Callables with get_action_metadata()",
    "text": "Crafting Callables with get_action_metadata()\nWhen creating a callable function to be used as an action, you can use the get_action_metadata() function to retrieve metadata about the step where the action is executed. This metadata contains information about the validation step, including the step type, level, step number, column name, and associated value. You can use this information to craft your action message or to take specific actions based on the metadata provided."
  },
  {
    "objectID": "reference/Actions.html#examples",
    "href": "reference/Actions.html#examples",
    "title": "Actions",
    "section": "Examples",
    "text": "Examples\nLet’s define both threshold values and actions for a data validation workflow. We’ll set these thresholds and actions globally for all validation steps. In this specific example, the only actions we’ll define are for the ‘critical’ level:\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=\"Major data quality issue found in step {step}.\"),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation\n\nMajor data quality issue found in step 3.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:16:19DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nBecause we set the ‘critical’ action to display \"Major data quality issue found.\" in the console, this message will be displayed if the number of failing test units exceeds the ‘critical’ threshold (set to 15% of the total number of test units). In step 3 of the validation workflow, the ‘critical’ threshold is exceeded, so the message is displayed in the console.\nActions can be defined locally for individual validation steps, which will override any global actions set at the beginning of the validation workflow. Here’s a variation of the above example where we set global threshold values but assign an action only for an individual validation step:\n\ndef dq_issue():\n    from datetime import datetime\n\n    print(f\"Data quality issue found ({datetime.now()}).\")\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n        actions=pb.Actions(warning=dq_issue),\n    )\n    .interrogate()\n)\n\nvalidation\n\nData quality issue found (2025-05-23 02:16:20.396724).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:16:20DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the ‘warning’ action is set to call the dq_issue() function. This action is only executed when the ‘warning’ threshold is exceeded in the ‘session_duration’ column. Because all three thresholds are exceeded in step 3, the ‘warning’ action of executing the function occurs (resulting in a message being printed to the console). If actions were set for the other two threshold levels, they would also be executed."
  },
  {
    "objectID": "reference/Actions.html#see-also",
    "href": "reference/Actions.html#see-also",
    "title": "Actions",
    "section": "See Also",
    "text": "See Also\nThe get_action_metadata() function, which can be used to retrieve metadata about the step where the action is executed."
  },
  {
    "objectID": "reference/Validate.n.html",
    "href": "reference/Validate.n.html",
    "title": "Validate.n",
    "section": "",
    "text": "Validate.n(i=None, scalar=False)\nProvides a dictionary of the number of test units for each validation step.\nThe n() method provides the number of test units for each validation step. This is the total number of test units that were evaluated in the validation step. It is always an integer value.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. The total number of test units for a validation step is the sum of the number of passing and failing test units (i.e., n = n_passed + n_failed)."
  },
  {
    "objectID": "reference/Validate.n.html#parameters",
    "href": "reference/Validate.n.html#parameters",
    "title": "Validate.n",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n.html#returns",
    "href": "reference/Validate.n.html#returns",
    "title": "Validate.n",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n.html#examples",
    "href": "reference/Validate.n.html#examples",
    "title": "Validate.n",
    "section": "Examples",
    "text": "Examples\nDifferent types of validation steps can have different numbers of test units. In the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the number of test units for each step will be a little bit different.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 9, 5],\n        \"b\": [5, 6, 10, 3],\n        \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_exists(columns=\"b\")\n    .col_vals_lt(columns=\"b\", value=9, pre=lambda df: df.filter(pl.col(\"a\") &gt; 1))\n    .interrogate()\n)\n\nThe first validation step checks that all values in column a are greater than 0. Let’s use the n() method to determine the number of test units this validation step.\n\nvalidation.n(i=1, scalar=True)\n\n4\n\n\nThe returned value of 4 is the number of test units for the first validation step. This value is the same as the number of rows in the table.\nThe second validation step checks for the existence of column b. Using the n() method we can get the number of test units for this the second step.\n\nvalidation.n(i=2, scalar=True)\n\n1\n\n\nThere’s a single test unit here because the validation step is checking for the presence of a single column.\nThe third validation step checks that all values in column b are less than 9 after filtering the table to only include rows where the value in column a is greater than 1. Because the table is filtered, the number of test units will be less than the total number of rows in the input table. Let’s prove this by using the n() method.\n\nvalidation.n(i=3, scalar=True)\n\n3\n\n\nThe returned value of 3 is the number of test units for the third validation step. When using the pre= argument, the input table can be mutated before performing the validation. The n() method is a good way to determine whether the mutation performed as expected.\nIn all of these examples, the scalar=True argument was used to return the value as a scalar integer value. If scalar=False, the method will return a dictionary with an entry for the validation step number (from the i= argument) and the number of test units. Futhermore, leaving out the i= argument altogether will return a dictionary with filled with the number of test units for each validation step. Here’s what that looks like:\n\nvalidation.n()\n\n{1: 4, 2: 1, 3: 3}"
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html",
    "href": "reference/Validate.col_vals_lt.html",
    "title": "Validate.col_vals_lt",
    "section": "",
    "text": "Validate.col_vals_lt(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data less than a fixed value or data in another column?\nThe col_vals_lt() validation method checks whether column values in a table are less than a specified value= (the exact comparison used in this function is col_val &lt; value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#parameters",
    "href": "reference/Validate.col_vals_lt.html#parameters",
    "title": "Validate.col_vals_lt",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#returns",
    "href": "reference/Validate.col_vals_lt.html#returns",
    "title": "Validate.col_vals_lt",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_lt.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_lt",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#preprocessing",
    "href": "reference/Validate.col_vals_lt.html#preprocessing",
    "title": "Validate.col_vals_lt",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#segmentation",
    "href": "reference/Validate.col_vals_lt.html#segmentation",
    "title": "Validate.col_vals_lt",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#thresholds",
    "href": "reference/Validate.col_vals_lt.html#thresholds",
    "title": "Validate.col_vals_lt",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#examples",
    "href": "reference/Validate.col_vals_lt.html#examples",
    "title": "Validate.col_vals_lt",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [1, 2, 1, 2, 2, 2],\n        \"c\": [2, 1, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    2\n    1\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    2\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    2\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all less than the value of 10. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=\"a\", value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_lt(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_lt() to check whether the values in column b are less than values in column c.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=\"b\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 1: b is 2 and c is 1.\nRow 2: b is 1 and c is 1."
  },
  {
    "objectID": "reference/DraftValidation.html",
    "href": "reference/DraftValidation.html",
    "title": "DraftValidation",
    "section": "",
    "text": "DraftValidation(self, data, model, api_key=None)\nDraft a validation plan for a given table using an LLM.\nBy using a large language model (LLM) to draft a validation plan, you can quickly generate a starting point for validating a table. This can be useful when you have a new table and you want to get a sense of how to validate it (and adjustments could always be made later). The DraftValidation class uses the chatlas package to draft a validation plan for a given table using an LLM from either the \"anthropic\", \"openai\", \"ollama\" or \"bedrock\" provider. You can install all requirements for the class through an optional ‘generate’ install of Pointblank via pip install pointblank[generate].\n\n\n\n\n\n\nWarning\n\n\n\nThe DraftValidation class is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\ndata : FrameT | Any\n\nThe data to be used for drafting a validation plan.\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\napi_key : str | None = None\n\nThe API key to be used for the model.\n\n\n\n\n\n\n : str\n\nThe drafted validation plan.\n\n\n\n\n\nThe model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names.\n\n\n\nProviding a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way.\n\n\n\nThe data sent to the model provider is a JSON summary of the table. This data summary is generated internally by DraftValidation using the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information to draft a validation plan. As such, even very large tables can be used with the DraftValidation class since the contents of the table are not sent to the model provider.\nThe Amazon Bedrock is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally, though only a few models are capable enough to perform the task of drafting a validation plan.\n\n\n\nLet’s look at how the DraftValidation class can be used to draft a validation plan for a table. The table to be used is \"nycflights\", which is available here via the load_dataset() function. The model to be used is \"anthropic:claude-3-5-sonnet-latest\" (which performs very well compared to other LLMs). The example assumes that the API key is stored in an .env file as ANTHROPIC_API_KEY.\nimport pointblank as pb\n\n# Load the \"nycflights\" dataset as a DuckDB table\ndata = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\n# Draft a validation plan for the \"nycflights\" table\npb.DraftValidation(data=data, model=\"anthropic:claude-3-5-sonnet-latest\")\nThe output will be a drafted validation plan for the \"nycflights\" table and this will appear in the console.\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"year\", \"int64\"),\n    (\"month\", \"int64\"),\n    (\"day\", \"int64\"),\n    (\"dep_time\", \"int64\"),\n    (\"sched_dep_time\", \"int64\"),\n    (\"dep_delay\", \"int64\"),\n    (\"arr_time\", \"int64\"),\n    (\"sched_arr_time\", \"int64\"),\n    (\"arr_delay\", \"int64\"),\n    (\"carrier\", \"string\"),\n    (\"flight\", \"int64\"),\n    (\"tailnum\", \"string\"),\n    (\"origin\", \"string\"),\n    (\"dest\", \"string\"),\n    (\"air_time\", \"int64\"),\n    (\"distance\", \"int64\"),\n    (\"hour\", \"int64\"),\n    (\"minute\", \"int64\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"year\", \"month\", \"day\", \"sched_dep_time\", \"carrier\", \"flight\",\n        \"origin\", \"dest\", \"distance\", \"hour\", \"minute\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12)\n    .col_vals_between(columns=\"day\", left=1, right=31)\n    .col_vals_between(columns=\"sched_dep_time\", left=106, right=2359)\n    .col_vals_between(columns=\"dep_delay\", left=-43, right=1301, na_pass=True)\n    .col_vals_between(columns=\"air_time\", left=20, right=695, na_pass=True)\n    .col_vals_between(columns=\"distance\", left=17, right=4983)\n    .col_vals_between(columns=\"hour\", left=1, right=23)\n    .col_vals_between(columns=\"minute\", left=0, right=59)\n    .col_vals_in_set(columns=\"origin\", set=[\"EWR\", \"LGA\", \"JFK\"])\n    .col_count_match(count=18)\n    .row_count_match(count=336776)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\nThe drafted validation plan can be copied and pasted into a Python script or notebook for further use. In other words, the generated plan can be adjusted as needed to suit the specific requirements of the table being validated.\nNote that the output does not know how the data was obtained, so it uses the placeholder your_data in the data= argument of the Validate class. When adapted for use, this should be replaced with the actual data variable."
  },
  {
    "objectID": "reference/DraftValidation.html#parameters",
    "href": "reference/DraftValidation.html#parameters",
    "title": "DraftValidation",
    "section": "",
    "text": "data : FrameT | Any\n\nThe data to be used for drafting a validation plan.\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\napi_key : str | None = None\n\nThe API key to be used for the model."
  },
  {
    "objectID": "reference/DraftValidation.html#returns",
    "href": "reference/DraftValidation.html#returns",
    "title": "DraftValidation",
    "section": "",
    "text": ": str\n\nThe drafted validation plan."
  },
  {
    "objectID": "reference/DraftValidation.html#constructing-the-model-argument",
    "href": "reference/DraftValidation.html#constructing-the-model-argument",
    "title": "DraftValidation",
    "section": "",
    "text": "The model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names."
  },
  {
    "objectID": "reference/DraftValidation.html#notes-on-authentication",
    "href": "reference/DraftValidation.html#notes-on-authentication",
    "title": "DraftValidation",
    "section": "",
    "text": "Providing a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way."
  },
  {
    "objectID": "reference/DraftValidation.html#notes-on-data-sent-to-the-model-provider",
    "href": "reference/DraftValidation.html#notes-on-data-sent-to-the-model-provider",
    "title": "DraftValidation",
    "section": "",
    "text": "The data sent to the model provider is a JSON summary of the table. This data summary is generated internally by DraftValidation using the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information to draft a validation plan. As such, even very large tables can be used with the DraftValidation class since the contents of the table are not sent to the model provider.\nThe Amazon Bedrock is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally, though only a few models are capable enough to perform the task of drafting a validation plan."
  },
  {
    "objectID": "reference/DraftValidation.html#examples",
    "href": "reference/DraftValidation.html#examples",
    "title": "DraftValidation",
    "section": "",
    "text": "Let’s look at how the DraftValidation class can be used to draft a validation plan for a table. The table to be used is \"nycflights\", which is available here via the load_dataset() function. The model to be used is \"anthropic:claude-3-5-sonnet-latest\" (which performs very well compared to other LLMs). The example assumes that the API key is stored in an .env file as ANTHROPIC_API_KEY.\nimport pointblank as pb\n\n# Load the \"nycflights\" dataset as a DuckDB table\ndata = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\n# Draft a validation plan for the \"nycflights\" table\npb.DraftValidation(data=data, model=\"anthropic:claude-3-5-sonnet-latest\")\nThe output will be a drafted validation plan for the \"nycflights\" table and this will appear in the console.\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"year\", \"int64\"),\n    (\"month\", \"int64\"),\n    (\"day\", \"int64\"),\n    (\"dep_time\", \"int64\"),\n    (\"sched_dep_time\", \"int64\"),\n    (\"dep_delay\", \"int64\"),\n    (\"arr_time\", \"int64\"),\n    (\"sched_arr_time\", \"int64\"),\n    (\"arr_delay\", \"int64\"),\n    (\"carrier\", \"string\"),\n    (\"flight\", \"int64\"),\n    (\"tailnum\", \"string\"),\n    (\"origin\", \"string\"),\n    (\"dest\", \"string\"),\n    (\"air_time\", \"int64\"),\n    (\"distance\", \"int64\"),\n    (\"hour\", \"int64\"),\n    (\"minute\", \"int64\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"year\", \"month\", \"day\", \"sched_dep_time\", \"carrier\", \"flight\",\n        \"origin\", \"dest\", \"distance\", \"hour\", \"minute\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12)\n    .col_vals_between(columns=\"day\", left=1, right=31)\n    .col_vals_between(columns=\"sched_dep_time\", left=106, right=2359)\n    .col_vals_between(columns=\"dep_delay\", left=-43, right=1301, na_pass=True)\n    .col_vals_between(columns=\"air_time\", left=20, right=695, na_pass=True)\n    .col_vals_between(columns=\"distance\", left=17, right=4983)\n    .col_vals_between(columns=\"hour\", left=1, right=23)\n    .col_vals_between(columns=\"minute\", left=0, right=59)\n    .col_vals_in_set(columns=\"origin\", set=[\"EWR\", \"LGA\", \"JFK\"])\n    .col_count_match(count=18)\n    .row_count_match(count=336776)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\nThe drafted validation plan can be copied and pasted into a Python script or notebook for further use. In other words, the generated plan can be adjusted as needed to suit the specific requirements of the table being validated.\nNote that the output does not know how the data was obtained, so it uses the placeholder your_data in the data= argument of the Validate class. When adapted for use, this should be replaced with the actual data variable."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "When performing data validation, you’ll need the Validate class to get the process started. It’s given the target table and you can optionally provide some metadata and/or failure thresholds (using the Thresholds class or through shorthands for this task). The Validate class has numerous methods for defining validation steps and for obtaining post-interrogation metrics and data.\n\n\n\nValidate\nWorkflow for defining a set of validations on a table and interrogating for results.\n\n\nThresholds\nDefinition of threshold values.\n\n\nActions\nDefinition of action values.\n\n\nFinalActions\nDefine actions to be taken after validation is complete.\n\n\nSchema\nDefinition of a schema object.\n\n\nDraftValidation\nDraft a validation plan for a given table using an LLM.\n\n\n\n\n\n\nValidation steps can be thought of as sequential validations on the target data. We call Validate’s validation methods to build up a validation plan: a collection of steps that, in the aggregate, provides good validation coverage.\n\n\n\nValidate.col_vals_gt\nAre column data greater than a fixed value or data in another column?\n\n\nValidate.col_vals_lt\nAre column data less than a fixed value or data in another column?\n\n\nValidate.col_vals_ge\nAre column data greater than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_le\nAre column data less than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_eq\nAre column data equal to a fixed value or data in another column?\n\n\nValidate.col_vals_ne\nAre column data not equal to a fixed value or data in another column?\n\n\nValidate.col_vals_between\nDo column data lie between two specified values or data in other columns?\n\n\nValidate.col_vals_outside\nDo column data lie outside of two specified values or data in other columns?\n\n\nValidate.col_vals_in_set\nValidate whether column values are in a set of values.\n\n\nValidate.col_vals_not_in_set\nValidate whether column values are not in a set of values.\n\n\nValidate.col_vals_null\nValidate whether values in a column are NULL.\n\n\nValidate.col_vals_not_null\nValidate whether values in a column are not NULL.\n\n\nValidate.col_vals_regex\nValidate whether column values match a regular expression pattern.\n\n\nValidate.col_vals_expr\nValidate column values using a custom expression.\n\n\nValidate.rows_distinct\nValidate whether rows in the table are distinct.\n\n\nValidate.rows_complete\nValidate whether row data are complete by having no missing values.\n\n\nValidate.col_exists\nValidate whether one or more columns exist in the table.\n\n\nValidate.col_schema_match\nDo columns in the table (and their types) match a predefined schema?\n\n\nValidate.row_count_match\nValidate whether the row count of the table matches a specified count.\n\n\nValidate.col_count_match\nValidate whether the column count of the table matches a specified count.\n\n\nValidate.conjointly\nPerform multiple row-wise validations for joint validity.\n\n\nValidate.specially\nPerform a specialized validation with customized logic.\n\n\n\n\n\n\nA flexible way to select columns for validation is to use the col() function along with column selection helper functions. A combination of col() + starts_with(), matches(), etc., allows for the selection of multiple target columns (mapping a validation across many steps). Furthermore, the col() function can be used to declare a comparison column (e.g., for the value= argument in many col_vals_*() methods) when you can’t use a fixed value for comparison.\n\n\n\ncol\nHelper function for referencing a column in the input table.\n\n\nstarts_with\nSelect columns that start with specified text.\n\n\nends_with\nSelect columns that end with specified text.\n\n\ncontains\nSelect columns that contain specified text.\n\n\nmatches\nSelect columns that match a specified regular expression pattern.\n\n\neverything\nSelect all columns.\n\n\nfirst_n\nSelect the first n columns in the column list.\n\n\nlast_n\nSelect the last n columns in the column list.\n\n\nexpr_col\nCreate a column expression for use in conjointly() validation.\n\n\n\n\n\n\nThe validation plan is put into action when interrogate() is called. The workflow for performing a comprehensive validation is then: (1) Validate(), (2) adding validation steps, (3) interrogate(). After interrogation of the data, we can view a validation report table (by printing the object or using get_tabular_report()), extract key metrics, or we can split the data based on the validation results (with get_sundered_data()).\n\n\n\nValidate.interrogate\nExecute each validation step against the table and store the results.\n\n\nValidate.get_tabular_report\nValidation report as a GT table.\n\n\nValidate.get_step_report\nGet a detailed report for a single validation step.\n\n\nValidate.get_json_report\nGet a report of the validation results as a JSON-formatted string.\n\n\nValidate.get_sundered_data\nGet the data that passed or failed the validation steps.\n\n\nValidate.get_data_extracts\nGet the rows that failed for each validation step.\n\n\nValidate.all_passed\nDetermine if every validation step passed perfectly, with no failing test units.\n\n\nValidate.assert_passing\nRaise an AssertionError if all tests are not passing.\n\n\nValidate.assert_below_threshold\nRaise an AssertionError if validation steps exceed a specified threshold level.\n\n\nValidate.above_threshold\nCheck if any validation steps exceed a specified threshold level.\n\n\nValidate.n\nProvides a dictionary of the number of test units for each validation step.\n\n\nValidate.n_passed\nProvides a dictionary of the number of test units that passed for each validation step.\n\n\nValidate.n_failed\nProvides a dictionary of the number of test units that failed for each validation step.\n\n\nValidate.f_passed\nProvides a dictionary of the fraction of test units that passed for each validation step.\n\n\nValidate.f_failed\nProvides a dictionary of the fraction of test units that failed for each validation step.\n\n\nValidate.warning\nGet the ‘warning’ level status for each validation step.\n\n\nValidate.error\nGet the ‘error’ level status for each validation step.\n\n\nValidate.critical\nGet the ‘critical’ level status for each validation step.\n\n\n\n\n\n\nThe Inspection and Assistance group contains functions that are helpful for getting to grips on a new data table. Use the DataScan class to get a quick overview of the data, preview() to see the first and last few rows of a table, col_summary_tbl() for a column-level summary of a table, and missing_vals_tbl() to see where there are missing values in a table. Several datasets included in the package can be accessed via the load_dataset() function. On the assistance side, the assistant() function can be used to get help with Pointblank.\n\n\n\nDataScan\nGet a summary of a dataset.\n\n\npreview\nDisplay a table preview that shows some rows from the top, some from the bottom.\n\n\ncol_summary_tbl\nGenerate a column-level summary table of a dataset.\n\n\nmissing_vals_tbl\nDisplay a table that shows the missing values in the input table.\n\n\nassistant\nChat with the PbA (Pointblank Assistant) about your data validation needs.\n\n\nload_dataset\nLoad a dataset hosted in the library as specified table type.\n\n\n\n\n\n\nThe Utility Functions group contains functions that are useful accessing metadata about the target data. Use get_column_count() or get_row_count() to get the number of columns or rows in a table. The get_action_metadata() function is useful when building custom actions since it returns metadata about the validation step that’s triggering the action. Lastly, the config() utility lets us set global configuration parameters.\n\n\n\nget_column_count\nGet the number of columns in a table.\n\n\nget_row_count\nGet the number of rows in a table.\n\n\nget_action_metadata\nAccess step-level metadata when authoring custom actions.\n\n\nget_validation_summary\nAccess validation summary information when authoring final actions.\n\n\nconfig\nConfiguration settings for the Pointblank library.\n\n\n\n\n\n\nThe Prebuilt Actions group contains a function that can be used to send a Slack notification when validation steps exceed failure threshold levels or just to provide a summary of the validation results, including the status, number of steps, passing and failing steps, table information, and timing details.\n\n\n\nsend_slack_notification\nCreate a Slack notification function using a webhook URL."
  },
  {
    "objectID": "reference/index.html#validate",
    "href": "reference/index.html#validate",
    "title": "API Reference",
    "section": "",
    "text": "When performing data validation, you’ll need the Validate class to get the process started. It’s given the target table and you can optionally provide some metadata and/or failure thresholds (using the Thresholds class or through shorthands for this task). The Validate class has numerous methods for defining validation steps and for obtaining post-interrogation metrics and data.\n\n\n\nValidate\nWorkflow for defining a set of validations on a table and interrogating for results.\n\n\nThresholds\nDefinition of threshold values.\n\n\nActions\nDefinition of action values.\n\n\nFinalActions\nDefine actions to be taken after validation is complete.\n\n\nSchema\nDefinition of a schema object.\n\n\nDraftValidation\nDraft a validation plan for a given table using an LLM."
  },
  {
    "objectID": "reference/index.html#validation-steps",
    "href": "reference/index.html#validation-steps",
    "title": "API Reference",
    "section": "",
    "text": "Validation steps can be thought of as sequential validations on the target data. We call Validate’s validation methods to build up a validation plan: a collection of steps that, in the aggregate, provides good validation coverage.\n\n\n\nValidate.col_vals_gt\nAre column data greater than a fixed value or data in another column?\n\n\nValidate.col_vals_lt\nAre column data less than a fixed value or data in another column?\n\n\nValidate.col_vals_ge\nAre column data greater than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_le\nAre column data less than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_eq\nAre column data equal to a fixed value or data in another column?\n\n\nValidate.col_vals_ne\nAre column data not equal to a fixed value or data in another column?\n\n\nValidate.col_vals_between\nDo column data lie between two specified values or data in other columns?\n\n\nValidate.col_vals_outside\nDo column data lie outside of two specified values or data in other columns?\n\n\nValidate.col_vals_in_set\nValidate whether column values are in a set of values.\n\n\nValidate.col_vals_not_in_set\nValidate whether column values are not in a set of values.\n\n\nValidate.col_vals_null\nValidate whether values in a column are NULL.\n\n\nValidate.col_vals_not_null\nValidate whether values in a column are not NULL.\n\n\nValidate.col_vals_regex\nValidate whether column values match a regular expression pattern.\n\n\nValidate.col_vals_expr\nValidate column values using a custom expression.\n\n\nValidate.rows_distinct\nValidate whether rows in the table are distinct.\n\n\nValidate.rows_complete\nValidate whether row data are complete by having no missing values.\n\n\nValidate.col_exists\nValidate whether one or more columns exist in the table.\n\n\nValidate.col_schema_match\nDo columns in the table (and their types) match a predefined schema?\n\n\nValidate.row_count_match\nValidate whether the row count of the table matches a specified count.\n\n\nValidate.col_count_match\nValidate whether the column count of the table matches a specified count.\n\n\nValidate.conjointly\nPerform multiple row-wise validations for joint validity.\n\n\nValidate.specially\nPerform a specialized validation with customized logic."
  },
  {
    "objectID": "reference/index.html#column-selection",
    "href": "reference/index.html#column-selection",
    "title": "API Reference",
    "section": "",
    "text": "A flexible way to select columns for validation is to use the col() function along with column selection helper functions. A combination of col() + starts_with(), matches(), etc., allows for the selection of multiple target columns (mapping a validation across many steps). Furthermore, the col() function can be used to declare a comparison column (e.g., for the value= argument in many col_vals_*() methods) when you can’t use a fixed value for comparison.\n\n\n\ncol\nHelper function for referencing a column in the input table.\n\n\nstarts_with\nSelect columns that start with specified text.\n\n\nends_with\nSelect columns that end with specified text.\n\n\ncontains\nSelect columns that contain specified text.\n\n\nmatches\nSelect columns that match a specified regular expression pattern.\n\n\neverything\nSelect all columns.\n\n\nfirst_n\nSelect the first n columns in the column list.\n\n\nlast_n\nSelect the last n columns in the column list.\n\n\nexpr_col\nCreate a column expression for use in conjointly() validation."
  },
  {
    "objectID": "reference/index.html#interrogation-and-reporting",
    "href": "reference/index.html#interrogation-and-reporting",
    "title": "API Reference",
    "section": "",
    "text": "The validation plan is put into action when interrogate() is called. The workflow for performing a comprehensive validation is then: (1) Validate(), (2) adding validation steps, (3) interrogate(). After interrogation of the data, we can view a validation report table (by printing the object or using get_tabular_report()), extract key metrics, or we can split the data based on the validation results (with get_sundered_data()).\n\n\n\nValidate.interrogate\nExecute each validation step against the table and store the results.\n\n\nValidate.get_tabular_report\nValidation report as a GT table.\n\n\nValidate.get_step_report\nGet a detailed report for a single validation step.\n\n\nValidate.get_json_report\nGet a report of the validation results as a JSON-formatted string.\n\n\nValidate.get_sundered_data\nGet the data that passed or failed the validation steps.\n\n\nValidate.get_data_extracts\nGet the rows that failed for each validation step.\n\n\nValidate.all_passed\nDetermine if every validation step passed perfectly, with no failing test units.\n\n\nValidate.assert_passing\nRaise an AssertionError if all tests are not passing.\n\n\nValidate.assert_below_threshold\nRaise an AssertionError if validation steps exceed a specified threshold level.\n\n\nValidate.above_threshold\nCheck if any validation steps exceed a specified threshold level.\n\n\nValidate.n\nProvides a dictionary of the number of test units for each validation step.\n\n\nValidate.n_passed\nProvides a dictionary of the number of test units that passed for each validation step.\n\n\nValidate.n_failed\nProvides a dictionary of the number of test units that failed for each validation step.\n\n\nValidate.f_passed\nProvides a dictionary of the fraction of test units that passed for each validation step.\n\n\nValidate.f_failed\nProvides a dictionary of the fraction of test units that failed for each validation step.\n\n\nValidate.warning\nGet the ‘warning’ level status for each validation step.\n\n\nValidate.error\nGet the ‘error’ level status for each validation step.\n\n\nValidate.critical\nGet the ‘critical’ level status for each validation step."
  },
  {
    "objectID": "reference/index.html#inspection-and-assistance",
    "href": "reference/index.html#inspection-and-assistance",
    "title": "API Reference",
    "section": "",
    "text": "The Inspection and Assistance group contains functions that are helpful for getting to grips on a new data table. Use the DataScan class to get a quick overview of the data, preview() to see the first and last few rows of a table, col_summary_tbl() for a column-level summary of a table, and missing_vals_tbl() to see where there are missing values in a table. Several datasets included in the package can be accessed via the load_dataset() function. On the assistance side, the assistant() function can be used to get help with Pointblank.\n\n\n\nDataScan\nGet a summary of a dataset.\n\n\npreview\nDisplay a table preview that shows some rows from the top, some from the bottom.\n\n\ncol_summary_tbl\nGenerate a column-level summary table of a dataset.\n\n\nmissing_vals_tbl\nDisplay a table that shows the missing values in the input table.\n\n\nassistant\nChat with the PbA (Pointblank Assistant) about your data validation needs.\n\n\nload_dataset\nLoad a dataset hosted in the library as specified table type."
  },
  {
    "objectID": "reference/index.html#utility-functions",
    "href": "reference/index.html#utility-functions",
    "title": "API Reference",
    "section": "",
    "text": "The Utility Functions group contains functions that are useful accessing metadata about the target data. Use get_column_count() or get_row_count() to get the number of columns or rows in a table. The get_action_metadata() function is useful when building custom actions since it returns metadata about the validation step that’s triggering the action. Lastly, the config() utility lets us set global configuration parameters.\n\n\n\nget_column_count\nGet the number of columns in a table.\n\n\nget_row_count\nGet the number of rows in a table.\n\n\nget_action_metadata\nAccess step-level metadata when authoring custom actions.\n\n\nget_validation_summary\nAccess validation summary information when authoring final actions.\n\n\nconfig\nConfiguration settings for the Pointblank library."
  },
  {
    "objectID": "reference/index.html#prebuilt-actions",
    "href": "reference/index.html#prebuilt-actions",
    "title": "API Reference",
    "section": "",
    "text": "The Prebuilt Actions group contains a function that can be used to send a Slack notification when validation steps exceed failure threshold levels or just to provide a summary of the validation results, including the status, number of steps, passing and failing steps, table information, and timing details.\n\n\n\nsend_slack_notification\nCreate a Slack notification function using a webhook URL."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html",
    "href": "reference/Validate.col_vals_not_in_set.html",
    "title": "Validate.col_vals_not_in_set",
    "section": "",
    "text": "Validate.col_vals_not_in_set(\n    columns,\n    set,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values are not in a set of values.\nThe col_vals_not_in_set() validation method checks whether column values in a table are not part of a specified set= of values. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#parameters",
    "href": "reference/Validate.col_vals_not_in_set.html#parameters",
    "title": "Validate.col_vals_not_in_set",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nset : list[float | int]\n\nA list of values to compare against.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#returns",
    "href": "reference/Validate.col_vals_not_in_set.html#returns",
    "title": "Validate.col_vals_not_in_set",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#preprocessing",
    "href": "reference/Validate.col_vals_not_in_set.html#preprocessing",
    "title": "Validate.col_vals_not_in_set",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#segmentation",
    "href": "reference/Validate.col_vals_not_in_set.html#segmentation",
    "title": "Validate.col_vals_not_in_set",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#thresholds",
    "href": "reference/Validate.col_vals_not_in_set.html#thresholds",
    "title": "Validate.col_vals_not_in_set",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#examples",
    "href": "reference/Validate.col_vals_not_in_set.html#examples",
    "title": "Validate.col_vals_not_in_set",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 8, 1, 9, 1, 7],\n        \"b\": [1, 8, 2, 6, 9, 1],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    7\n    1\n  \n  \n    2\n    8\n    8\n  \n  \n    3\n    1\n    2\n  \n  \n    4\n    9\n    6\n  \n  \n    5\n    1\n    9\n  \n  \n    6\n    7\n    1\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that none of the values in column a are in the set of [2, 3, 4, 5, 6]. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_in_set(columns=\"a\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    a\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_not_in_set(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_in_set(columns=\"b\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    b\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the column b values of 2 and 6, both of which are in the set of [2, 3, 4, 5, 6]."
  },
  {
    "objectID": "reference/ends_with.html",
    "href": "reference/ends_with.html",
    "title": "ends_with",
    "section": "",
    "text": "ends_with(text, case_sensitive=False)\nSelect columns that end with specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The ends_with() selector function can be used to select one or more columns that end with some specified text. So if the set of table columns consists of\n[first_name, last_name, age, address]\nand you want to validate columns that end with \"name\", you can use columns=ends_with(\"name\"). This will select the first_name and last_name columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using ends_with() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/ends_with.html#parameters",
    "href": "reference/ends_with.html#parameters",
    "title": "ends_with",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should end with.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/ends_with.html#returns",
    "href": "reference/ends_with.html#returns",
    "title": "ends_with",
    "section": "Returns",
    "text": "Returns\n\n : EndsWith\n\nAn EndsWith object, which can be used to select columns that end with the specified text."
  },
  {
    "objectID": "reference/ends_with.html#relevant-validation-methods-where-ends_with-can-be-used",
    "href": "reference/ends_with.html#relevant-validation-methods-where-ends_with-can-be-used",
    "title": "ends_with",
    "section": "Relevant Validation Methods where ends_with() can be Used",
    "text": "Relevant Validation Methods where ends_with() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe ends_with() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/ends_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/ends_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "ends_with",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe ends_with() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that end with \"e\" and start with \"a\", you can use the ends_with() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(ends_with(\"e\") & starts_with(\"a\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/ends_with.html#examples",
    "href": "reference/ends_with.html#examples",
    "title": "ends_with",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, 2021_pay, 2022_pay, and person_id and we’d like to validate that the values in columns that end with \"pay\" are greater than 10. We can use the ends_with() column selector function to specify the columns that end with \"pay\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2021_pay\": [16.32, 16.25, 15.75],\n        \"2022_pay\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.ends_with(\"pay\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2021_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2022_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2021_pay and one for 2022_pay. The values in both columns were all greater than 10.\nWe can also use the ends_with() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that end with \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay\": [18.62, 16.95, 18.25],\n        \"2023_pay\": [19.29, 17.75, 18.35],\n        \"2024_pay\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.ends_with(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2023_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2024_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay and one for 2024_pay."
  },
  {
    "objectID": "reference/Validate.assert_below_threshold.html",
    "href": "reference/Validate.assert_below_threshold.html",
    "title": "Validate.assert_below_threshold",
    "section": "",
    "text": "Validate.assert_below_threshold(level='warning', i=None, message=None)\nRaise an AssertionError if validation steps exceed a specified threshold level.\nThe assert_below_threshold() method checks whether validation steps’ failure rates are below a given threshold level (\"warning\", \"error\", or \"critical\"). This is particularly useful in automated testing environments where you want to ensure your data quality meets minimum standards before proceeding.\nIf any validation step exceeds the specified threshold level, an AssertionError will be raised with details about which steps failed. If the validation has not yet been interrogated, this method will automatically call interrogate() with default parameters."
  },
  {
    "objectID": "reference/Validate.assert_below_threshold.html#parameters",
    "href": "reference/Validate.assert_below_threshold.html#parameters",
    "title": "Validate.assert_below_threshold",
    "section": "Parameters",
    "text": "Parameters\n\nlevel : str = 'warning'\n\nThe threshold level to check against, which could be any of \"warning\" (the default), \"error\", or \"critical\". An AssertionError will be raised if any validation step exceeds this level.\n\ni : int | None = None\n\nSpecific validation step number(s) to check. Can be provided as a single integer or a list of integers. If None (the default), all steps are checked.\n\nmessage : str | None = None\n\nCustom error message to use if assertion fails. If None, a default message will be generated that lists the specific steps that exceeded the threshold."
  },
  {
    "objectID": "reference/Validate.assert_below_threshold.html#returns",
    "href": "reference/Validate.assert_below_threshold.html#returns",
    "title": "Validate.assert_below_threshold",
    "section": "Returns",
    "text": "Returns\n\n : None"
  },
  {
    "objectID": "reference/Validate.assert_below_threshold.html#raises",
    "href": "reference/Validate.assert_below_threshold.html#raises",
    "title": "Validate.assert_below_threshold",
    "section": "Raises",
    "text": "Raises\n\n: AssertionError\n\nIf any specified validation step exceeds the given threshold level.\n\n: ValueError\n\nIf an invalid threshold level is provided."
  },
  {
    "objectID": "reference/Validate.assert_below_threshold.html#examples",
    "href": "reference/Validate.assert_below_threshold.html#examples",
    "title": "Validate.assert_below_threshold",
    "section": "Examples",
    "text": "Examples\nBelow are some examples of how to use the assert_below_threshold() method. First, we’ll create a simple Polars DataFrame with two columns (a and b).\n\nimport polars as pl\n\ntbl = pl.DataFrame({\n    \"a\": [7, 4, 9, 7, 12],\n    \"b\": [9, 8, 10, 5, 10]\n})\n\nThen a validation plan will be created with thresholds (warning=0.1, error=0.2, critical=0.3). After interrogating, we display the validation report table:\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(0.1, 0.2, 0.3))\n    .col_vals_gt(columns=\"a\", value=5)   # 1 failing test unit\n    .col_vals_lt(columns=\"b\", value=10)  # 2 failing test units\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    40.80\n    10.20\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    30.60\n    20.40\n    ●\n    ●\n    ●\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nUsing assert_below_threshold(level=\"warning\") will raise an AssertionError if any step exceeds the ‘warning’ threshold:\n\ntry:\n    validation.assert_below_threshold(level=\"warning\")\nexcept AssertionError as e:\n    print(f\"Assertion failed: {e}\")\n\nAssertion failed: The following steps exceeded the warning threshold level:\nStep 1: Expect that values in `a` should be &gt; `5`.\nStep 2: Expect that values in `b` should be &lt; `10`.\n\n\nCheck a specific step against the ‘critical’ threshold using the i= parameter:\n\nvalidation.assert_below_threshold(level=\"critical\", i=1)  # Won't raise an error\n\nAs the first step is below the ‘critical’ threshold (it exceeds the ‘warning’ and ‘error’ thresholds), no error is raised and nothing is printed.\nWe can also provide a custom error message with the message= parameter. Let’s try that here:\n\ntry:\n    validation.assert_below_threshold(\n        level=\"error\",\n        message=\"Data quality too low for processing!\"\n    )\nexcept AssertionError as e:\n    print(f\"Custom error: {e}\")\n\nCustom error: Data quality too low for processing!"
  },
  {
    "objectID": "reference/Validate.assert_below_threshold.html#see-also",
    "href": "reference/Validate.assert_below_threshold.html#see-also",
    "title": "Validate.assert_below_threshold",
    "section": "See Also",
    "text": "See Also\n\nwarning(): get the ‘warning’ status for each validation step\nerror(): get the ‘error’ status for each validation step\ncritical(): get the ‘critical’ status for each validation step\nassert_passing(): assert all validations pass completely"
  },
  {
    "objectID": "reference/Validate.n_failed.html",
    "href": "reference/Validate.n_failed.html",
    "title": "Validate.n_failed",
    "section": "",
    "text": "Validate.n_failed(i=None, scalar=False)\nProvides a dictionary of the number of test units that failed for each validation step.\nThe n_failed() method provides the number of test units that failed for each validation step. This is the number of test units that did not pass in the the validation step. It is always some integer value between 0 and the total number of test units.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of failing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the n_passed() method (i.e., n - n_passed)."
  },
  {
    "objectID": "reference/Validate.n_failed.html#parameters",
    "href": "reference/Validate.n_failed.html#parameters",
    "title": "Validate.n_failed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of failing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n_failed.html#returns",
    "href": "reference/Validate.n_failed.html#returns",
    "title": "Validate.n_failed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of failing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n_failed.html#examples",
    "href": "reference/Validate.n_failed.html#examples",
    "title": "Validate.n_failed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps and, as it turns out, all of them will have failing test units. After interrogation, the n_failed() method is used to determine the number of failing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12],\n        \"b\": [9, 8, 10, 5, 10],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.n_failed()\n\n{1: 1, 2: 2, 3: 1}\n\n\nThe returned dictionary shows that all validation steps had failing test units.\nIf we wanted to check the number of failing test units for a single validation step, we can provide the step number. Also, we could forego the dictionary and get a scalar value by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.n_failed(i=1)\n\n{1: 1}\n\n\nThe returned value of 1 is the number of failing test units for the first validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html",
    "href": "reference/Validate.col_vals_le.html",
    "title": "Validate.col_vals_le",
    "section": "",
    "text": "Validate.col_vals_le(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data less than or equal to a fixed value or data in another column?\nThe col_vals_le() validation method checks whether column values in a table are less than or equal to a specified value= (the exact comparison used in this function is col_val &lt;= value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#parameters",
    "href": "reference/Validate.col_vals_le.html#parameters",
    "title": "Validate.col_vals_le",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#returns",
    "href": "reference/Validate.col_vals_le.html#returns",
    "title": "Validate.col_vals_le",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_le.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_le",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#preprocessing",
    "href": "reference/Validate.col_vals_le.html#preprocessing",
    "title": "Validate.col_vals_le",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#segmentation",
    "href": "reference/Validate.col_vals_le.html#segmentation",
    "title": "Validate.col_vals_le",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#thresholds",
    "href": "reference/Validate.col_vals_le.html#thresholds",
    "title": "Validate.col_vals_le",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#examples",
    "href": "reference/Validate.col_vals_le.html#examples",
    "title": "Validate.col_vals_le",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [1, 3, 1, 5, 2, 5],\n        \"c\": [2, 1, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    3\n    1\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    5\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    5\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all less than or equal to the value of 9. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_le(columns=\"a\", value=9)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    a\n    9\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_le(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_le() to check whether the values in column c are less than values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_le(columns=\"c\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 0: c is 2 and b is 1.\nRow 4: c is 3 and b is 2."
  },
  {
    "objectID": "reference/assistant.html",
    "href": "reference/assistant.html",
    "title": "assistant",
    "section": "",
    "text": "assistant(model, data=None, tbl_name=None, api_key=None, display=None)\nChat with the PbA (Pointblank Assistant) about your data validation needs.\nThe assistant() function provides an interactive chat session with the PbA (Pointblank Assistant) to help you with your data validation needs. The PbA can help you with constructing validation plans, suggesting validation methods, and providing code snippets for using the Pointblank Python package. Feel free to ask the PbA about any aspect of the Pointblank package and it will do its best to assist you.\nThe PbA can also help you with constructing validation plans for your data tables. If you provide a data table to the PbA, it will internally generate a JSON summary of the table and use that information to suggest validation methods that can be used with the Pointblank package. If using a Polars table as the data source, the PbA will be knowledgeable about the Polars API and can smartly suggest validation steps that use aggregate measures with up-to-date Polars methods.\nThe PbA can be used with models from the following providers:\n\nAnthropic\nOpenAI\nOllama\nAmazon Bedrock\n\nThe PbA can be displayed in a browser (the default) or in the terminal. You can choose one or the other by setting the display= parameter to \"browser\" or \"terminal\".\n\n\n\n\n\n\nWarning\n\n\n\nThe assistant() function is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\ndata : FrameT | Any | None = None\n\nAn optional data table to focus on during discussion with the PbA, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str = None\n\nThe name of the data table. This is optional and is only used to provide a more detailed prompt to the PbA.\n\napi_key : str = None\n\nThe API key to be used for the model.\n\ndisplay : str = None\n\nThe display mode to use for the chat session. Supported values are \"browser\" and \"terminal\". If not provided, the default value is \"browser\".\n\n\n\n\n\n\n : None\n\nNothing is returned. Rather, you get an an interactive chat session with the PbA, which is displayed in a browser or in the terminal.\n\n\n\n\n\nThe model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names.\n\n\n\nProviding a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way.\n\n\n\nIf data= is provided then that data is sent to the model provider is a JSON summary of the table. This data summary is generated internally by use of the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information be knowledgable about the data table. Compared to the size of the entire table, the JSON summary is quite small and can be safely sent to the model provider.\nThe Amazon Bedrock provider is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally.\n\n\n\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using preview() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/assistant.html#parameters",
    "href": "reference/assistant.html#parameters",
    "title": "assistant",
    "section": "",
    "text": "model : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\ndata : FrameT | Any | None = None\n\nAn optional data table to focus on during discussion with the PbA, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str = None\n\nThe name of the data table. This is optional and is only used to provide a more detailed prompt to the PbA.\n\napi_key : str = None\n\nThe API key to be used for the model.\n\ndisplay : str = None\n\nThe display mode to use for the chat session. Supported values are \"browser\" and \"terminal\". If not provided, the default value is \"browser\"."
  },
  {
    "objectID": "reference/assistant.html#returns",
    "href": "reference/assistant.html#returns",
    "title": "assistant",
    "section": "",
    "text": ": None\n\nNothing is returned. Rather, you get an an interactive chat session with the PbA, which is displayed in a browser or in the terminal."
  },
  {
    "objectID": "reference/assistant.html#constructing-the-model-argument",
    "href": "reference/assistant.html#constructing-the-model-argument",
    "title": "assistant",
    "section": "",
    "text": "The model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names."
  },
  {
    "objectID": "reference/assistant.html#notes-on-authentication",
    "href": "reference/assistant.html#notes-on-authentication",
    "title": "assistant",
    "section": "",
    "text": "Providing a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way."
  },
  {
    "objectID": "reference/assistant.html#notes-on-data-sent-to-the-model-provider",
    "href": "reference/assistant.html#notes-on-data-sent-to-the-model-provider",
    "title": "assistant",
    "section": "",
    "text": "If data= is provided then that data is sent to the model provider is a JSON summary of the table. This data summary is generated internally by use of the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information be knowledgable about the data table. Compared to the size of the entire table, the JSON summary is quite small and can be safely sent to the model provider.\nThe Amazon Bedrock provider is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally."
  },
  {
    "objectID": "reference/assistant.html#supported-input-table-types",
    "href": "reference/assistant.html#supported-input-table-types",
    "title": "assistant",
    "section": "",
    "text": "The data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using preview() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html",
    "href": "reference/Validate.col_schema_match.html",
    "title": "Validate.col_schema_match",
    "section": "",
    "text": "Validate.col_schema_match(\n    schema,\n    complete=True,\n    in_order=True,\n    case_sensitive_colnames=True,\n    case_sensitive_dtypes=True,\n    full_match_dtypes=True,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo columns in the table (and their types) match a predefined schema?\nThe col_schema_match() method works in conjunction with an object generated by the Schema class. That class object is the expectation for the actual schema of the target table. The validation step operates over a single test unit, which is whether the schema matches that of the table (within the constraints enforced by the complete=, and in_order= options)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#parameters",
    "href": "reference/Validate.col_schema_match.html#parameters",
    "title": "Validate.col_schema_match",
    "section": "Parameters",
    "text": "Parameters\n\nschema : Schema\n\nA Schema object that represents the expected schema of the table. This object is generated by the Schema class.\n\ncomplete : bool = True\n\nShould the schema match be complete? If True, then the target table must have all columns specified in the schema. If False, then the table can have additional columns not in the schema (i.e., the schema is a subset of the target table’s columns).\n\nin_order : bool = True\n\nShould the schema match be in order? If True, then the columns in the schema must appear in the same order as they do in the target table. If False, then the order of columns in the schema and the target table can differ.\n\ncase_sensitive_colnames : bool = True\n\nShould the schema match be case-sensitive with regard to column names? If True, then the column names in the schema and the target table must match exactly. If False, then the column names are compared in a case-insensitive manner.\n\ncase_sensitive_dtypes : bool = True\n\nShould the schema match be case-sensitive with regard to column data types? If True, then the column data types in the schema and the target table must match exactly. If False, then the column data types are compared in a case-insensitive manner.\n\nfull_match_dtypes : bool = True\n\nShould the schema match require a full match of data types? If True, then the column data types in the schema and the target table must match exactly. If False then substring matches are allowed, so a schema data type of Int would match a target table data type of Int64.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#returns",
    "href": "reference/Validate.col_schema_match.html#returns",
    "title": "Validate.col_schema_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#preprocessing",
    "href": "reference/Validate.col_schema_match.html#preprocessing",
    "title": "Validate.col_schema_match",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#thresholds",
    "href": "reference/Validate.col_schema_match.html#thresholds",
    "title": "Validate.col_schema_match",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#examples",
    "href": "reference/Validate.col_schema_match.html#examples",
    "title": "Validate.col_schema_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three columns (string, integer, and float). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n        \"c\": [1.1, 2.2, 3.3, 4.4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bInt64\n  cFloat64\n\n\n\n  \n    1\n    apple\n    1\n    1.1\n  \n  \n    2\n    banana\n    6\n    2.2\n  \n  \n    3\n    cherry\n    3\n    3.3\n  \n  \n    4\n    date\n    5\n    4.4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the columns in the table match a predefined schema. A schema can be defined using the Schema class.\n\nschema = pb.Schema(\n    columns=[(\"a\", \"String\"), (\"b\", \"Int64\"), (\"c\", \"Float64\")]\n)\n\nYou can print the schema object to verify that the expected schema is as intended.\n\nprint(schema)\n\nPointblank Schema\n  a: String\n  b: Int64\n  c: Float64\n\n\nNow, we’ll use the col_schema_match() method to validate the table against the expected schema object. There is a single test unit for this validation step (whether the schema matches the table or not).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the schema matches the table. The single test unit passed since the table columns and their types match the schema."
  },
  {
    "objectID": "reference/Validate.rows_complete.html",
    "href": "reference/Validate.rows_complete.html",
    "title": "Validate.rows_complete",
    "section": "",
    "text": "Validate.rows_complete(\n    columns_subset=None,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether row data are complete by having no missing values.\nThe rows_complete() method checks whether rows in the table are complete. Completeness of a row means that there are no missing values within the row. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied). A subset of columns can be specified for the completeness check. If no subset is provided, all columns in the table will be used."
  },
  {
    "objectID": "reference/Validate.rows_complete.html#parameters",
    "href": "reference/Validate.rows_complete.html#parameters",
    "title": "Validate.rows_complete",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns_subset : str | list[str] | None = None\n\nA single column or a list of columns to use as a subset for the completeness check. If None (the default), then all columns in the table will be used.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.rows_complete.html#returns",
    "href": "reference/Validate.rows_complete.html#returns",
    "title": "Validate.rows_complete",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.rows_complete.html#preprocessing",
    "href": "reference/Validate.rows_complete.html#preprocessing",
    "title": "Validate.rows_complete",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns_subset= that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.rows_complete.html#segmentation",
    "href": "reference/Validate.rows_complete.html#segmentation",
    "title": "Validate.rows_complete",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.rows_complete.html#thresholds",
    "href": "reference/Validate.rows_complete.html#thresholds",
    "title": "Validate.rows_complete",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.rows_complete.html#examples",
    "href": "reference/Validate.rows_complete.html#examples",
    "title": "Validate.rows_complete",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three string columns (col_1, col_2, and col_3). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"col_1\": [\"a\", None, \"c\", \"d\"],\n        \"col_2\": [\"a\", \"a\", \"c\", None],\n        \"col_3\": [\"a\", \"a\", \"d\", None],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  col_1String\n  col_2String\n  col_3String\n\n\n\n  \n    1\n    a\n    a\n    a\n  \n  \n    2\n    None\n    a\n    a\n  \n  \n    3\n    c\n    c\n    d\n  \n  \n    4\n    d\n    None\n    None\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the rows in the table are complete with rows_complete(). We’ll determine if this validation had any failing test units (there are four test units, one for each row). A failing test units means that a given row is not complete (i.e., has at least one missing value).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_complete()\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_complete\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            rows_complete()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nFrom this validation table we see that there are two failing test units. This is because two rows in the table have at least one missing value (the second row and the last row).\nWe can also use a subset of columns to determine completeness. Let’s specify the subset using columns col_2 and col_3 for the next validation.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_complete(columns_subset=[\"col_2\", \"col_3\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_complete\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            rows_complete()\n        \n        \n        \n    col_2, col_3\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports a single failing test units. The last row contains missing values in both the col_2 and col_3 columns. others."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html",
    "href": "reference/Validate.col_vals_expr.html",
    "title": "Validate.col_vals_expr",
    "section": "",
    "text": "Validate.col_vals_expr(\n    expr,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate column values using a custom expression.\nThe col_vals_expr() validation method checks whether column values in a table satisfy a custom expr= expression. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#parameters",
    "href": "reference/Validate.col_vals_expr.html#parameters",
    "title": "Validate.col_vals_expr",
    "section": "Parameters",
    "text": "Parameters\n\nexpr : any\n\nA column expression that will evaluate each row in the table, returning a boolean value per table row. If the target table is a Polars DataFrame, the expression should either be a Polars column expression or a Narwhals one. For a Pandas DataFrame, the expression should either be a lambda expression or a Narwhals column expression.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#returns",
    "href": "reference/Validate.col_vals_expr.html#returns",
    "title": "Validate.col_vals_expr",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#preprocessing",
    "href": "reference/Validate.col_vals_expr.html#preprocessing",
    "title": "Validate.col_vals_expr",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#segmentation",
    "href": "reference/Validate.col_vals_expr.html#segmentation",
    "title": "Validate.col_vals_expr",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#thresholds",
    "href": "reference/Validate.col_vals_expr.html#thresholds",
    "title": "Validate.col_vals_expr",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#examples",
    "href": "reference/Validate.col_vals_expr.html#examples",
    "title": "Validate.col_vals_expr",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 1, 7, 8, 6],\n        \"b\": [0, 0, 0, 1, 1, 1],\n        \"c\": [0.5, 0.3, 0.8, 1.4, 1.9, 1.2],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cFloat64\n\n\n\n  \n    1\n    1\n    0\n    0.5\n  \n  \n    2\n    2\n    0\n    0.3\n  \n  \n    3\n    1\n    0\n    0.8\n  \n  \n    4\n    7\n    1\n    1.4\n  \n  \n    5\n    8\n    1\n    1.9\n  \n  \n    6\n    6\n    1\n    1.2\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the values in column a are all integers. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_expr(expr=pl.col(\"a\") % 1 == 0)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        \n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_expr(). All test units passed, with no failing test units."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html",
    "href": "reference/Validate.get_sundered_data.html",
    "title": "Validate.get_sundered_data",
    "section": "",
    "text": "Validate.get_sundered_data(type='pass')\nGet the data that passed or failed the validation steps.\nValidation of the data is one thing but, sometimes, you want to use the best part of the input dataset for something else. The get_sundered_data() method works with a Validate object that has been interrogated (i.e., the interrogate() method was used). We can get either the ‘pass’ data piece (rows with no failing test units across all row-based validation functions), or, the ‘fail’ data piece (rows with at least one failing test unit across the same series of validations)."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#details",
    "href": "reference/Validate.get_sundered_data.html#details",
    "title": "Validate.get_sundered_data",
    "section": "Details",
    "text": "Details\nThere are some caveats to sundering. The validation steps considered for this splitting will only involve steps where:\n\nof certain check types, where test units are cells checked row-by-row (e.g., the col_vals_*() methods)\nactive= is not set to False\npre= has not been given an expression for modifying the input table\n\nSo long as these conditions are met, the data will be split into two constituent tables: one with the rows that passed all validation steps and another with the rows that failed at least one validation step."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#parameters",
    "href": "reference/Validate.get_sundered_data.html#parameters",
    "title": "Validate.get_sundered_data",
    "section": "Parameters",
    "text": "Parameters\n\ntype :  = 'pass'\n\nThe type of data to return. Options are \"pass\" or \"fail\", where the former returns a table only containing rows where test units always passed validation steps, and the latter returns a table only containing rows had test units that failed in at least one validation step."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#returns",
    "href": "reference/Validate.get_sundered_data.html#returns",
    "title": "Validate.get_sundered_data",
    "section": "Returns",
    "text": "Returns\n\n : FrameT\n\nA table containing the data that passed or failed the validation steps."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#examples",
    "href": "reference/Validate.get_sundered_data.html#examples",
    "title": "Validate.get_sundered_data",
    "section": "Examples",
    "text": "Examples\nLet’s create a Validate object with three validation steps and then interrogate the data.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 6, 9, 7, 3, 2],\n        \"b\": [9, 8, 10, 5, 10, 6],\n        \"c\": [\"c\", \"d\", \"a\", \"b\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:17:36Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-05-23 02:17:36 UTC&lt; 1 s2025-05-23 02:17:36 UTC\n  \n\n\n\n\n\n\n        \n\n\nFrom the validation table, we can see that the first and second steps each had 4 passing test units. A failing test unit will mark the entire row as failing in the context of the get_sundered_data() method. We can use this method to get the rows of data that passed the during interrogation.\n\npb.preview(validation.get_sundered_data())\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cString\n\n\n\n  \n    1\n    9\n    10\n    a\n  \n  \n    2\n    7\n    5\n    b\n  \n\n\n\n\n\n\n        \n\n\nThe returned DataFrame contains the rows that passed all validation steps (we passed this object to preview() to show it in an HTML view). From the six-row input DataFrame, the first two rows and the last two rows had test units that failed validation. Thus the middle two rows are the only ones that passed all validation steps and that’s what we see in the returned DataFrame."
  },
  {
    "objectID": "reference/get_row_count.html",
    "href": "reference/get_row_count.html",
    "title": "get_row_count",
    "section": "",
    "text": "get_row_count(data)\nGet the number of rows in a table.\nThe get_row_count() function returns the number of rows in a table. The function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.)."
  },
  {
    "objectID": "reference/get_row_count.html#parameters",
    "href": "reference/get_row_count.html#parameters",
    "title": "get_row_count",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to get the row count, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/get_row_count.html#returns",
    "href": "reference/get_row_count.html#returns",
    "title": "get_row_count",
    "section": "Returns",
    "text": "Returns\n\n : int\n\nThe number of rows in the table."
  },
  {
    "objectID": "reference/get_row_count.html#supported-input-table-types",
    "href": "reference/get_row_count.html#supported-input-table-types",
    "title": "get_row_count",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nMicrosoft SQL Server table (\"mssql\")*\nSnowflake table (\"snowflake\")*\nDatabricks table (\"databricks\")*\nPySpark table (\"pyspark\")*\nBigQuery table (\"bigquery\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using get_row_count() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/get_row_count.html#examples",
    "href": "reference/get_row_count.html#examples",
    "title": "get_row_count",
    "section": "Examples",
    "text": "Examples\nGetting the number of rows in a table is easily done by using the get_row_count() function. Here’s an example using the game_revenue dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\ngame_revenue_polars = pb.load_dataset(\"game_revenue\")\n\npb.get_row_count(game_revenue_polars)\n\n2000\n\n\nThis table is a Polars DataFrame, but the get_row_count() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\ngame_revenue_duckdb = pb.load_dataset(\"game_revenue\", tbl_type=\"duckdb\")\n\npb.get_row_count(game_revenue_duckdb)\n\n2000\n\n\nThe function always returns the number of rows in the table as an integer value, which is 2000 for the game_revenue dataset."
  },
  {
    "objectID": "reference/Validate.f_passed.html",
    "href": "reference/Validate.f_passed.html",
    "title": "Validate.f_passed",
    "section": "",
    "text": "Validate.f_passed(i=None, scalar=False)\nProvides a dictionary of the fraction of test units that passed for each validation step.\nA measure of the fraction of test units that passed is provided by the f_passed attribute. This is the fraction of test units that passed the validation step over the total number of test units. Given this is a fractional value, it will always be in the range of 0 to 1.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThis method provides a dictionary of the fraction of passing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the f_failed() method (i.e., 1 - f_failed())."
  },
  {
    "objectID": "reference/Validate.f_passed.html#parameters",
    "href": "reference/Validate.f_passed.html#parameters",
    "title": "Validate.f_passed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the fraction of passing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.f_passed.html#returns",
    "href": "reference/Validate.f_passed.html#returns",
    "title": "Validate.f_passed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, float] | float\n\nA dictionary of the fraction of passing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.f_passed.html#examples",
    "href": "reference/Validate.f_passed.html#examples",
    "title": "Validate.f_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, all having some failing test units. After interrogation, the f_passed() method is used to determine the fraction of passing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\", \"d\", \"c\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.f_passed()\n\n{1: 0.7142857142857143, 2: 0.5714285714285714, 3: 0.5714285714285714}\n\n\nThe returned dictionary shows the fraction of passing test units for each validation step. The values are all less than 1 since there were failing test units in each step.\nIf we wanted to check the fraction of passing test units for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.f_passed(i=1)\n\n{1: 0.7142857142857143}\n\n\nThe returned value is the proportion of passing test units for the first validation step (5 passing test units out of 7 total test units)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html",
    "href": "reference/Validate.col_vals_not_null.html",
    "title": "Validate.col_vals_not_null",
    "section": "",
    "text": "Validate.col_vals_not_null(\n    columns,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether values in a column are not NULL.\nThe col_vals_not_null() validation method checks whether column values in a table are not NULL. This validation will operate over the number of test units that is equal to the number of rows in the table."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#parameters",
    "href": "reference/Validate.col_vals_not_null.html#parameters",
    "title": "Validate.col_vals_not_null",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#returns",
    "href": "reference/Validate.col_vals_not_null.html#returns",
    "title": "Validate.col_vals_not_null",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#preprocessing",
    "href": "reference/Validate.col_vals_not_null.html#preprocessing",
    "title": "Validate.col_vals_not_null",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#segmentation",
    "href": "reference/Validate.col_vals_not_null.html#segmentation",
    "title": "Validate.col_vals_not_null",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#thresholds",
    "href": "reference/Validate.col_vals_not_null.html#thresholds",
    "title": "Validate.col_vals_not_null",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#examples",
    "href": "reference/Validate.col_vals_not_null.html#examples",
    "title": "Validate.col_vals_not_null",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [4, 7, 2, 8],\n        \"b\": [5, None, 1, None],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    4\n    5\n  \n  \n    2\n    7\n    None\n  \n  \n    3\n    2\n    1\n  \n  \n    4\n    8\n    None\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that none of the values in column a are Null values. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_null(columns=\"a\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_not_null(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_null(columns=\"b\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the two Null values in column b."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html",
    "href": "reference/Validate.col_vals_ge.html",
    "title": "Validate.col_vals_ge",
    "section": "",
    "text": "Validate.col_vals_ge(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    segments=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data greater than or equal to a fixed value or data in another column?\nThe col_vals_ge() validation method checks whether column values in a table are greater than or equal to a specified value= (the exact comparison used in this function is col_val &gt;= value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#parameters",
    "href": "reference/Validate.col_vals_ge.html#parameters",
    "title": "Validate.col_vals_ge",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nsegments : SegmentSpec | None = None\n\nAn optional directive on segmentation, which serves to split a validation step into multiple (one step per segment). Can be a single column name, a tuple that specifies a column name and its corresponding values to segment on, or a combination of both (provided as a list). Read the Segmentation section for usage information.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#returns",
    "href": "reference/Validate.col_vals_ge.html#returns",
    "title": "Validate.col_vals_ge",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_ge.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_ge",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#preprocessing",
    "href": "reference/Validate.col_vals_ge.html#preprocessing",
    "title": "Validate.col_vals_ge",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#segmentation",
    "href": "reference/Validate.col_vals_ge.html#segmentation",
    "title": "Validate.col_vals_ge",
    "section": "Segmentation",
    "text": "Segmentation\nThe segments= argument allows for the segmentation of a validation step into multiple segments. This is useful for applying the same validation step to different subsets of the data. The segmentation can be done based on a single column or specific fields within a column.\nProviding a single column name will result in a separate validation step for each unique value in that column. For example, if you have a column called \"region\" with values \"North\", \"South\", and \"East\", the validation step will be applied separately to each region.\nAlternatively, you can provide a tuple that specifies a column name and its corresponding values to segment on. For example, if you have a column called \"date\" and you want to segment on only specific dates, you can provide a tuple like (\"date\", [\"2023-01-01\", \"2023-01-02\"]). Any other values in the column will be disregarded (i.e., no validation steps will be created for them).\nA list with a combination of column names and tuples can be provided as well. This allows for more complex segmentation scenarios. The following inputs are all valid:\n\nsegments=[\"region\", (\"date\", [\"2023-01-01\", \"2023-01-02\"])]: segments on unique values in the \"region\" column and specific dates in the \"date\" column\nsegments=[\"region\", \"date\"]: segments on unique values in the \"region\" and \"date\" columns\n\nThe segmentation is performed during interrogation, and the resulting validation steps will be numbered sequentially. Each segment will have its own validation step, and the results will be reported separately. This allows for a more granular analysis of the data and helps identify issues within specific segments.\nImportantly, the segmentation process will be performed after any preprocessing of the data table. Because of this, one can conceivably use the pre= argument to generate a column that can be used for segmentation. For example, you could create a new column called \"segment\" through use of pre= and then use that column for segmentation."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#thresholds",
    "href": "reference/Validate.col_vals_ge.html#thresholds",
    "title": "Validate.col_vals_ge",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#examples",
    "href": "reference/Validate.col_vals_ge.html#examples",
    "title": "Validate.col_vals_ge",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [5, 3, 1, 8, 2, 3],\n        \"c\": [2, 3, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    5\n    2\n  \n  \n    2\n    6\n    3\n    3\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    8\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    3\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all greater than or equal to the value of 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=\"a\", value=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_ge(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_ge() to check whether the values in column b are greater than values in column c.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=\"b\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    b\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 0: b is 2 and c is 3.\nRow 4: b is 3 and c is 4."
  },
  {
    "objectID": "reference/Validate.col_count_match.html",
    "href": "reference/Validate.col_count_match.html",
    "title": "Validate.col_count_match",
    "section": "",
    "text": "Validate.col_count_match(\n    count,\n    inverse=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether the column count of the table matches a specified count.\nThe col_count_match() method checks whether the column count of the target table matches a specified count. This validation will operate over a single test unit, which is whether the column count matches the specified count.\nWe also have the option to invert the validation step by setting inverse=True. This will make the expectation that column row count of the target table does not match the specified count."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#parameters",
    "href": "reference/Validate.col_count_match.html#parameters",
    "title": "Validate.col_count_match",
    "section": "Parameters",
    "text": "Parameters\n\ncount : int | FrameT | Any\n\nThe expected column count of the table. This can be an integer value, a Polars or Pandas DataFrame object, or an Ibis backend table. If a DataFrame/table is provided, the column count of that object will be used as the expected count.\n\ninverse : bool = False\n\nShould the validation step be inverted? If True, then the expectation is that the column count of the target table should not match the specified count= value.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#returns",
    "href": "reference/Validate.col_count_match.html#returns",
    "title": "Validate.col_count_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#preprocessing",
    "href": "reference/Validate.col_count_match.html#preprocessing",
    "title": "Validate.col_count_match",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#thresholds",
    "href": "reference/Validate.col_count_match.html#thresholds",
    "title": "Validate.col_count_match",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#examples",
    "href": "reference/Validate.col_count_match.html#examples",
    "title": "Validate.col_count_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use the built in dataset \"game_revenue\". The table can be obtained by calling load_dataset(\"game_revenue\").\n\nimport pointblank as pb\n\ngame_revenue = pb.load_dataset(\"game_revenue\")\n\npb.preview(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the number of columns in the table matches a fixed value. In this case, we will use the value 11 as the expected column count.\n\nvalidation = (\n    pb.Validate(data=game_revenue)\n    .col_count_match(count=11)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the expectation value of 11 matches the actual count of columns in the target table. So, the single test unit passed."
  },
  {
    "objectID": "reference/get_action_metadata.html",
    "href": "reference/get_action_metadata.html",
    "title": "get_action_metadata",
    "section": "",
    "text": "get_action_metadata()\nAccess step-level metadata when authoring custom actions.\nGet the metadata for the validation step where an action was triggered. This can be called by user functions to get the metadata for the current action. This function can only be used within callables crafted for the Actions class."
  },
  {
    "objectID": "reference/get_action_metadata.html#returns",
    "href": "reference/get_action_metadata.html#returns",
    "title": "get_action_metadata",
    "section": "Returns",
    "text": "Returns\n\n : dict | None\n\nA dictionary containing the metadata for the current step. If called outside of an action (i.e., when no action is being executed), this function will return None."
  },
  {
    "objectID": "reference/get_action_metadata.html#description-of-the-metadata-fields",
    "href": "reference/get_action_metadata.html#description-of-the-metadata-fields",
    "title": "get_action_metadata",
    "section": "Description of the Metadata Fields",
    "text": "Description of the Metadata Fields\nThe metadata dictionary contains the following fields for a given validation step:\n\nstep: The step number.\ncolumn: The column name.\nvalue: The value being compared (only available in certain validation steps).\ntype: The assertion type (e.g., \"col_vals_gt\", etc.).\ntime: The time the validation step was executed (in ISO format).\nlevel: The severity level (\"warning\", \"error\", or \"critical\").\nlevel_num: The severity level as a numeric value (30, 40, or 50).\nautobrief: A localized and brief statement of the expectation for the step.\nfailure_text: Localized text that explains how the validation step failed."
  },
  {
    "objectID": "reference/get_action_metadata.html#examples",
    "href": "reference/get_action_metadata.html#examples",
    "title": "get_action_metadata",
    "section": "Examples",
    "text": "Examples\nWhen creating a custom action, you can access the metadata for the current step using the get_action_metadata() function. Here’s an example of a custom action that logs the metadata for the current step:\n\nimport pointblank as pb\n\ndef log_issue():\n    metadata = pb.get_action_metadata()\n    print(f\"Type: {metadata['type']}, Step: {metadata['step']}\")\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(warning=log_issue),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n    )\n    .interrogate()\n)\n\nvalidation\n\nType: col_vals_gt, Step: 2\nType: col_vals_gt, Step: 3\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:14DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:18:14 UTC&lt; 1 s2025-05-23 02:18:14 UTC\n  \n\n\n\n\n\n\n        \n\n\nKey pieces to note in the above example:\n\nlog_issue() (the custom action) collects metadata by calling get_action_metadata()\nthe metadata is a dictionary that is used to craft the log message\nthe action is passed as a bare function to the Actions object within the Validate object (placing it within Validate(actions=) ensures it’s set as an action for every validation step)"
  },
  {
    "objectID": "reference/get_action_metadata.html#see-also",
    "href": "reference/get_action_metadata.html#see-also",
    "title": "get_action_metadata",
    "section": "See Also",
    "text": "See Also\nHave a look at Actions for more information on how to create custom actions for validation steps that exceed a set threshold value."
  },
  {
    "objectID": "reference/Validate.get_json_report.html",
    "href": "reference/Validate.get_json_report.html",
    "title": "Validate.get_json_report",
    "section": "",
    "text": "Validate.get_json_report(use_fields=None, exclude_fields=None)\nGet a report of the validation results as a JSON-formatted string.\nThe get_json_report() method provides a machine-readable report of validation results in JSON format. This is particularly useful for programmatic processing, storing validation results, or integrating with other systems. The report includes detailed information about each validation step, such as assertion type, columns validated, threshold values, test results, and more.\nBy default, all available validation information fields are included in the report. However, you can customize the fields to include or exclude using the use_fields= and exclude_fields= parameters."
  },
  {
    "objectID": "reference/Validate.get_json_report.html#parameters",
    "href": "reference/Validate.get_json_report.html#parameters",
    "title": "Validate.get_json_report",
    "section": "Parameters",
    "text": "Parameters\n\nuse_fields : list[str] | None = None\n\nAn optional list of specific fields to include in the report. If provided, only these fields will be included in the JSON output. If None (the default), all standard validation report fields are included. Have a look at the Available Report Fields section below for a list of fields that can be included in the report.\n\nexclude_fields : list[str] | None = None\n\nAn optional list of fields to exclude from the report. If provided, these fields will be omitted from the JSON output. If None (the default), no fields are excluded. This parameter cannot be used together with use_fields=. The Available Report Fields provides a listing of fields that can be excluded from the report."
  },
  {
    "objectID": "reference/Validate.get_json_report.html#returns",
    "href": "reference/Validate.get_json_report.html#returns",
    "title": "Validate.get_json_report",
    "section": "Returns",
    "text": "Returns\n\n : str\n\nA JSON-formatted string representing the validation report, with each validation step as an object in the report array."
  },
  {
    "objectID": "reference/Validate.get_json_report.html#available-report-fields",
    "href": "reference/Validate.get_json_report.html#available-report-fields",
    "title": "Validate.get_json_report",
    "section": "Available Report Fields",
    "text": "Available Report Fields\nThe JSON report can include any of the standard validation report fields, including:\n\ni: the step number (1-indexed)\ni_o: the original step index from the validation plan (pre-expansion)\nassertion_type: the type of validation assertion (e.g., \"col_vals_gt\", etc.)\ncolumn: the column being validated (or columns used in certain validations)\nvalues: the comparison values or parameters used in the validation\ninclusive: whether the comparison is inclusive (for range-based validations)\nna_pass: whether NA/Null values are considered passing (for certain validations)\npre: preprocessing function applied before validation\nsegments: data segments to which the validation was applied\nthresholds: threshold level statement that was used for the validation step\nlabel: custom label for the validation step\nbrief: a brief description of the validation step\nactive: whether the validation step is active\nall_passed: whether all test units passed in the step\nn: total number of test units\nn_passed, n_failed: number of test units that passed and failed\nf_passed, f_failed: Fraction of test units that passed and failed\nwarning, error, critical: whether the namesake threshold level was exceeded (is null if threshold not set)\ntime_processed: when the validation step was processed (ISO 8601 format)\nproc_duration_s: the processing duration in seconds"
  },
  {
    "objectID": "reference/Validate.get_json_report.html#examples",
    "href": "reference/Validate.get_json_report.html#examples",
    "title": "Validate.get_json_report",
    "section": "Examples",
    "text": "Examples\nLet’s create a validation plan with a few validation steps and generate a JSON report of the results:\n\nimport pointblank as pb\nimport polars as pl\n\n# Create a sample DataFrame\ntbl = pl.DataFrame({\n    \"a\": [5, 7, 8, 9],\n    \"b\": [3, 4, 2, 1]\n})\n\n# Create and execute a validation plan\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=6)\n    .col_vals_lt(columns=\"b\", value=4)\n    .interrogate()\n)\n\n# Get the full JSON report\njson_report = validation.get_json_report()\n\nprint(json_report)\n\n[\n    {\n        \"i\": 1,\n        \"i_o\": 1,\n        \"assertion_type\": \"col_vals_gt\",\n        \"column\": \"a\",\n        \"values\": 6,\n        \"inclusive\": null,\n        \"na_pass\": false,\n        \"pre\": null,\n        \"segments\": null,\n        \"thresholds\": \"Thresholds(warning=None, error=None, critical=None)\",\n        \"label\": null,\n        \"brief\": null,\n        \"active\": true,\n        \"all_passed\": false,\n        \"n\": 4,\n        \"n_passed\": 3,\n        \"n_failed\": 1,\n        \"f_passed\": 0.75,\n        \"f_failed\": 0.25,\n        \"warning\": null,\n        \"error\": null,\n        \"critical\": null,\n        \"time_processed\": \"2025-05-23T02:18:23.173+00:00\",\n        \"proc_duration_s\": 0.006807\n    },\n    {\n        \"i\": 2,\n        \"i_o\": 2,\n        \"assertion_type\": \"col_vals_lt\",\n        \"column\": \"b\",\n        \"values\": 4,\n        \"inclusive\": null,\n        \"na_pass\": false,\n        \"pre\": null,\n        \"segments\": null,\n        \"thresholds\": \"Thresholds(warning=None, error=None, critical=None)\",\n        \"label\": null,\n        \"brief\": null,\n        \"active\": true,\n        \"all_passed\": false,\n        \"n\": 4,\n        \"n_passed\": 3,\n        \"n_failed\": 1,\n        \"f_passed\": 0.75,\n        \"f_failed\": 0.25,\n        \"warning\": null,\n        \"error\": null,\n        \"critical\": null,\n        \"time_processed\": \"2025-05-23T02:18:23.176+00:00\",\n        \"proc_duration_s\": 0.002329\n    }\n]\n\n\nYou can also customize which fields to include:\n\njson_report = validation.get_json_report(\n    use_fields=[\"i\", \"assertion_type\", \"column\", \"n_passed\", \"n_failed\"]\n)\n\nprint(json_report)\n\n[\n    {\n        \"i\": 1,\n        \"assertion_type\": \"col_vals_gt\",\n        \"column\": \"a\",\n        \"n_passed\": 3,\n        \"n_failed\": 1\n    },\n    {\n        \"i\": 2,\n        \"assertion_type\": \"col_vals_lt\",\n        \"column\": \"b\",\n        \"n_passed\": 3,\n        \"n_failed\": 1\n    }\n]\n\n\nOr which fields to exclude:\n\njson_report = validation.get_json_report(\n    exclude_fields=[\n        \"i_o\", \"thresholds\", \"pre\", \"segments\", \"values\",\n        \"na_pass\", \"inclusive\", \"label\", \"brief\", \"active\",\n        \"time_processed\", \"proc_duration_s\"\n    ]\n)\n\nprint(json_report)\n\n[\n    {\n        \"i\": 1,\n        \"assertion_type\": \"col_vals_gt\",\n        \"column\": \"a\",\n        \"all_passed\": false,\n        \"n\": 4,\n        \"n_passed\": 3,\n        \"n_failed\": 1,\n        \"f_passed\": 0.75,\n        \"f_failed\": 0.25,\n        \"warning\": null,\n        \"error\": null,\n        \"critical\": null\n    },\n    {\n        \"i\": 2,\n        \"assertion_type\": \"col_vals_lt\",\n        \"column\": \"b\",\n        \"all_passed\": false,\n        \"n\": 4,\n        \"n_passed\": 3,\n        \"n_failed\": 1,\n        \"f_passed\": 0.75,\n        \"f_failed\": 0.25,\n        \"warning\": null,\n        \"error\": null,\n        \"critical\": null\n    }\n]\n\n\nThe JSON output can be further processed or analyzed programmatically:\n\nimport json\n\n# Parse the JSON report\nreport_data = json.loads(validation.get_json_report())\n\n# Extract and analyze validation results\nfailing_steps = [step for step in report_data if step[\"n_failed\"] &gt; 0]\nprint(f\"Number of failing validation steps: {len(failing_steps)}\")\n\nNumber of failing validation steps: 2"
  },
  {
    "objectID": "reference/Validate.get_json_report.html#see-also",
    "href": "reference/Validate.get_json_report.html#see-also",
    "title": "Validate.get_json_report",
    "section": "See Also",
    "text": "See Also\n\nget_tabular_report(): Get a formatted HTML report as a GT table\nget_data_extracts(): Get rows that failed validation"
  },
  {
    "objectID": "reference/Validate.f_failed.html",
    "href": "reference/Validate.f_failed.html",
    "title": "Validate.f_failed",
    "section": "",
    "text": "Validate.f_failed(i=None, scalar=False)\nProvides a dictionary of the fraction of test units that failed for each validation step.\nA measure of the fraction of test units that failed is provided by the f_failed attribute. This is the fraction of test units that failed the validation step over the total number of test units. Given this is a fractional value, it will always be in the range of 0 to 1.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThis method provides a dictionary of the fraction of failing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the f_passed() method (i.e., 1 - f_passed())."
  },
  {
    "objectID": "reference/Validate.f_failed.html#parameters",
    "href": "reference/Validate.f_failed.html#parameters",
    "title": "Validate.f_failed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the fraction of failing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.f_failed.html#returns",
    "href": "reference/Validate.f_failed.html#returns",
    "title": "Validate.f_failed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, float] | float\n\nA dictionary of the fraction of failing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.f_failed.html#examples",
    "href": "reference/Validate.f_failed.html#examples",
    "title": "Validate.f_failed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, all having some failing test units. After interrogation, the f_failed() method is used to determine the fraction of failing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\", \"d\", \"c\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.f_failed()\n\n{1: 0.2857142857142857, 2: 0.42857142857142855, 3: 0.42857142857142855}\n\n\nThe returned dictionary shows the fraction of failing test units for each validation step. The values are all greater than 0 since there were failing test units in each step.\nIf we wanted to check the fraction of failing test units for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.f_failed(i=1)\n\n{1: 0.2857142857142857}\n\n\nThe returned value is the proportion of failing test units for the first validation step (2 failing test units out of 7 total test units)."
  },
  {
    "objectID": "blog/overhauled-user-guide/index.html",
    "href": "blog/overhauled-user-guide/index.html",
    "title": "Overhauling Pointblank’s User Guide",
    "section": "",
    "text": "The Pointblank documentation just got a major upgrade! We’ve completely overhauled our User Guide. Our goal was to enable readers to start fast on validation and work through the many pieces needed in realistic situations.\nWe realized that at the core of Pointblank is the validation plan. Its made up of rules, results, and steps.\nFor example, the first row is a step that checks whether values in column ‘a’ are less than 10. The COLUMNS and VALUES column contain the rules used to configure the step. The PASS column for the first row indicates that all 13 values in the column passed.\nIn this post, we’ll cover:\nLet us walk you through the key improvements in our refreshed User Guide!"
  },
  {
    "objectID": "blog/overhauled-user-guide/index.html#introduction-embracing-the-spiral-sequence",
    "href": "blog/overhauled-user-guide/index.html#introduction-embracing-the-spiral-sequence",
    "title": "Overhauling Pointblank’s User Guide",
    "section": "Introduction: Embracing the Spiral Sequence",
    "text": "Introduction: Embracing the Spiral Sequence\nWe chose to use a spiral sequence for our Introduction and Validation Plan section. The Introduction quickly covers parts of validation plan, while each article of the Validation Plan section dives deeper into different aspects of defining validation rules.\n\n\n\nThe introduction does a broad pass on the validation plan table diagram, identifying the core pieces of the output and then giving a quick overview of the code that produced it. This sets people up for the Validation Plan section in the guide, where each concept is discussed in depth."
  },
  {
    "objectID": "blog/overhauled-user-guide/index.html#improving-examples",
    "href": "blog/overhauled-user-guide/index.html#improving-examples",
    "title": "Overhauling Pointblank’s User Guide",
    "section": "Improving Examples",
    "text": "Improving Examples\nExamples are everywhere in the User Guide. We’ve tightened up our approach to examples by:\n\npresenting example code, output, or both, early in each section\nshowing the actual output you’ll see in your environment\nfollowing up with explanatory text that guides attention to specific places in the output\n\nThis approach makes learning more intuitive. Here’s an excerpt that shows this in practice.\n\nThe blue arrow marks the flow of reading and the red arrows map where we anticipate people will look from the text to the output. Focusing explicitly on where we think attention will go forces us to think carefully about exactly what readers will get from the output. The hope is that readers get to work more quickly on new concepts."
  },
  {
    "objectID": "blog/overhauled-user-guide/index.html#rounding-out-api-coverage",
    "href": "blog/overhauled-user-guide/index.html#rounding-out-api-coverage",
    "title": "Overhauling Pointblank’s User Guide",
    "section": "Rounding Out API Coverage",
    "text": "Rounding Out API Coverage\nDocumentation has to balance jobs between a user guide and an API Reference:\n\nUser Guide: explains concepts that cut across functions (like common arguments across validation methods)\nAPI Reference: explains each individual function\n\nImportantly, user guides often link to the API reference so, as part of this work, we made sure that all individual API entries are well-documented and linked to from the guide. Here’s an excerpt from the User Guide that shows links marked:"
  },
  {
    "objectID": "blog/overhauled-user-guide/index.html#surfacing-advanced-topics",
    "href": "blog/overhauled-user-guide/index.html#surfacing-advanced-topics",
    "title": "Overhauling Pointblank’s User Guide",
    "section": "Surfacing Advanced Topics",
    "text": "Surfacing Advanced Topics\nThere’s a lot of potential slicing and dicing involved in validation, as well as work after validation (post interrogation) to make sense of the results. We added pages to the User Guide for some core situations. In this section, I’ll highlight two advanced topics we added pages for:\n\nsegmentation: splitting a column into groups, and validating each group\nstep reports: view failing cases (e.g., view rows of data that failed validation)\n\nThese are marked in the User Guide sidebar screenshot below:\n\n\n\n\nSegmentation\nHere’s a screenshot of a validation report with two validation steps, one for each segment (\"low\" and \"high\") in the f column of the small_table dataset.\n\nNotice that segments split columns into groups and apply the same validation to each of the groups. Each group is given its own step.\nEach of the 20+ validation methods accept a segment= argument. The value of the Segmentation article in the User Guide is to describe this cross-cutting behavior in a single place.\nCompare the segments= parameter in the API Reference (e.g., look at col_vals_gt()) and the Segmentation article to get a feel for how each location documents the segments feature.\n\n\nStep Report\nStep reports display failing cases (e.g., rows) for a validation step, so you can dig deeper into validation failures. Here’s a screenshot of a step report for some validation step 2:\n\nNotice the arrow pointing to ‘Step 2’ in the title. Failing values are highlighted in red. Once we know we have failures, it’s important to take action and discover why data is failing. Looking at failing cases in step reports often uncovers obvious causes behind failures.\nThe get_step_report() entry is one of 50 in the API Reference. Here it is listed the API Reference, in the Interrogation and Reporting section. Critically, it’s only one of 20 entries in the User Guide, which emphasizes its importance in validation workflows."
  },
  {
    "objectID": "blog/overhauled-user-guide/index.html#looking-forward",
    "href": "blog/overhauled-user-guide/index.html#looking-forward",
    "title": "Overhauling Pointblank’s User Guide",
    "section": "Looking Forward",
    "text": "Looking Forward\nThe refreshed User Guide is just the beginning of our documentation improvements. We’re committed to continuously enhancing our documentation to support your data validation needs.\nMichael Chow gave feedback on this User Guide in preparation for his upcoming talk at SciPy 2025.\nWe’d love to hear your feedback on the new User Guide! Feel free to open an issue on our GitHub repository with suggestions, corrections, or requests for additional topics you’d like to see covered. You can also join our community discussions in the dedicated #Documentation channel on our Discord server, where you can share ideas, ask questions, and get help directly from the Pointblank team and other users."
  },
  {
    "objectID": "blog/all-about-actions/index.html",
    "href": "blog/all-about-actions/index.html",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "",
    "text": "Data validation is only useful if you can respond appropriately when problems arise. That’s why Pointblank’s recent v0.8.0 and v0.8.1 releases have significantly enhanced our action framework, allowing you to create sophisticated, automated responses to validation failures.\nIn this post, we’ll explore how to use:\nLet’s dive into how these features can transform your data validation process from passive reporting to active response."
  },
  {
    "objectID": "blog/all-about-actions/index.html#from-passive-validation-to-active-response",
    "href": "blog/all-about-actions/index.html#from-passive-validation-to-active-response",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "From Passive Validation to Active Response",
    "text": "From Passive Validation to Active Response\nTraditional data validation simply reports problems: “Column X has invalid values.” But what if you want to:\n\nsend a Slack message when critical errors occur?\nlog detailed diagnostics about failing data?\ntrigger automatic data cleaning processes?\ngenerate custom reports for stakeholders?\n\nThis is where Pointblank’s action system can help. By pairing thresholds with actions, you can create automated responses that trigger exactly when needed."
  },
  {
    "objectID": "blog/all-about-actions/index.html#getting-started-with-actions",
    "href": "blog/all-about-actions/index.html#getting-started-with-actions",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Getting Started with Actions",
    "text": "Getting Started with Actions\nActions are executed when validation steps fail to meet certain thresholds. Let’s start with a simple example:\n\nimport pointblank as pb\n\nvalidation_1 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_gt(\n        columns=\"d\",\n        value=1000,\n        thresholds=pb.Thresholds(warning=1, error=5),\n        actions=pb.Actions(\n            warning=\"⚠️ WARNING: Some values in column 'd' are below the minimum threshold!\"\n        )\n    )\n    .interrogate()\n)\n\nvalidation_1\n\n⚠️ WARNING: Some values in column 'd' are below the minimum threshold!\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:38Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    ●\n    ●\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nIn this example:\n\nwe’re validating that values in column “d” are greater than 1000\nwe set a warning threshold of 1 (triggers if any values fail)\nwe define an action that prints a warning message when the threshold is exceeded\n\nSince several values in column d are below 1000, our ‘warning’ action is triggered and the message appears above the validation report."
  },
  {
    "objectID": "blog/all-about-actions/index.html#the-anatomy-of-actions",
    "href": "blog/all-about-actions/index.html#the-anatomy-of-actions",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "The Anatomy of Actions",
    "text": "The Anatomy of Actions\nThe Actions class is a very important piece of Pointblank’s response system. Actions can be defined in several ways:\n\nString messages: simple text output to the console\nCallable functions: custom Python functions that execute when triggered\nLists of strings/callables: multiple actions that execute in sequence\n\nActions can be paired with different severity levels:\n\n‘warning’: for minor issues that need attention\n‘error’: for more significant problems\n‘critical’: for severe issues that require immediate action\n\nThe v0.8.0 release added two (very) useful new parameters:\n\ndefault=: apply the same action to all threshold levels\nhighest_only=: only trigger the action for the highest threshold level reached (True by default)\n\nLet’s see how these work in practice:\n\ndef log_problem():\n    # Simple action that runs when thresholds are exceeded\n    print(\"A validation threshold has been exceeded!\")\n\nvalidation_2 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(default=log_problem)  # Apply this action to all threshold levels\n    )\n    .col_vals_regex(\n        columns=\"player_id\",\n        pattern=r\"[A-Z]{12}\\d{3}\"\n    )\n    .col_vals_gt(\n        columns=\"item_revenue\",\n        value=0.10\n    )\n    .interrogate()\n)\n\nvalidation_2\n\nA validation threshold has been exceeded!\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:38PolarsWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    14400.72\n    5600.28\n    ●\n    ●\n    ●\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we’re using a simple function that prints a generic message whenever any threshold is exceeded. By using the Actions(default=) parameter, this same function gets applied to all threshold levels (‘warning’, ‘error’, and ‘critical’). This saves you from having to define separate actions for each level when you want the same behavior for all of them. The highest_only= parameter (True by default, so not shown here) is complementary and it ensures that only the action for the highest threshold level reached will be triggered, preventing multiple notifications for the same validation failure."
  },
  {
    "objectID": "blog/all-about-actions/index.html#dynamic-messages-with-templating",
    "href": "blog/all-about-actions/index.html#dynamic-messages-with-templating",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Dynamic Messages with Templating",
    "text": "Dynamic Messages with Templating\nActions don’t have to be static messages. With Pointblank’s templating system, you can create context-aware notifications that include details about the specific validation failure.\nAvailable placeholders include:\n\n{type}: the validation step type (e.g., \"col_vals_gt\")\n{level}: the threshold level (‘warning’, ‘error’, ‘critical’)\n{step} or {i}: the step number in the validation workflow\n{col} or {column}: the column name being validated\n{val} or {value}: the comparison value used in the validation\n{time}: when the action was executed\n\nYou can also capitalize placeholders (like {LEVEL}) to get uppercase text.\n\naction_template = \"[{LEVEL}] Step {step}: Values in '{column}' failed validation against {value}.\"\n\nvalidation_3 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        thresholds=pb.Thresholds(warning=1, error=4, critical=10),\n        actions=pb.Actions(default=action_template)\n    )\n    .col_vals_lt(\n        columns=\"d\",\n        value=3000\n    )\n    .interrogate()\n)\n\nvalidation_3\n\n[ERROR] Step 1: Values in 'd' failed validation against 3000.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:38PolarsWARNING1ERROR4CRITICAL10\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    3000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    ●\n    ●\n    ○\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThis templating approach is a great way to create context-aware notifications that adapt to the specific validation failures occurring. As the example shows, when values in column d fail validation against the limit of 3000, the template automatically generates a meaningful error message showing exactly which step, column, and threshold value was involved."
  },
  {
    "objectID": "blog/all-about-actions/index.html#accessing-metadata-in-custom-action-functions",
    "href": "blog/all-about-actions/index.html#accessing-metadata-in-custom-action-functions",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Accessing Metadata in Custom Action Functions",
    "text": "Accessing Metadata in Custom Action Functions\nFor more sophisticated actions, you often need access to details about the validation failure. The get_action_metadata() function provides this context when called inside an action function:\n\ndef send_detailed_alert():\n    # Get metadata about the validation failure\n    metadata = pb.get_action_metadata()\n\n    # Create a customized alert message\n    print(f\"\"\"\n    VALIDATION FAILURE DETAILS\n    -------------------------\n    Step: {metadata['step']}\n    Column: {metadata['column']}\n    Validation type: {metadata['type']}\n    Severity: {metadata['level']} (level {metadata['level_num']})\n    Time: {metadata['time']}\n\n    Explanation: {metadata['failure_text']}\n    \"\"\")\n\nvalidation_4 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        thresholds=pb.Thresholds(critical=1),\n        actions=pb.Actions(critical=send_detailed_alert)\n    )\n    .col_vals_gt(\n        columns=\"d\",\n        value=5000\n    )\n    .interrogate()\n)\n\nvalidation_4\n\n\n    VALIDATION FAILURE DETAILS\n    -------------------------\n    Step: 1\n    Column: d\n    Validation type: col_vals_gt\n    Severity: critical (level 50)\n    Time: 2025-05-23 02:18:38.628015+00:00\n\n    Explanation: Exceedance of failed test units where values in `d` should have been &gt; `5000`.\n    \n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:38PolarsWARNING—ERROR—CRITICAL1\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    5000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    10.08\n    120.92\n    —\n    —\n    ●\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe metadata dictionary contains essential fields for a given validation step, including the step number, column name, validation type, severity level, and failure explanation. This gives you complete flexibility to create highly customized responses based on the specific nature of the validation failure."
  },
  {
    "objectID": "blog/all-about-actions/index.html#final-actions-with-finalactions",
    "href": "blog/all-about-actions/index.html#final-actions-with-finalactions",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Final Actions with FinalActions",
    "text": "Final Actions with FinalActions\nWhile regular Actions are great for responding to individual validation steps, sometimes you need to take action based on the overall validation results. This is where the new FinalActions feature from v0.8.1 comes in.\nUnlike regular Actions that trigger during validation, FinalActions execute after all validation steps are complete. FinalActions accepts any number of actions (strings or callables) and executes them in sequence. Each argument can be a string message to display in the console, a callable function, or a list of strings/callables for multiple actions to execute in sequence.\nThe real power of FinalActions comes from the ability to access comprehensive information about your validation results using get_validation_summary(). When called inside a function passed to FinalActions, this function provides a dictionary containing counts of passing/failing steps and test units, threshold levels exceeded, and much more:\n\ndef generate_summary():\n    # Access comprehensive validation results\n    summary = pb.get_validation_summary()\n\n    print(\"\\n=== VALIDATION SUMMARY ===\")\n    print(f\"Total steps: {summary['n_steps']}\")\n    print(f\"Passing steps: {summary['n_passing_steps']}\")\n    print(f\"Failing steps: {summary['n_failing_steps']}\")\n\n    if summary['highest_severity'] == \"critical\":\n        print(\"\\n⚠️ CRITICAL FAILURES DETECTED - immediate action required!\")\n    elif summary['highest_severity'] == \"error\":\n        print(\"\\n⚠️ ERRORS DETECTED - review needed\")\n    elif summary['highest_severity'] == \"warning\":\n        print(\"\\n⚠️ WARNINGS DETECTED - please investigate\")\n    else:\n        print(\"\\n✅ All validations passed!\")\n\nvalidation_5 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        tbl_name=\"small_table\",\n        thresholds=pb.Thresholds(warning=1, error=5, critical=10),\n        final_actions=pb.FinalActions(\n            \"Validation process complete.\",  # A simple string message\n            generate_summary               # Our function using get_validation_summary()\n        )\n    )\n    .col_vals_gt(columns=\"a\", value=1)\n    .col_vals_lt(columns=\"d\", value=10000)\n    .interrogate()\n)\n\nvalidation_5\n\nValidation process complete.\n\n=== VALIDATION SUMMARY ===\nTotal steps: 2\nPassing steps: 1\nFailing steps: 1\n\n⚠️ WARNINGS DETECTED - please investigate\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:38Polarssmall_tableWARNING1ERROR5CRITICAL10\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    10000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe get_validation_summary() function is only available within functions passed to FinalActions. It gives you access to these key dictionary fields:\n\ntbl_name: name of the validated table\nn_steps: total number of validation steps\nn_passing_steps, n_failing_steps: count of passing/failing steps\nn, n_passed, n_failed: total test units and their pass/fail counts\nhighest_severity: the most severe threshold level reached (‘warning’, ‘error’, ‘critical’)\nand many more detailed statistics\n\nThis information allows you to create detailed and specific final actions that can respond appropriately to the overall validation results."
  },
  {
    "objectID": "blog/all-about-actions/index.html#combining-regular-and-final-actions",
    "href": "blog/all-about-actions/index.html#combining-regular-and-final-actions",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Combining Regular and Final Actions",
    "text": "Combining Regular and Final Actions\nYou can use both Actions and FinalActions together for comprehensive control over your validation workflow:\n\ndef step_alert():\n    metadata = pb.get_action_metadata()\n    print(f\"Step {metadata['step']} failed with {metadata['level']} severity\")\n\n\ndef final_summary():\n    summary = pb.get_validation_summary()\n\n    # Get counts by checking each step's status in the dictionaries\n    steps = range(1, summary['n_steps'] + 1)\n    n_critical = sum(1 for step in steps if summary['dict_critical'].get(step, False))\n    n_error = sum(1 for step in steps if summary['dict_error'].get(step, False))\n    n_warning = sum(1 for step in steps if summary['dict_warning'].get(step, False))\n\n    print(f\"\\nValidation complete with:\")\n    print(f\"- {n_critical} critical issues\")\n    print(f\"- {n_error} errors\")\n    print(f\"- {n_warning} warnings\")\n\n\nvalidation_6 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        thresholds=pb.Thresholds(warning=1, error=5, critical=10),\n        actions=pb.Actions(default=step_alert),\n        final_actions=pb.FinalActions(final_summary),\n    )\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"d\", value=1000)\n    .interrogate()\n)\n\nvalidation_6\n\nStep 1 failed with critical severity\nStep 2 failed with error severity\n\nValidation complete with:\n- 1 critical issues\n- 2 errors\n- 2 warnings\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:38PolarsWARNING1ERROR5CRITICAL10\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    30.23\n    100.77\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    ●\n    ●\n    ○\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThis approach allows you to log individual step failures during the validation process using Actions and generate a comprehensive report after all validation steps are complete using FinalActions. Using both action types gives you fine-grained control over when and how notifications and other actions are triggered in your validation workflow."
  },
  {
    "objectID": "blog/all-about-actions/index.html#real-world-example-building-an-automated-validation-pipeline",
    "href": "blog/all-about-actions/index.html#real-world-example-building-an-automated-validation-pipeline",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Real-World Example: Building an Automated Validation Pipeline",
    "text": "Real-World Example: Building an Automated Validation Pipeline\nLet’s put everything together in a more realistic example. Imagine you’re validating a gaming revenue dataset and want to:\n\nlog detailed information about each failure\nsend a Slack notification if critical failures occur\ngenerate a comprehensive report after validation completes\n\n\ndef log_step_failure():\n    metadata = pb.get_action_metadata()\n    print(f\"[{metadata['level'].upper()}] Step {metadata['step']}: {metadata['failure_text']}\")\n\ndef analyze_results():\n    summary = pb.get_validation_summary()\n\n    # Calculate overall pass rate\n    pass_rate = (summary['n_passing_steps'] / summary['n_steps']) * 100\n\n    print(f\"\\n==== VALIDATION RESULTS ====\")\n    print(f\"Table: {summary['tbl_name']}\")\n    print(f\"Pass rate: {pass_rate:.2f}%\")\n    print(f\"Failing steps: {summary['n_failing_steps']} of {summary['n_steps']}\")\n\n    # In a real scenario, here you might:\n    # 1. Save results to a database\n    # 2. Generate and email an HTML report\n    # 3. Trigger data cleansing workflows\n\n    # Simulate a Slack notification\n    if summary['highest_severity'] == \"critical\":\n        print(\"\\n🚨 [SLACK NOTIFICATION] Critical data quality issues detected!\")\n        print(\"@data-team Please investigate immediately.\")\n\n# Create our validation workflow with actions\nvalidation_7 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\"),\n        tbl_name=\"game_revenue\",\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(default=log_step_failure, highest_only=True),\n        final_actions=pb.FinalActions(analyze_results),\n        brief=True  # Add automatically-generated briefs\n    )\n    .col_vals_regex(\n        columns=\"player_id\",\n        pattern=r\"[A-Z]{12}\\d{3}\",\n        brief=\"Player IDs must follow standard format\"  # Custom brief text\n    )\n    .col_vals_gt(\n        columns=\"item_revenue\",\n        value=0.10\n    )\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15\n    )\n    .interrogate()\n)\n\nvalidation_7\n\n[CRITICAL] Step 2: Exceedance of failed test units where values in `item_revenue` should have been &gt; `0.1`.\n[CRITICAL] Step 3: Exceedance of failed test units where values in `session_duration` should have been &gt; `15`.\n\n==== VALIDATION RESULTS ====\nTable: game_revenue\nPass rate: 33.33%\nFailing steps: 2 of 3\n\n🚨 [SLACK NOTIFICATION] Critical data quality issues detected!\n@data-team Please investigate immediately.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:18:39Polarsgame_revenueWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        Player IDs must follow standard format\n\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in item_revenue should be &gt; 0.1.\n\n        \n    item_revenue\n    0.1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    14400.72\n    5600.28\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in session_duration should be &gt; 15.\n\n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    CSV"
  },
  {
    "objectID": "blog/all-about-actions/index.html#wrapping-up-from-passive-validation-to-active-data-quality-management",
    "href": "blog/all-about-actions/index.html#wrapping-up-from-passive-validation-to-active-data-quality-management",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Wrapping Up: from Passive Validation to Active Data Quality Management",
    "text": "Wrapping Up: from Passive Validation to Active Data Quality Management\nWith Actions and FinalActions, Pointblank is now more of a complete data quality management system. Instead of just detecting problems, you can now:\n\nrespond immediately to validation failures\ncustomize responses based on severity level\ngenerate comprehensive reports after validation completes\nintegrate with other systems through custom action functions\nautomate workflows based on validation results\n\nThese capabilities transform data validation from a passive reporting activity into an active component of your data pipeline, helping ensure that data quality issues are detected, reported, and addressed efficiently.\nAs we continue to enhance Pointblank, we’d love to hear how you’re using Actions and FinalActions in your workflows. Share your experiences or suggestions with us on Discord or file an issue on GitHub."
  },
  {
    "objectID": "blog/all-about-actions/index.html#learn-more",
    "href": "blog/all-about-actions/index.html#learn-more",
    "title": "Level Up Your Data Validation with Actions and FinalActions",
    "section": "Learn More",
    "text": "Learn More\nExplore our documentation to learn more about Pointblank’s action capabilities:\n\nActions documentation\nFinalActions documentation\nUser Guide on Triggering Actions"
  },
  {
    "objectID": "user-guide/segmentation.html",
    "href": "user-guide/segmentation.html",
    "title": "Segmentation",
    "section": "",
    "text": "When validating data, you often need to analyze specific subsets or segments of your data separately. Maybe you want to ensure that data quality meets standards in each geographic region, for each product category, or across different time periods. This is where the segments= argument can be useful.\nData segmentation lets you split a validation step into multiple segments, with each segment receiving its own validation step. Rather than validating an entire table at once, you could instead validate different partitions separately and get separate results for each.\nThe segments= argument is available in many validation methods; typically it’s in those methods that check values within rows, and those methods that examine entire rows (rows_distinct(), rows_complete()). When you use it, Pointblank will:\nLet’s explore how to use the segments= argument through a few practical examples.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#basic-segmentation-by-column-values",
    "href": "user-guide/segmentation.html#basic-segmentation-by-column-values",
    "title": "Segmentation",
    "section": "Basic Segmentation by Column Values",
    "text": "Basic Segmentation by Column Values\nThe simplest way to segment data is by the unique values in a column. For the upcoming example, we’ll use the small_table dataset, which contains a categorical-value column called f.\nFirst, let’s preview the dataset:\n\ntable = pb.load_dataset()\n\npb.preview(table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nNow, let’s validate that values in column d are greater than 100, but we’ll also segment the validation by the categorical values in column f:\n\nvalidation_1 = (\n    pb.Validate(\n        data=pb.load_dataset(),\n        tbl_name=\"small_table\",\n        label=\"Segmented validation by category\"\n    )\n    .col_vals_gt(\n        columns=\"d\",\n        value=100,\n        segments=\"f\"  # Segment by values in column f\n    )\n    .interrogate()\n)\n\nvalidation_1\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Segmented validation by categoryPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    SEGMENT  f / high \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    SEGMENT  f / low \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    SEGMENT  f / mid \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    2\n    21.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the validation report, notice that instead of a single validation step, we have multiple steps: one for each unique value in the f column. The segmentation is clearly indicated in the STEP column with labels like SEGMENT  f / high, making it easy to identify which segment each validation result belongs to. This clear labeling helps when reviewing reports, especially with complex validations that use multiple segmentation criteria.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#segmenting-on-specific-values",
    "href": "user-guide/segmentation.html#segmenting-on-specific-values",
    "title": "Segmentation",
    "section": "Segmenting on Specific Values",
    "text": "Segmenting on Specific Values\nSometimes you don’t want to segment on all unique values in a column, but only on specific ones of interest. You can do this by providing a tuple with the column name and a list of values:\n\nvalidation_2 = (\n    pb.Validate(\n        data=pb.load_dataset(),\n        tbl_name=\"small_table\",\n        label=\"Segmented validation on specific categories\"\n    )\n    .col_vals_gt(\n        columns=\"d\",\n        value=100,\n        segments=(\"f\", [\"low\", \"high\"])  # Only segment on \"low\" and \"high\" values in column `f`\n    )\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Segmented validation on specific categoriesPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    SEGMENT  f / low \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    SEGMENT  f / high \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we only create validation steps for the \"low\" and \"high\" segments, ignoring any rows with f equal to \"mid\".",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#multiple-segmentation-criteria",
    "href": "user-guide/segmentation.html#multiple-segmentation-criteria",
    "title": "Segmentation",
    "section": "Multiple Segmentation Criteria",
    "text": "Multiple Segmentation Criteria\nFor more complex segmentation, you can provide a list of columns or column-value tuples. This creates segments based on combinations of criteria:\n\nvalidation_3 = (\n    pb.Validate(\n        data=pb.load_dataset(),\n        tbl_name=\"small_table\",\n        label=\"Multiple segmentation criteria\"\n    )\n    .col_vals_gt(\n        columns=\"d\",\n        value=100,\n        segments=[\"f\", (\"a\", [1, 2])]  # Segment by values in `f` AND specific values in `a`\n    )\n    .interrogate()\n)\n\nvalidation_3\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Multiple segmentation criteriaPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    SEGMENT  f / high \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    SEGMENT  f / low \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    SEGMENT  f / mid \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    2\n    21.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    SEGMENT  a / 1 \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    SEGMENT  a / 2 \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis creates validation steps for each combination of values in column f and the specified values in column a.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#segmentation-with-preprocessing",
    "href": "user-guide/segmentation.html#segmentation-with-preprocessing",
    "title": "Segmentation",
    "section": "Segmentation with Preprocessing",
    "text": "Segmentation with Preprocessing\nYou can combine segmentation with preprocessing for powerful and flexible validations. All preprocessing is applied before segmentation occurs, which means you can create derived columns to segment on:\n\nimport polars as pl\n\nvalidation_4 = (\n    pb.Validate(\n        data=pb.load_dataset(tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"Segmentation with preprocessing\",\n    )\n    .col_vals_gt(\n        columns=\"d\",\n        value=100,\n        pre=lambda df: df.with_columns(\n            d_category=pl.when(pl.col(\"d\") &gt; 150).then(pl.lit(\"high\")).otherwise(pl.lit(\"low\"))\n        ),\n        segments=\"d_category\",  # Segment by the computed column generated via `pre=`\n    )\n    .interrogate()\n)\n\nvalidation_4\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Segmentation with preprocessingPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    SEGMENT  d_category / high \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    12\n    121.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    SEGMENT  d_category / low \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we first create a derived column d_category based on whether d is greater than 150. Then, we segment our validation based on this derived column by using segments=\"d_category\".",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#when-to-use-segmentation",
    "href": "user-guide/segmentation.html#when-to-use-segmentation",
    "title": "Segmentation",
    "section": "When to Use Segmentation",
    "text": "When to Use Segmentation\nSegmentation is particularly useful when:\n\nData quality standards vary by group: different regions, product lines, or customer segments might have different acceptable thresholds\nIdentifying problem areas: segmentation helps pinpoint exactly where data quality issues exist, rather than just knowing that some issue exists somewhere in the data\nGenerating detailed reports: by segmenting, you get more granular reporting that can be shared with different stakeholders responsible for different parts of the data\nTracking improvements over time: segmented validations make it easier to see if data quality is improving in specific areas that were previously problematic\n\nBy using segmentation strategically in these scenarios, you can transform your data validation from a simple pass/fail system into a much more nuanced diagnostic tool that provides actionable insights about data quality across different dimensions. This targeted approach not only helps identify issues more precisely but also enables more effective communication of data quality metrics to relevant stakeholders.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#segmentation-vs.-multiple-validation-steps",
    "href": "user-guide/segmentation.html#segmentation-vs.-multiple-validation-steps",
    "title": "Segmentation",
    "section": "Segmentation vs. Multiple Validation Steps",
    "text": "Segmentation vs. Multiple Validation Steps\nSo why use segmentation instead of just creating separate validation steps for each segment using filtering in the pre= argument? Well, segmentation offers several nice advantages:\n\nConciseness: you define your validation logic once, not repeatedly for each segment\nConsistency: we can be certain that the same validation is applied uniformly across segments\nClarity: the validation report will clearly organize results by segment (with extra labeling)\nConvenience: there’s no need to manually extract and filter subsets of your data\n\nSegmentation can end of simplifying your validation code while also providing more structured and informative reporting about different portions of your data.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#practical-example-validating-sales-data-by-region-and-product-type",
    "href": "user-guide/segmentation.html#practical-example-validating-sales-data-by-region-and-product-type",
    "title": "Segmentation",
    "section": "Practical Example: Validating Sales Data by Region and Product Type",
    "text": "Practical Example: Validating Sales Data by Region and Product Type\nLet’s see a more realistic example where we validate sales data segmented by both region and product type:\n\nimport pandas as pd\nimport numpy as np\n\n# Create a sample sales dataset\nnp.random.seed(123)\n\n# Create a simple sales dataset\nsales_data = pd.DataFrame({\n    \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], 100),\n    \"product_type\": np.random.choice([\"Electronics\", \"Clothing\", \"Food\"], 100),\n    \"units_sold\": np.random.randint(5, 100, 100),\n    \"revenue\": np.random.uniform(100, 10000, 100),\n    \"cost\": np.random.uniform(50, 5000, 100)\n})\n\n# Calculate profit\nsales_data[\"profit\"] = sales_data[\"revenue\"] - sales_data[\"cost\"]\nsales_data[\"profit_margin\"] = sales_data[\"profit\"] / sales_data[\"revenue\"]\n\n# Preview the dataset\npb.preview(sales_data)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows100Columns7\n  \n\n  \n  regionobject\n  product_typeobject\n  units_soldint64\n  revenuefloat64\n  costfloat64\n  profitfloat64\n  profit_marginfloat64\n\n\n\n  \n    1\n    East\n    Clothing\n    55\n    8428.654356103547\n    1363.5197435071943\n    7065.134612596353\n    0.8382280627607168\n  \n  \n    2\n    South\n    Electronics\n    7\n    6589.7066024003025\n    3824.069456121553\n    2765.6371462787497\n    0.41969048292246663\n  \n  \n    3\n    East\n    Food\n    23\n    4680.5819759229435\n    4122.545156369359\n    558.0368195535848\n    0.11922381071929586\n  \n  \n    4\n    East\n    Clothing\n    51\n    5693.611988153584\n    1797.3122335569797\n    3896.2997545966045\n    0.6843282897927435\n  \n  \n    5\n    North\n    Clothing\n    50\n    4296.763518753258\n    4872.448283639371\n    -575.684764886113\n    -0.13398102138354426\n  \n  \n    96\n    West\n    Clothing\n    85\n    6551.261354681658\n    936.7119894981438\n    5614.549365183515\n    0.8570180704470368\n  \n  \n    97\n    South\n    Electronics\n    29\n    9543.579639173184\n    2779.779531480257\n    6763.800107692927\n    0.7087277901396456\n  \n  \n    98\n    East\n    Food\n    20\n    4822.302251263769\n    2833.48720726181\n    1988.815044001959\n    0.41242023837903463\n  \n  \n    99\n    North\n    Clothing\n    54\n    8801.046116310079\n    2185.8559620190636\n    6615.1901542910155\n    0.7516368016788095\n  \n  \n    100\n    North\n    Clothing\n    85\n    7942.857049695305\n    1834.7969383843642\n    6108.060111310941\n    0.7690003827458094\n  \n\n\n\n\n\n\n        \n\n\nNow, let’s validate that profit margins are above 20% across different regions and product types:\n\nvalidation_5 = (\n    pb.Validate(\n        data=sales_data,\n        tbl_name=\"sales_data\",\n        label=\"Sales data validation by region and product\"\n    )\n    .col_vals_gt(\n        columns=\"profit_margin\",\n        value=0.2,\n        segments=[\"region\", \"product_type\"],\n        brief=\"Profit margin &gt; 20% check\"\n    )\n    .interrogate()\n)\n\nvalidation_5\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Sales data validation by region and productPandassales_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    SEGMENT  region / East \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Profit margin &gt; 20% check\n\n        \n    profit_margin\n    0.2\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    30\n    200.67\n    100.33\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    SEGMENT  region / North \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Profit margin &gt; 20% check\n\n        \n    profit_margin\n    0.2\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    25\n    170.68\n    80.32\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    3\n    SEGMENT  region / South \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Profit margin &gt; 20% check\n\n        \n    profit_margin\n    0.2\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    21\n    180.86\n    30.14\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    4\n    SEGMENT  region / West \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Profit margin &gt; 20% check\n\n        \n    profit_margin\n    0.2\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    24\n    160.67\n    80.33\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    5\n    SEGMENT  product_type / Clothing \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Profit margin &gt; 20% check\n\n        \n    profit_margin\n    0.2\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    38\n    280.74\n    100.26\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    6\n    SEGMENT  product_type / Electronics \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Profit margin &gt; 20% check\n\n        \n    profit_margin\n    0.2\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    33\n    210.64\n    120.36\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    7\n    SEGMENT  product_type / Food \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Profit margin &gt; 20% check\n\n        \n    profit_margin\n    0.2\n    \n    \n        \n            \n            \n            \n            \n        \n    \n\n    ✓\n    29\n    220.76\n    70.24\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThis validation gives us a detailed breakdown of profit margin performance across the different regions and product types, making it easy to identify areas that need attention.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#best-practices-for-segmentation",
    "href": "user-guide/segmentation.html#best-practices-for-segmentation",
    "title": "Segmentation",
    "section": "Best Practices for Segmentation",
    "text": "Best Practices for Segmentation\nEffective data segmentation requires thoughtful planning about how to divide your data in ways that make sense for your validation needs. When implementing segmentation in your data validation workflow, consider these key principles:\n\nChoose meaningful segments: select segmentation columns that align with your business logic and organizational structure\nUse preprocessing when needed: if your raw data doesn’t have good segmentation columns, create them through preprocessing (with the pre= argument)\nCombine with actions: for critical segments, define segment-specific actions using the actions= parameter to respond to validation failures.\n\nBy implementing these best practices, you’ll create more targeted, maintainable, and actionable data validations. Segmentation becomes most powerful when it aligns with natural divisions in your data and analytical processes, allowing for more precise identification of quality issues while maintaining a unified validation framework.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/segmentation.html#conclusion",
    "href": "user-guide/segmentation.html#conclusion",
    "title": "Segmentation",
    "section": "Conclusion",
    "text": "Conclusion\nData segmentation can make your validations more targeted and informative. By dividing your data into meaningful segments, you can identify quality issues with greater precision, apply appropriate validation standards to different parts of your data, and generate more actionable reports.\nThe segments= parameter transforms validation from a monolithic process into a granular assessment of data quality across various dimensions of your dataset. Whether you’re dealing with regional differences, product categories, time periods, or any other meaningful divisions in your data, segmentation makes it possible to validate each portion according to its specific requirements while maintaining the simplicity of a unified validation framework.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Segmentation"
    ]
  },
  {
    "objectID": "user-guide/preview.html",
    "href": "user-guide/preview.html",
    "title": "Previewing Data",
    "section": "",
    "text": "In many cases, it’s good to look at your data tables. Before validating a table, you’ll likely want to inspect a portion of it before diving into the creation of data-quality rules. This is pretty easily done with Polars and Pandas DataFrames, however, it’s not as easy with database tables and each table backend displays things differently.\nTo make this common task a little better, you can use the preview() function in Pointblank. It has been designed to work with every table that the package supports (i.e., DataFrames and Ibis-backend tables, the latter of which are largely database tables). Plus, what’s shown in the output is consistent, no matter what type of data you’re looking at.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  },
  {
    "objectID": "user-guide/preview.html#viewing-a-table-with-preview",
    "href": "user-guide/preview.html#viewing-a-table-with-preview",
    "title": "Previewing Data",
    "section": "Viewing a Table with preview()",
    "text": "Viewing a Table with preview()\nLet’s look at how preview() works. It requires only a table and, for this first example, let’s use the nycflights dataset:\n\nimport pointblank as pb\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"polars\")\n\npb.preview(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  yearInt64\n  monthInt64\n  dayInt64\n  dep_timeInt64\n  sched_dep_timeInt64\n  dep_delayInt64\n  arr_timeInt64\n  sched_arr_timeInt64\n  arr_delayInt64\n  carrierString\n  flightInt64\n  tailnumString\n  originString\n  destString\n  air_timeInt64\n  distanceInt64\n  hourInt64\n  minuteInt64\n\n\n\n  \n    1\n    2013\n    1\n    1\n    517\n    515\n    2\n    830\n    819\n    11\n    UA\n    1545\n    N14228\n    EWR\n    IAH\n    227\n    1400\n    5\n    15\n  \n  \n    2\n    2013\n    1\n    1\n    533\n    529\n    4\n    850\n    830\n    20\n    UA\n    1714\n    N24211\n    LGA\n    IAH\n    227\n    1416\n    5\n    29\n  \n  \n    3\n    2013\n    1\n    1\n    542\n    540\n    2\n    923\n    850\n    33\n    AA\n    1141\n    N619AA\n    JFK\n    MIA\n    160\n    1089\n    5\n    40\n  \n  \n    4\n    2013\n    1\n    1\n    544\n    545\n    -1\n    1004\n    1022\n    -18\n    B6\n    725\n    N804JB\n    JFK\n    BQN\n    183\n    1576\n    5\n    45\n  \n  \n    5\n    2013\n    1\n    1\n    554\n    600\n    -6\n    812\n    837\n    -25\n    DL\n    461\n    N668DN\n    LGA\n    ATL\n    116\n    762\n    6\n    0\n  \n  \n    336772\n    2013\n    9\n    30\n    None\n    1455\n    None\n    None\n    1634\n    None\n    9E\n    3393\n    None\n    JFK\n    DCA\n    None\n    213\n    14\n    55\n  \n  \n    336773\n    2013\n    9\n    30\n    None\n    2200\n    None\n    None\n    2312\n    None\n    9E\n    3525\n    None\n    LGA\n    SYR\n    None\n    198\n    22\n    0\n  \n  \n    336774\n    2013\n    9\n    30\n    None\n    1210\n    None\n    None\n    1330\n    None\n    MQ\n    3461\n    N535MQ\n    LGA\n    BNA\n    None\n    764\n    12\n    10\n  \n  \n    336775\n    2013\n    9\n    30\n    None\n    1159\n    None\n    None\n    1344\n    None\n    MQ\n    3572\n    N511MQ\n    LGA\n    CLE\n    None\n    419\n    11\n    59\n  \n  \n    336776\n    2013\n    9\n    30\n    None\n    840\n    None\n    None\n    1020\n    None\n    MQ\n    3531\n    N839MQ\n    LGA\n    RDU\n    None\n    431\n    8\n    40\n  \n\n\n\n\n\n\n        \n\n\nThis is an HTML table using the style of the other reporting tables in the library. The header is more minimal here, only showing the type of table we’re looking at (POLARS in this case) along with the table dimensions. The column headers provide both the column names and the column data types.\nBy default, we’re getting the first five rows and the last five rows. Row numbers (from the original dataset) provide an indication of which rows are the head and tail rows. The blue lines provide additional demarcation of the column containing the row numbers and the head and tail row groups. Finally, any cells with missing values are prominently styled with red lettering and a lighter red background.\nIf you’d rather not see the row numbers in the table, you can use the show_row_numbers=False option. Let’s try that with the game_revenue dataset as a DuckDB table:\n\ngame_revenue = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\")\n\npb.preview(game_revenue, show_row_numbers=False)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nWith the above preview, the row numbers are gone. The horizontal blue line still serves to divide the top and bottom rows of the table, however.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  },
  {
    "objectID": "user-guide/preview.html#adjusting-the-number-of-rows-shown",
    "href": "user-guide/preview.html#adjusting-the-number-of-rows-shown",
    "title": "Previewing Data",
    "section": "Adjusting the Number of Rows Shown",
    "text": "Adjusting the Number of Rows Shown\nIt could be that displaying the five top and bottom rows is not preferred. This can be changed with the n_head= and n_tail=. Maybe, you want three from the top along with the last row? Let’s try that out with the small_table dataset as a Pandas DataFrame:\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\n\npb.preview(small_table, n_head=3, n_tail=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIf you’re looking at a small table and want to see the entirety of it, you can enlarge the n_head= and n_tail= values:\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\n\npb.preview(small_table, n_head=10, n_tail=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nGiven that the table has 13 rows, asking for 20 rows to be displayed effectively shows the entire table.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  },
  {
    "objectID": "user-guide/preview.html#previewing-a-subset-of-columns",
    "href": "user-guide/preview.html#previewing-a-subset-of-columns",
    "title": "Previewing Data",
    "section": "Previewing a Subset of Columns",
    "text": "Previewing a Subset of Columns\nThe preview scales well to tables that have many columns by allowing for a horizontal scroll. However, previewing data from all columns can be impractical if you’re only concerned with a key set of them. To preview only a subset of a table’s columns, we can use the columns_subset= argument. Let’s do this with the nycflights dataset and provide a list of six columns from that table.\n\npb.preview(\n    nycflights,\n    columns_subset=[\"hour\", \"minute\", \"sched_dep_time\", \"year\", \"month\", \"day\"]\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  hourInt64\n  minuteInt64\n  sched_dep_timeInt64\n  yearInt64\n  monthInt64\n  dayInt64\n\n\n\n  \n    1\n    5\n    15\n    515\n    2013\n    1\n    1\n  \n  \n    2\n    5\n    29\n    529\n    2013\n    1\n    1\n  \n  \n    3\n    5\n    40\n    540\n    2013\n    1\n    1\n  \n  \n    4\n    5\n    45\n    545\n    2013\n    1\n    1\n  \n  \n    5\n    6\n    0\n    600\n    2013\n    1\n    1\n  \n  \n    336772\n    14\n    55\n    1455\n    2013\n    9\n    30\n  \n  \n    336773\n    22\n    0\n    2200\n    2013\n    9\n    30\n  \n  \n    336774\n    12\n    10\n    1210\n    2013\n    9\n    30\n  \n  \n    336775\n    11\n    59\n    1159\n    2013\n    9\n    30\n  \n  \n    336776\n    8\n    40\n    840\n    2013\n    9\n    30\n  \n\n\n\n\n\n\n        \n\n\nWhat we see are the six columns we specified from the nycflights dataset.\nNote that the columns are displayed in the order provided in the columns_subset= list. This can be useful for making quick, side-by-side comparisons. In the example above, we placed hour and minute next to the sched_dep_time column. In the original dataset, sched_dep_time is far apart from the other two columns, but, it’s useful to have them next to each other in the preview since hour and minute are derived from sched_dep_time (and this lets us spot check any issues).\nWe can also use column selectors within columns_subset=. Suppose we want to only see those columns that have \"dep_\" or \"arr_\" in the name. To do that, we use the matches() column selector function:\n\npb.preview(nycflights, columns_subset=pb.matches(\"dep_|arr_\"))\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  dep_timeInt64\n  sched_dep_timeInt64\n  dep_delayInt64\n  arr_timeInt64\n  sched_arr_timeInt64\n  arr_delayInt64\n\n\n\n  \n    1\n    517\n    515\n    2\n    830\n    819\n    11\n  \n  \n    2\n    533\n    529\n    4\n    850\n    830\n    20\n  \n  \n    3\n    542\n    540\n    2\n    923\n    850\n    33\n  \n  \n    4\n    544\n    545\n    -1\n    1004\n    1022\n    -18\n  \n  \n    5\n    554\n    600\n    -6\n    812\n    837\n    -25\n  \n  \n    336772\n    None\n    1455\n    None\n    None\n    1634\n    None\n  \n  \n    336773\n    None\n    2200\n    None\n    None\n    2312\n    None\n  \n  \n    336774\n    None\n    1210\n    None\n    None\n    1330\n    None\n  \n  \n    336775\n    None\n    1159\n    None\n    None\n    1344\n    None\n  \n  \n    336776\n    None\n    840\n    None\n    None\n    1020\n    None\n  \n\n\n\n\n\n\n        \n\n\nSeveral selectors can be combined together through use of the col() function and operators such as & (and), | (or), - (difference), and ~ (not). Let’s look at a column selection case where:\n\nthe first three columns are selected\nall columns containing \"dep_\" or \"arr_\" are selected\nany columns beginning with \"sched\" are omitted\n\nThis is how we put that together within col():\n\npb.preview(\n    nycflights,\n    columns_subset=pb.col((pb.first_n(3) | pb.matches(\"dep_|arr_\")) & ~ pb.starts_with(\"sched\"))\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  yearInt64\n  monthInt64\n  dayInt64\n  dep_timeInt64\n  dep_delayInt64\n  arr_timeInt64\n  arr_delayInt64\n\n\n\n  \n    1\n    2013\n    1\n    1\n    517\n    2\n    830\n    11\n  \n  \n    2\n    2013\n    1\n    1\n    533\n    4\n    850\n    20\n  \n  \n    3\n    2013\n    1\n    1\n    542\n    2\n    923\n    33\n  \n  \n    4\n    2013\n    1\n    1\n    544\n    -1\n    1004\n    -18\n  \n  \n    5\n    2013\n    1\n    1\n    554\n    -6\n    812\n    -25\n  \n  \n    336772\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336773\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336774\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336775\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336776\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n\n\n\n\n\n\n        \n\n\nThis gives us a preview with only the columns that fit the specific selection rules. Incidentally, using selectors with a dataset through preview() is a good way to test out the use of selectors more generally. Since they are primarily used to select columns for validation, trying them beforehand with preview() can help verify that your selection logic is sound.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html",
    "href": "user-guide/schema-validation.html",
    "title": "Schema Validation",
    "section": "",
    "text": "Schema validation in Pointblank allows you to verify that your data conforms to an expected structure and type specification. This is particularly useful when ensuring data consistency across systems or validating incoming data against predefined requirements.\nLet’s first look at the dataset we’ll use for the first example:\nimport pointblank as pb\n\n# Preview the small_table dataset we'll use throughout this guide\npb.preview(pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#schema-definition-and-validation",
    "href": "user-guide/schema-validation.html#schema-definition-and-validation",
    "title": "Schema Validation",
    "section": "Schema Definition and Validation",
    "text": "Schema Definition and Validation\nA schema in Pointblank is created using the Schema class which defines the expected structure of a table. Once created, you apply schema validation through the col_schema_match() validation step.\n\n# Create a schema definition matching small_table structure\nschema = pb.Schema(\n    columns=[\n        (\"date_time\",),  # Only check column name\n        (\"date\",),  # Only check column name\n        (\"a\", \"Int64\"),  # Check name and type\n        (\"b\", \"String\"),  # Check name and type\n        (\"c\", \"Int64\"),  # Check name and type\n        (\"d\", \"Float64\"),  # Check name and type\n        (\"e\", \"Boolean\"),  # Check name and type\n        (\"f\",),  # Only check column name\n    ]\n)\n\n# Validate the small_table against the schema\nsmall_table_validation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        label=\"Schema validation of `small_table`.\",\n    )\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nsmall_table_validation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Schema validation of `small_table`.Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe output shows the validation passed successfully. When all columns have the correct names and types as specified in the schema, the validation passes with a single passing test unit. If there were discrepancies, this would fail, but the basic output wouldn’t show specific issues.\nFor detailed information about validation results, use get_step_report():\n\nsmall_table_validation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    date_time\n    Datetime(time_unit='us', time_zone=None)\n    1\n    date_time\n    ✓\n    —\n    \n  \n  \n    2\n    date\n    Date\n    2\n    date\n    ✓\n    —\n    \n  \n  \n    3\n    a\n    Int64\n    3\n    a\n    ✓\n    Int64\n    ✓\n  \n  \n    4\n    b\n    String\n    4\n    b\n    ✓\n    String\n    ✓\n  \n  \n    5\n    c\n    Int64\n    5\n    c\n    ✓\n    Int64\n    ✓\n  \n  \n    6\n    d\n    Float64\n    6\n    d\n    ✓\n    Float64\n    ✓\n  \n  \n    7\n    e\n    Boolean\n    7\n    e\n    ✓\n    Boolean\n    ✓\n  \n  \n    8\n    f\n    String\n    8\n    f\n    ✓\n    —\n    \n  \n\n  \n  \n  \n    Supplied Column Schema:[('date_time',), ('date',), ('a', 'Int64'), ('b', 'String'), ('c', 'Int64'), ('d', 'Float64'), ('e', 'Boolean'), ('f',)]\n  \n\n\n\n\n\n\n        \n\n\nThe step report provides specific details about which columns were checked and whether they matched the schema, helping diagnose issues when validation fails.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#schema-components-and-column-types",
    "href": "user-guide/schema-validation.html#schema-components-and-column-types",
    "title": "Schema Validation",
    "section": "Schema Components and Column Types",
    "text": "Schema Components and Column Types\nWhen defining a schema, you need to specify column names and optionally their data types. By default, Pointblank enforces strict validation where:\n\nall columns in your table must match the specified schema\ncolumn order must match the schema\ncolumn types are case-sensitive\ntype names must match exactly\n\nThe schema definition accepts column types as string representations, which vary depending on your data source:\n\nstring: Character data (may also be \"String\", \"varchar\", \"character\", etc.)\ninteger: Integer values (may also be \"Int64\", \"int\", \"bigint\", etc.)\nnumeric: Numeric values including integers and floating-point numbers (may also be \"Float64\", \"double\", \"decimal\", etc.)\nboolean: Logical values (True/False) (may also be \"Boolean\", \"bool\", etc.)\ndatetime: Date and time values (may also be \"Datetime\", \"timestamp\", etc.)\ndate: Date values (may also be \"Date\", etc.)\ntime: Time values\n\nFor specific database engines or DataFrame libraries, you may need to use their exact type names (like \"VARCHAR(255)\" for SQL databases or \"Int64\" for Polars integers).",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#discovering-column-types",
    "href": "user-guide/schema-validation.html#discovering-column-types",
    "title": "Schema Validation",
    "section": "Discovering Column Types",
    "text": "Discovering Column Types\nTo easily determine the correct type string for columns in your data, Pointblank provides two helpful functions:\n\nimport polars as pl\nfrom datetime import date\n\n# Define a sample dataframe\nsample_df = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"join_date\": [date(2020, 1, 1), date(2021, 3, 15), date(2022, 7, 10)]\n})\n\n\n# Method 1: Using `preview()` with `show_types=True` to see column types\npb.preview(sample_df)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows3Columns3\n  \n\n  \n  idInt64\n  nameString\n  join_dateDate\n\n\n\n  \n    1\n    1\n    Alice\n    2020-01-01\n  \n  \n    2\n    2\n    Bob\n    2021-03-15\n  \n  \n    3\n    3\n    Charlie\n    2022-07-10\n  \n\n\n\n\n\n\n        \n\n\n\n# Method 2: Using `col_summary_tbl()` which shows column types and other details\npb.col_summary_tbl(sample_df)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows3Columns3\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    idInt64\n    0 0.00\n    3 1.00\n    2.00\n    1.00\n    1.00\n    1.10\n    1.50\n    2.00\n    2.50\n    2.90\n    3.00\n    1.00\n  \n  \n    2\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    nameString\n    0 0.00\n    3 1.00\n    5.00SL\n    2.00SL\n    3SL\n    —\n    —\n    5SL\n    —\n    —\n    7SL\n    —\n  \n  \n    3\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    join_dateDate\n    0 0.00\n    3 1.00\n    —\n    —\n     2020-01-01 – 2022-07-10\n    \n    \n    \n    \n    \n    \n    —\n  \n\n\n\n\n\n\n        \n\n\nThese functions help you identify the exact type strings to use in your schema definitions, eliminating guesswork and ensuring compatibility with your data source.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#creating-a-schema",
    "href": "user-guide/schema-validation.html#creating-a-schema",
    "title": "Schema Validation",
    "section": "Creating a Schema",
    "text": "Creating a Schema\nYou can create a schema in four different ways, each with its own advantages. All schema objects can be printed to display their column names and data types.\n\n1. Using a List of Tuples with columns=\nThis approach allows for mixed validation: some columns checked for both name and type, others only for name:\n\nschema_tuples = pb.Schema(\n\n    # List of tuples approach: flexible for mixed type/name checking ---\n    columns=[\n        (\"name\", \"String\"), # Check name and type\n        (\"age\", \"Int64\"),   # Check name and type\n        (\"height\",)         # Check name only\n    ]\n)\n\nprint(schema_tuples)\n\nPointblank Schema\n  name: String\n  age: Int64\n  height: &lt;ANY&gt;\n\n\nThis is the only method that allows checking just column names for some columns while checking both names and types for others.\n\n\n2. Using a Dictionary with columns=\nThis approach is often the most readable when defining a schema manually, especially for larger schemas:\n\nschema_dict = pb.Schema(\n\n    # Dictionary approach (keys are column names, values are data types) ---\n    columns={\n        \"name\": \"String\",\n        \"age\": \"Int64\",\n        \"height\": \"Float64\",\n        \"created_at\": \"Datetime\"\n    }\n)\n\nprint(schema_dict)\n\nPointblank Schema\n  name: String\n  age: Int64\n  height: Float64\n  created_at: Datetime\n\n\nWith this method, you must always provide both column names (as keys) and their types (as values).\n\n\n3. Using Keyword Arguments\nFor more readable code with a small number of columns:\n\nschema_kwargs = pb.Schema(\n\n    # Keyword arguments approach (more readable for simple schemas) ---\n    name=\"String\",\n    age=\"Int64\",\n    height=\"Float64\"\n)\n\nprint(schema_kwargs)\n\nPointblank Schema\n  name: String\n  age: Int64\n  height: Float64\n\n\nLike the dictionary method, this approach requires both column names and types.\n\n\n4. Extracting from an Existing Table with tbl=\nYou can automatically extract a schema from an existing table:\n\nimport polars as pl\n\n# Create a sample dataframe\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"height\": [5.6, 6.0, 5.8]\n})\n\n# Extract schema from table\nschema_from_table = pb.Schema(tbl=df)\n\nprint(schema_from_table)\n\nPointblank Schema\n  name: String\n  age: Int64\n  height: Float64\n\n\nThis is especially useful when you want to validate that future data matches the structure of a reference dataset.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#multiple-data-types-for-a-column",
    "href": "user-guide/schema-validation.html#multiple-data-types-for-a-column",
    "title": "Schema Validation",
    "section": "Multiple Data Types for a Column",
    "text": "Multiple Data Types for a Column\nYou can specify multiple acceptable types for a column by providing a list of types:\n\n# Schema with multiple possible types for a column\nschema_multi_types = pb.Schema(\n    columns={\n        \"name\": \"String\",\n        \"age\": [\"Int64\", \"Float64\"],  # Accept either integer or float\n        \"active\": \"Boolean\"\n    }\n)\n\nprint(schema_multi_types)\n\nPointblank Schema\n  name: String\n  age: ['Int64', 'Float64']\n  active: Boolean\n\n\nThis is useful when working with data sources that might represent the same information in different ways (e.g., integers sometimes stored as floats).",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#schema-validation-options",
    "href": "user-guide/schema-validation.html#schema-validation-options",
    "title": "Schema Validation",
    "section": "Schema Validation Options",
    "text": "Schema Validation Options\nWhen using col_schema_match(), you can customize validation behavior with several important options:\n\n\n\n\n\n\n\n\nOption\nDefault\nDescription\n\n\n\n\ncomplete\nTRUE\nRequire exact column presence (no extra columns allowed)\n\n\nin_order\nTRUE\nEnforce column order\n\n\ncase_sensitive_colnames\nTRUE\nMake column name matching case-sensitive\n\n\ncase_sensitive_dtypes\nTRUE\nMake data type matching case-sensitive\n\n\nfull_match_dtypes\nTRUE\nRequire exact (not partial) type name matches\n\n\n\n\nControlling Column Presence\nBy default, col_schema_match() requires a complete match between the schema’s columns and the table’s columns. You can make this more flexible:\n\n# Create a sample table\nusers_table_extra = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"extra_col\": [\"a\", \"b\", \"c\"]  # Extra column not in schema\n})\n\n# Create a schema\nschema = pb.Schema(\n    columns={\"id\": \"Int64\", \"name\": \"String\", \"age\": \"Int64\"}\n)\n\n# Validate without requiring all columns to be present\nvalidation = (\n    pb.Validate(data=users_table_extra)\n    .col_schema_match(\n        schema=schema,\n\n        # Allow schema columns to be a subset ---\n        complete=False\n    )\n    .interrogate()\n)\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    id\n    Int64\n    1\n    id\n    ✓\n    Int64\n    ✓\n  \n  \n    2\n    name\n    String\n    2\n    name\n    ✓\n    String\n    ✓\n  \n  \n    3\n    age\n    Int64\n    3\n    age\n    ✓\n    Int64\n    ✓\n  \n  \n    4\n    extra_col\n    String\n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    Supplied Column Schema:[('id', 'Int64'), ('name', 'String'), ('age', 'Int64')]\n  \n\n\n\n\n\n\n        \n\n\n\n\nColumn Order Enforcement\nYou can control whether column order matters in your validation:\n\n# Create a sample table\nusers_table = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n})\n\n# Create a schema\nschema = pb.Schema(\n    columns={\"name\": \"String\", \"age\": \"Int64\", \"id\": \"Int64\"}\n)\n\n# Validate without enforcing column order\nvalidation = (\n    pb.Validate(data=users_table)\n    .col_schema_match(\n        schema=schema,\n\n        # Don't enforce column order ---\n        in_order=False\n    )\n    .interrogate()\n)\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    id\n    Int64\n    3\n    id\n    ✓\n    Int64\n    ✓\n  \n  \n    2\n    name\n    String\n    1\n    name\n    ✓\n    String\n    ✓\n  \n  \n    3\n    age\n    Int64\n    2\n    age\n    ✓\n    Int64\n    ✓\n  \n\n  \n  \n  \n    Supplied Column Schema:[('name', 'String'), ('age', 'Int64'), ('id', 'Int64')]\n  \n\n\n\n\n\n\n        \n\n\n\n\nCase Sensitivity\nControl whether column names and data types are case-sensitive:\n\n# Create schema with different case charactistics\ncase_schema = pb.Schema(\n    columns={\"ID\": \"int64\", \"NAME\": \"string\", \"AGE\": \"int64\"}\n)\n\n# Create validation with case-insensitive column names and types\nvalidation = (\n    pb.Validate(data=users_table)\n    .col_schema_match(\n        schema=case_schema,\n\n        # Ignore case in column names ---\n        case_sensitive_colnames=False,\n\n        # Ignore case in data type names ---\n        case_sensitive_dtypes=False\n    )\n    .interrogate()\n)\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN = columnDTYPE = dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    id\n    Int64\n    1\n    ID\n    ✓\n    int64\n    ✓\n  \n  \n    2\n    name\n    String\n    2\n    NAME\n    ✓\n    string\n    ✓\n  \n  \n    3\n    age\n    Int64\n    3\n    AGE\n    ✓\n    int64\n    ✓\n  \n\n  \n  \n  \n    Supplied Column Schema:[('ID', 'int64'), ('NAME', 'string'), ('AGE', 'int64')]\n  \n\n\n\n\n\n\n        \n\n\n\n\nType Matching Precision\nControl how strictly data types must match:\n\n# Create schema with simplified type names\ntype_schema = pb.Schema(\n\n    # Using simplified type names ---\n    columns={\"id\": \"int\", \"name\": \"str\", \"age\": \"int\"}\n)\n\n# Allow partial type matches\nvalidation = (\n    pb.Validate(data=users_table)\n    .col_schema_match(\n        schema=type_schema,\n\n        # Ignore case in data type names ---\n        case_sensitive_dtypes=False,\n\n        # Allow partial type name matches ---\n        full_match_dtypes=False\n    )\n    .interrogate()\n)\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE = dtypefloat = float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    id\n    Int64\n    1\n    id\n    ✓\n    int\n    ✓\n  \n  \n    2\n    name\n    String\n    2\n    name\n    ✓\n    str\n    ✓\n  \n  \n    3\n    age\n    Int64\n    3\n    age\n    ✓\n    int\n    ✓\n  \n\n  \n  \n  \n    Supplied Column Schema:[('id', 'int'), ('name', 'str'), ('age', 'int')]",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#common-schema-validation-patterns",
    "href": "user-guide/schema-validation.html#common-schema-validation-patterns",
    "title": "Schema Validation",
    "section": "Common Schema Validation Patterns",
    "text": "Common Schema Validation Patterns\nThis section explores common patterns for applying schema validation to different scenarios. Each pattern addresses specific validation needs you might encounter when working with real-world data. We’ll examine the step reports for these validations since they provide more detailed information about what was checked and how the validation performed, offering an intuitive way to understand the results beyond simple pass/fail indicators.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#common-schema-validation-patterns-1",
    "href": "user-guide/schema-validation.html#common-schema-validation-patterns-1",
    "title": "Schema Validation",
    "section": "Common Schema Validation Patterns",
    "text": "Common Schema Validation Patterns\nThis section explores common patterns for applying schema validation to different scenarios. Each pattern addresses specific validation needs you might encounter when working with real-world data. We’ll examine the step reports (get_step_report()) for these validations since they provide more detailed information about what was checked and how the validation performed, offering an intuitive way to understand the results beyond simple pass/fail indicators.\n\nStructural Validation Only\nWhen you only care about column names but not their types:\n\n# Create a schema with only column names\nstructure_schema = pb.Schema(\n    columns=[\"id\", \"name\", \"age\", \"extra_col\"]\n)\n\n# Validate structure only\nvalidation = (\n    pb.Validate(data=users_table_extra)\n    .col_schema_match(schema=structure_schema)\n    .interrogate()\n)\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    id\n    Int64\n    1\n    id\n    ✓\n    —\n    \n  \n  \n    2\n    name\n    String\n    2\n    name\n    ✓\n    —\n    \n  \n  \n    3\n    age\n    Int64\n    3\n    age\n    ✓\n    —\n    \n  \n  \n    4\n    extra_col\n    String\n    4\n    extra_col\n    ✓\n    —\n    \n  \n\n  \n  \n  \n    Supplied Column Schema:[('id',), ('name',), ('age',), ('extra_col',)]\n  \n\n\n\n\n\n\n        \n\n\n\n\nMixed Validation\nValidate types for critical columns but just presence for others:\n\n# Mixed validation for different columns\nmixed_schema = pb.Schema(\n    columns=[\n        (\"id\", \"Int64\"),       # Check name and type\n        (\"name\", \"String\"),    # Check name and type\n        (\"age\",),              # Check name only\n        (\"extra_col\",)         # Check name only\n    ]\n)\n\n# Validate with mixed approach\nvalidation = (\n    pb.Validate(data=users_table_extra)\n    .col_schema_match(schema=mixed_schema)\n    .interrogate()\n)\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    id\n    Int64\n    1\n    id\n    ✓\n    Int64\n    ✓\n  \n  \n    2\n    name\n    String\n    2\n    name\n    ✓\n    String\n    ✓\n  \n  \n    3\n    age\n    Int64\n    3\n    age\n    ✓\n    —\n    \n  \n  \n    4\n    extra_col\n    String\n    4\n    extra_col\n    ✓\n    —\n    \n  \n\n  \n  \n  \n    Supplied Column Schema:[('id', 'Int64'), ('name', 'String'), ('age',), ('extra_col',)]\n  \n\n\n\n\n\n\n        \n\n\n\n\nProgressive Schema Evolution\nAs your data evolves, you might need to adapt your validation approach:\n\n# Original schema\noriginal_schema = pb.Schema(\n    columns={\n        \"id\": \"Int64\",\n        \"name\": \"String\"\n    }\n)\n\n# New data with additional columns\nevolved_data = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],          # New column\n    \"active\": [True, False, True] # New column\n})\n\n# Validate with flexible parameters\nvalidation = (\n    pb.Validate(evolved_data)\n    .col_schema_match(\n        schema=original_schema,\n        complete=False,           # Allow extra columns\n        in_order=False            # Don't enforce order\n    )\n    .interrogate()\n)\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✓COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    id\n    Int64\n    1\n    id\n    ✓\n    Int64\n    ✓\n  \n  \n    2\n    name\n    String\n    2\n    name\n    ✓\n    String\n    ✓\n  \n  \n    3\n    age\n    Int64\n    \n    \n    \n    \n    \n  \n  \n    4\n    active\n    Boolean\n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    Supplied Column Schema:[('id', 'Int64'), ('name', 'String')]",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#integrating-with-larger-validation-workflows",
    "href": "user-guide/schema-validation.html#integrating-with-larger-validation-workflows",
    "title": "Schema Validation",
    "section": "Integrating with Larger Validation Workflows",
    "text": "Integrating with Larger Validation Workflows\nSchema validation is often just one part of a comprehensive data validation strategy. You can combine schema checks with other validation steps:\n\n# Define a schema\nschema = pb.Schema(\n    columns={\n        \"id\": \"Int64\",\n        \"name\": \"String\",\n        \"age\": \"Int64\"\n    }\n)\n\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        users_table,\n        label=\"User data validation\",\n        thresholds=pb.Thresholds(warning=0.05, error=0.1)\n    )\n    # Add schema validation\n    .col_schema_match(schema=schema)\n\n    # Add other validation steps\n    .col_vals_not_null(columns=\"id\")\n    .col_vals_gt(columns=\"age\", value=26)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    User data validationPolarsWARNING0.05ERROR0.1CRITICAL—\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    id\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    ○\n    ○\n    —\n    —\n  \n  \n    #EBBC14\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    age\n    26\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    20.67\n    10.33\n    ●\n    ●\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThis approach allows you to first validate the structure of your data and then check specific business rules or constraints.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#best-practices",
    "href": "user-guide/schema-validation.html#best-practices",
    "title": "Schema Validation",
    "section": "Best Practices",
    "text": "Best Practices\n\nDefine schemas early: document and define expected data structures early in your data workflow.\nChoose the right creation method:\n\nuse columns=&lt;dict&gt; for readability with many columns\nuse columns=&lt;list of tuples&gt; for mixed name/type validation\nuse kwargs for small schemas with simple column names\nuse tbl= to extract schemas from reference datasets\n\nBe deliberate about strictness: choose validation parameters based on your specific needs:\n\nstrict validation (complete=True) for critical data interfaces\nflexible validation (complete=False, in_order=False) for evolving datasets\n\nReuse schemas: create schema definitions that can be reused across multiple validation contexts.\nVersion control schemas: as your data evolves, maintain versions of your schemas to track changes.\nExtract schemas from reference data: when you have a ‘golden’ dataset that represents your ideal structure, use Schema(tbl=reference_data) to extract its schema.\nConsider type flexibility: use multiple types per column ([\"Int64\", \"Float64\"]) when working with data from diverse sources.\nCombine with targeted validation: use schema validation for structural checks and add specific validation steps for business rules.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/schema-validation.html#conclusion",
    "href": "user-guide/schema-validation.html#conclusion",
    "title": "Schema Validation",
    "section": "Conclusion",
    "text": "Conclusion\nSchema validation provides a powerful mechanism for ensuring your data adheres to expected structural requirements. It serves as an excellent first line of defense in your data validation strategy, verifying that the data you’re working with has the expected shape before applying more detailed business rule validations.\nThe Schema class offers multiple ways to define schemas, from manual specification with dictionaries or keyword arguments to automatic extraction from reference tables. When combined with the flexible options of col_schema_match(), you can implement validation approaches ranging from strict structural enforcement to more flexible evolution-friendly checks.\nBy understanding the different schema creation methods and validation options, you can efficiently validate the structure of your data tables and ensure they meet your requirements before processing.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Schema Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html",
    "href": "user-guide/draft-validation.html",
    "title": "Draft Validation",
    "section": "",
    "text": "Draft Validation in Pointblank leverages large language models (LLMs) to automatically generate validation plans for your data. This feature is especially useful when starting validation on a new dataset or when you need to quickly establish baseline validation coverage.\nThe DraftValidation class connects to various LLM providers to analyze your data’s characteristics and generate a complete validation plan tailored to its structure and content.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#how-draftvalidation-works",
    "href": "user-guide/draft-validation.html#how-draftvalidation-works",
    "title": "Draft Validation",
    "section": "How DraftValidation Works",
    "text": "How DraftValidation Works\nWhen you use DraftValidation, the process works through these steps:\n\na statistical summary of your data is generated using the DataScan class\nthis summary is converted to JSON format and sent to your selected LLM provider\nthe LLM uses the summary along with knowledge about Pointblank’s validation capabilities to generate a validation plan\nthe result is returned as executable Python code that you can use directly or modify as needed\n\nThe entire process happens without sending your actual data to the LLM provider, only a summary that includes column names, data types, basic statistics, and a small sample of values.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#requirements-and-setup",
    "href": "user-guide/draft-validation.html#requirements-and-setup",
    "title": "Draft Validation",
    "section": "Requirements and Setup",
    "text": "Requirements and Setup\nTo use the DraftValidation feature, you’ll need:\n\nan API key from a supported LLM provider\nthe required Python packages installed\n\nYou can install all necessary dependencies with:\npip install pointblank[generate]\nThis will install the chatlas package and other dependencies required for DraftValidation.\n\nSupported LLM Providers\nThe DraftValidation class supports multiple LLM providers:\n\nAnthropic (Claude models)\nOpenAI (GPT models)\nOllama (local LLMs)\nAmazon Bedrock (AWS-hosted models)\n\nEach provider has different capabilities and performance characteristics, but all can be used to generate validation plans through a consistent interface.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#basic-usage",
    "href": "user-guide/draft-validation.html#basic-usage",
    "title": "Draft Validation",
    "section": "Basic Usage",
    "text": "Basic Usage\nThe simplest way to use DraftValidation is to provide your data and specify an LLM model. Let’s try it out with the global_sales dataset.\nimport pointblank as pb\n\n# Load a dataset\ndata = pb.load_dataset(dataset=\"global_sales\", tbl_type=\"polars\")\n\n# Generate a validation plan\npb.DraftValidation(\n    data=data,\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    api_key=\"your_api_key_here\"  # Replace with your actual API key\n)\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"product_id\", \"String\"),\n    (\"product_category\", \"String\"),\n    (\"customer_id\", \"String\"),\n    (\"customer_segment\", \"String\"),\n    (\"region\", \"String\"),\n    (\"country\", \"String\"),\n    (\"city\", \"String\"),\n    (\"timestamp\", \"Datetime(time_unit='us', time_zone=None)\"),\n    (\"quarter\", \"String\"),\n    (\"month\", \"Int64\"),\n    (\"year\", \"Int64\"),\n    (\"price\", \"Float64\"),\n    (\"quantity\", \"Int64\"),\n    (\"status\", \"String\"),\n    (\"email\", \"String\"),\n    (\"revenue\", \"Float64\"),\n    (\"tax\", \"Float64\"),\n    (\"total\", \"Float64\"),\n    (\"payment_method\", \"String\"),\n    (\"sales_channel\", \"String\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"product_category\", \"customer_segment\", \"region\", \"country\",\n        \"price\", \"quantity\", \"status\", \"email\", \"revenue\", \"tax\",\n        \"total\", \"payment_method\", \"sales_channel\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12, na_pass=True)\n    .col_vals_between(columns=\"year\", left=2021, right=2023, na_pass=True)\n    .col_vals_gt(columns=\"price\", value=0)\n    .col_vals_gt(columns=\"quantity\", value=0)\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .col_vals_gt(columns=\"tax\", value=0)\n    .col_vals_gt(columns=\"total\", value=0)\n    .col_vals_in_set(columns=\"product_category\", set=[\n        \"Manufacturing\", \"Retail\", \"Healthcare\"\n    ])\n    .col_vals_in_set(columns=\"customer_segment\", set=[\n        \"Government\", \"Consumer\", \"SMB\"\n    ])\n    .col_vals_in_set(columns=\"region\", set=[\n        \"Asia Pacific\", \"Europe\", \"North America\"\n    ])\n    .col_vals_in_set(columns=\"status\", set=[\n        \"returned\", \"shipped\", \"delivered\"\n    ])\n    .col_vals_in_set(columns=\"payment_method\", set=[\n        \"Apple Pay\", \"PayPal\", \"Bank Transfer\", \"Credit Card\"\n    ])\n    .col_vals_in_set(columns=\"sales_channel\", set=[\n        \"Partner\", \"Distributor\", \"Phone\"\n    ])\n    .row_count_match(count=50000)\n    .col_count_match(count=20)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\n\nManaging API Keys\nWhile you can directly provide API keys as shown above, there are more secure approaches:\nimport os\n\n# Get API key from environment variable\napi_key = os.getenv(\"ANTHROPIC_API_KEY\")\n\ndraft_validation = pb.DraftValidation(\n    data=data,\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    api_key=api_key\n)\nYou can also store API keys in a .env file in your project’s root directory:\n# Contents of .env file\nANTHROPIC_API_KEY=your_anthropic_api_key_here\nOPENAI_API_KEY=your_openai_api_key_here\nIf your API keys have standard names (like ANTHROPIC_API_KEY or OPENAI_API_KEY), DraftValidation will automatically find and use them:\n# No API key needed if stored in .env with standard names\ndraft_validation = pb.DraftValidation(\n    data=data,\n    model=\"anthropic:claude-3-7-sonnet-latest\"\n)",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#example-output-for-nycflights",
    "href": "user-guide/draft-validation.html#example-output-for-nycflights",
    "title": "Draft Validation",
    "section": "Example Output for nycflights",
    "text": "Example Output for nycflights\nHere’s an example of a validation plan that might be generated by DraftValidation for the nycflights dataset:\npb.DraftValidation(\n    pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\",\n    model=\"anthropic:claude-3-7-sonnet-latest\"\n)\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"year\", \"int64\"),\n    (\"month\", \"int64\"),\n    (\"day\", \"int64\"),\n    (\"dep_time\", \"int64\"),\n    (\"sched_dep_time\", \"int64\"),\n    (\"dep_delay\", \"int64\"),\n    (\"arr_time\", \"int64\"),\n    (\"sched_arr_time\", \"int64\"),\n    (\"arr_delay\", \"int64\"),\n    (\"carrier\", \"string\"),\n    (\"flight\", \"int64\"),\n    (\"tailnum\", \"string\"),\n    (\"origin\", \"string\"),\n    (\"dest\", \"string\"),\n    (\"air_time\", \"int64\"),\n    (\"distance\", \"int64\"),\n    (\"hour\", \"int64\"),\n    (\"minute\", \"int64\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"year\", \"month\", \"day\", \"sched_dep_time\", \"carrier\", \"flight\",\n        \"origin\", \"dest\", \"distance\", \"hour\", \"minute\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12)\n    .col_vals_between(columns=\"day\", left=1, right=31)\n    .col_vals_between(columns=\"sched_dep_time\", left=106, right=2359)\n    .col_vals_between(columns=\"dep_delay\", left=-43, right=1301, na_pass=True)\n    .col_vals_between(columns=\"air_time\", left=20, right=695, na_pass=True)\n    .col_vals_between(columns=\"distance\", left=17, right=4983)\n    .col_vals_between(columns=\"hour\", left=1, right=23)\n    .col_vals_between(columns=\"minute\", left=0, right=59)\n    .col_vals_in_set(columns=\"origin\", set=[\"EWR\", \"LGA\", \"JFK\"])\n    .col_count_match(count=18)\n    .row_count_match(count=336776)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\nNotice how the generated plan includes:\n\nA schema validation with appropriate data types\nNot-null checks for required columns\nRange validations for numerical data\nSet membership checks for categorical data\nRow and column count validations\nAppropriate handling of missing values with na_pass=True",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#working-with-model-providers",
    "href": "user-guide/draft-validation.html#working-with-model-providers",
    "title": "Draft Validation",
    "section": "Working with Model Providers",
    "text": "Working with Model Providers\n\nSpecifying Models\nWhen using DraftValidation, you specify the model in the format \"provider:model_name\":\n# Using Anthropic's Claude model\npb.DraftValidation(data=data, model=\"anthropic:claude-3-7-sonnet-latest\")\n\n# Using OpenAI's GPT model\npb.DraftValidation(data=data, model=\"openai:gpt-4-turbo\")\n\n# Using a local model with Ollama\npb.DraftValidation(data=data, model=\"ollama:llama3:latest\")\n\n# Using Amazon Bedrock\npb.DraftValidation(data=data, model=\"bedrock:anthropic.claude-3-sonnet-20240229-v1:0\")\n\n\nModel Performance and Privacy\nDifferent models have different capabilities when it comes to generating validation plans:\n\nAnthropic Claude 3.7 Sonnet generally provides the most comprehensive and accurate validation plans\nOpenAI GPT-4 models also perform well\nLocal models through Ollama can be useful for private data but may have reduced capabilities\n\nA key advantage of DraftValidation is that your actual dataset is not sent to the LLM provider. Instead, only a summary is transmitted, which includes:\n\nthe number of rows and columns\ncolumn names and data types\nbasic statistics (min, max, mean, median, missing values count)\na small sample of values from each column (usually 5-10 values)\n\nThis approach protects your data while still providing enough context for the LLM to generate relevant validation rules.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#customizing-generated-plans",
    "href": "user-guide/draft-validation.html#customizing-generated-plans",
    "title": "Draft Validation",
    "section": "Customizing Generated Plans",
    "text": "Customizing Generated Plans\nThe validation plan generated by DraftValidation is just a starting point. You’ll typically want to:\n\nreview the generated code for correctness\nreplace your_data with your actual data variable name that exists in your workspace\nensure the data object referenced is actually present in your workspace\nadjust thresholds and validation parameters\nadd domain-specific validation rules\nremove any unnecessary checks\n\nFor example, you might start by capturing the text representation of your draft validation. This will give you the raw Python code that you can copy into a new code cell in your notebook or script. From there, you can customize it by modifying thresholds to match your organization’s data quality standards, adding business-specific validation rules that require domain knowledge, or removing checks that aren’t relevant to your use case. Once you’ve made your modifications, you can execute the customized validation plan as you would any other Pointblank validation.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#under-the-hood",
    "href": "user-guide/draft-validation.html#under-the-hood",
    "title": "Draft Validation",
    "section": "Under the Hood",
    "text": "Under the Hood\n\nThe Generated Data Summary\nTo understand what the LLM works with, here’s an example of the data summary format that’s sent:\n{\n  \"table_info\": {\n    \"rows\": 336776,\n    \"columns\": 18,\n    \"table_type\": \"duckdb\"\n  },\n  \"column_info\": [\n    {\n      \"column_name\": \"year\",\n      \"column_type\": \"int64\",\n      \"missing_values\": 0,\n      \"min\": 2013,\n      \"max\": 2013,\n      \"mean\": 2013.0,\n      \"median\": 2013,\n      \"sample_values\": [2013, 2013, 2013, 2013, 2013]\n    },\n    {\n      \"column_name\": \"month\",\n      \"column_type\": \"int64\",\n      \"missing_values\": 0,\n      \"min\": 1,\n      \"max\": 12,\n      \"mean\": 6.548819,\n      \"median\": 7,\n      \"sample_values\": [1, 1, 1, 1, 1]\n    },\n    // Additional columns...\n  ]\n}\n\n\nThe Prompt Strategy\nThe DraftValidation class uses a carefully crafted prompt that instructs the LLM to:\n\nuse the schema information to create a Schema object\ninclude col_vals_not_null() for columns with no missing values\nadd appropriate range validations based on min/max values\ninclude row and column count validations\nformat the output as clean, executable Python code\n\nThe prompt also contains constraints to ensure consistent, high-quality results, such as using line breaks in long lists for readability, applying na_pass=True for columns with missing values, and avoiding duplicate validations.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#best-practices-and-troubleshooting",
    "href": "user-guide/draft-validation.html#best-practices-and-troubleshooting",
    "title": "Draft Validation",
    "section": "Best Practices and Troubleshooting",
    "text": "Best Practices and Troubleshooting\n\nWhen to Use DraftValidation\nDrafting a validation is most useful when:\n\nworking with a new dataset for the first time\nneeding to quickly establish baseline validation\nexploring potential validation rules before formalizing them\nvalidating columns with consistent patterns (numeric ranges, categories, etc.)\n\nConsider writing validation plans manually when you need very specific business rules, are working with sensitive data, need complex validation logic, or need to validate relationships between columns.\n\n\nRecommended Workflow\nA recommended workflow incorporating DraftValidation:\n\ngenerate an initial plan with DraftValidation\nreview the generated validations for relevance\nadjust thresholds and parameters as needed\nadd specific business logic and cross-column validations\nstore the final validation plan in version control\n\n\n\nCommon Issues\n\nAuthentication Errors: ensure your API key is valid and correctly passed to DraftValidation\nPackage Not Found: make sure you’ve installed the required packages with pip install pointblank[generate]\nUnsupported Model: verify you’re using the correct provider:model format\nPoor Quality Plans: try a more capable model or provide better context\n\nBe aware that LLM providers typically have rate limits. If you’re generating many validation plans, consider using local models through Ollama, implementing rate limiting in your code, or caching generated plans for datasets that don’t change frequently.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/draft-validation.html#conclusion",
    "href": "user-guide/draft-validation.html#conclusion",
    "title": "Draft Validation",
    "section": "Conclusion",
    "text": "Conclusion\nDraftValidation provides a powerful way to jumpstart your data validation process by leveraging LLMs to generate context-aware validation plans. By analyzing your data’s structure and content, DraftValidation can create comprehensive validation rules that would otherwise take significant time to develop manually.\nThe feature balances privacy (by sending only data summaries) with utility (by generating executable validation code). While the generated plans should always be reviewed and refined, they provide an excellent starting point for ensuring your data meets your quality requirements.\nBy understanding how DraftValidation works and how to customize its output, you can significantly accelerate your data validation workflows and improve the quality of your data throughout your projects.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Draft Validation"
    ]
  },
  {
    "objectID": "user-guide/sundering.html",
    "href": "user-guide/sundering.html",
    "title": "Sundering Validated Data",
    "section": "",
    "text": "Sundering data? First off, let’s get the correct meaning across here. Sundering is really just splitting, dividing, cutting into two pieces. And it’s a useful thing we can do in Pointblank to any data that we are validating. When you interrogate the data, you learn about which rows have test failures within them. With more validation steps, we get an even better picture of this simply by virtue of more testing.\nThe power of sundering lies in its ability to separate your data into two distinct categories:\nThis approach allows you to:\nLet’s use the small_table in our examples to show just how sundering is done. Here’s that table:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Validated Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#a-simple-example-where-data-is-torn-asunder",
    "href": "user-guide/sundering.html#a-simple-example-where-data-is-torn-asunder",
    "title": "Sundering Validated Data",
    "section": "A Simple Example Where Data is Torn Asunder",
    "text": "A Simple Example Where Data is Torn Asunder\nWe’ll begin with a very simple validation plan, having only a single step. There will be failing test units here.\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_ge(\n        columns=\"d\",\n        value=1000\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWe see six failing test units in FAIL column of the above validation report table. There is a data extract (collection of failing rows) available. Let’s use the get_data_extracts() method to have a look at it.\n\nvalidation.get_data_extracts(i=1, frame=True)\n\n\nshape: (6, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr52016-01-09 12:36:002016-01-098\"3-ldm-038\"7283.94true\"low\"72016-01-15 18:46:002016-01-157\"1-knw-093\"3843.34true\"high\"92016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"102016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"112016-01-26 20:07:002016-01-264\"2-dmx-010\"7833.98true\"low\"122016-01-28 02:51:002016-01-282\"7-dmx-010\"8108.34false\"low\"\n\n\nThis is six rows of data that had failing test units in column d. Indeed we can see that all values in that column are less than 1000 (and we asserted that values should be greater than or equal to 1000). This is the ‘bad’ data, if you will. Using the get_sundered_data() method, we get the ‘good’ part:\n\nvalidation.get_sundered_data()\n\n\nshape: (7, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-04 11:00:002016-01-042\"1-bcd-345\"33423.29true\"high\"2016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"2016-01-05 13:32:002016-01-056\"8-kdg-938\"32343.23true\"high\"2016-01-06 17:23:002016-01-062\"5-jdo-903\"null3892.4false\"mid\"2016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"2016-01-17 11:27:002016-01-174\"5-boe-639\"21035.64false\"low\"2016-01-30 11:23:002016-01-301\"3-dka-303\"null2230.09true\"high\"\n\n\nThis is a Polars DataFrame of seven rows. All values in d were passing test units (i.e., fulfilled the expectation outlined in the validation step) and, in many ways, this is like a ‘good extract’.\nYou can always collect the failing rows with get_sundered_data() by using the type=\"fail\" option. Let’s try that here:\n\nvalidation.get_sundered_data(type=\"fail\")\n\n\nshape: (6, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-09 12:36:002016-01-098\"3-ldm-038\"7283.94true\"low\"2016-01-15 18:46:002016-01-157\"1-knw-093\"3843.34true\"high\"2016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"2016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"2016-01-26 20:07:002016-01-264\"2-dmx-010\"7833.98true\"low\"2016-01-28 02:51:002016-01-282\"7-dmx-010\"8108.34false\"low\"\n\n\nIt gives us the same rows as in the DataFrame obtained from using validation.get_data_extracts(i=1, frame=True). Two important things to know about get_sundered_data() is that the table rows returned from type=pass (the default) and type=fail are:\n\nthe sum of rows across these returned tables will be equal to that of the original table\nthe rows in each split table are mutually exclusive (i.e., you won’t find the same row in both)\n\nYou can think of sundered data as a filtered version of the original dataset based on validation results. While the simple example illustrates how this process works on a basic level, the value of the method is better seen in a slightly more complex example.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Validated Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#using-get_sundered_data-with-a-more-comprehensive-validation",
    "href": "user-guide/sundering.html#using-get_sundered_data-with-a-more-comprehensive-validation",
    "title": "Sundering Validated Data",
    "section": "Using get_sundered_data() with a More Comprehensive Validation",
    "text": "Using get_sundered_data() with a More Comprehensive Validation\nThe previous example used exactly one validation step. You’re likely to use more than that in standard practice so let’s see how get_sundered_data() works in those common situations. Here’s a validation with three steps:\n\nvalidation_2 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_ge(\n        columns=\"d\",\n        value=1000\n    )\n    .col_vals_not_null(columns=\"c\")\n    .col_vals_gt(\n        columns=\"a\",\n        value=2\n    )\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThere are quite a few failures here across the three validation steps. In the FAIL column of the validation report table, there are 12 failing test units if we were to tally them up. So if the input table has 13 rows in total, does this mean there would be one row in the table returned by get_sundered_data()? Not so:\n\nvalidation_2.get_sundered_data()\n\n\nshape: (4, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"2016-01-05 13:32:002016-01-056\"8-kdg-938\"32343.23true\"high\"2016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"2016-01-17 11:27:002016-01-174\"5-boe-639\"21035.64false\"low\"\n\n\nThere are four rows. This is because the different validation steps tested values in different columns of the table. Some of the failing test units had to have occurred in more than once in certain rows. The rows that didn’t have any failing test units across the three different tests (in three different columns) are the ones seen above. This brings us to the third important thing about the sundering process:\n\nthe absence of test-unit failures in a row across all validation steps means those rows are returned as the ‘passing’ set, all others are placed in the ‘failing’ set\n\nIn validations where many validation steps are used, we can be more confident about the level of data quality for those rows returned in the passing set. But not every type of validation step is considered within this splitting procedure. The next section will explain the rules on that.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Validated Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#the-validation-methods-considered-when-sundering",
    "href": "user-guide/sundering.html#the-validation-methods-considered-when-sundering",
    "title": "Sundering Validated Data",
    "section": "The Validation Methods Considered When Sundering",
    "text": "The Validation Methods Considered When Sundering\nThe sundering procedure relies on row-level validation types to be used. This makes sense as it’s impossible to judge the quality of a row when using the col_exists() validation method, for example. Luckily, we have many row-level validation methods; here’s a list:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\nrows_distinct()\nrows_complete()\nconjointly()\n\nThis is the same list of validation methods that are considered when creating data extracts.\nThere are some additional caveats though. Even if using a validation method drawn from the set above, the validation step won’t be used for sundering if:\n\nthe active= parameter for that step has been set to False\nthe pre= parameter has been used\n\nThe first one makes intuitive sense (you decided to skip this validation step entirely), the second one requires some explanation. Using pre= allows you to modify the target table, there’s no easy or practical way to compare rows in a mutated table compared to the original table (e.g., a mutation may drastically reduce the number of rows).",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Validated Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#practical-applications-of-sundering",
    "href": "user-guide/sundering.html#practical-applications-of-sundering",
    "title": "Sundering Validated Data",
    "section": "Practical Applications of Sundering",
    "text": "Practical Applications of Sundering\n1. Creating Clean Datasets for Analysis\nOne of the most common use cases for sundering is preparing validated data for downstream analysis:\n\n# Comprehensive validation for analysis-ready data\nanalysis_validation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_not_null(columns=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])  # No missing values\n    .col_vals_gt(columns=\"a\", value=0)                          # Positive values only\n    .col_vals_lt(columns=\"d\", value=10000)                      # No extreme outliers\n    .interrogate()\n)\n\n# Extract only the clean data that passed all checks\nclean_data = analysis_validation.get_sundered_data(type=\"pass\")\n\n# Use the clean data for your analysis\npb.preview(clean_data)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows11Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    5\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    8\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    11\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\nThis approach ensures that any subsequent analysis is based on data that meets your quality standards, reducing the risk of misleading results or spurious conclusions due to problematic records. By making validation an explicit step in your analytical workflow, you create a natural quality gate that prevents invalid data from influencing your findings.\n2. Creating Parallel Workflows for Clean and Problematic Data\nYou can use sundering to create parallel processing paths:\n\n# Get both clean and problematic data\nclean_data = analysis_validation.get_sundered_data(type=\"pass\")\nproblem_data = analysis_validation.get_sundered_data(type=\"fail\")\n\n# Process clean data (in real applications, you'd do more here)\nprint(f\"Clean data size: {len(clean_data)} rows\")\n\n# Log problematic data for investigation\nprint(f\"Problematic data size: {len(problem_data)} rows\")\n\nClean data size: 11 rows\nProblematic data size: 2 rows\n\n\nThis approach enables you to build robust data processing pathways with separate handling for clean and problematic data. In production environments, you could save problematic records to a separate location for further investigation, generate detailed logs of validation failures, and trigger automated notifications to data stewards when issues arise. By establishing clear protocols for handling both data streams, you create a systematic approach to data quality that balances immediate analytical needs with longer-term data improvement goals.\n3. Data Quality Monitoring and Improvement\nTracking the ratio of passing to failing rows over time can help monitor data quality trends:\n\n# Calculate data quality metrics\ntotal_rows = len(pb.load_dataset(dataset=\"small_table\"))\npassing_rows = len(clean_data)\nquality_score = passing_rows / total_rows\n\nprint(f\"Data quality score: {quality_score:.2%}\")\nprint(f\"Passing rows: {passing_rows} out of {total_rows}\")\n\nData quality score: 84.62%\nPassing rows: 11 out of 13\n\n\nBy tracking these metrics over time, you can measure the impact of your data quality improvement efforts and communicate progress to stakeholders. This approach transforms sundering from a one-time filtering tool into an ongoing data quality management system, where improving the ratio of passing rows becomes a measurable business objective aligned with broader data governance goals.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Validated Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#best-practices-for-using-sundered-data",
    "href": "user-guide/sundering.html#best-practices-for-using-sundered-data",
    "title": "Sundering Validated Data",
    "section": "Best Practices for Using Sundered Data",
    "text": "Best Practices for Using Sundered Data\nWhen incorporating data sundering into your workflow, consider these best practices:\n\nBe comprehensive in your validation: the more validation steps you include (assuming they’re meaningful), the more confidence you can have in your passing dataset\nDocument your validation criteria: when sharing sundered data with others, always document the criteria used to determine passing rows\nConsider traceability: for audit purposes, it may be valuable to add a column indicating whether a record was originally in the passing or failing set\nBalance strictness and practicality: if you’re too strict with validation rules, you might end up with very few passing rows; consider the appropriate level of strictness for your use case\nUse sundering as part of a pipeline: automate the process of validation, sundering, and subsequent handling of the two resulting datasets\nContinually refine validation rules: as you learn more about your data and domain, update your validation rules to improve the accuracy of your sundering process\n\nBy following these best practices, data scientists and engineers can transform sundering from a simple utility into a strategic component of their data quality framework. When implemented thoughtfully, sundering enables a shift from reactive data cleaning to proactive quality management, where validation criteria evolve alongside your understanding of the data.\nThe ultimate goal isn’t just to separate good data from bad, but to gradually improve your entire dataset over time by addressing the root causes of validation failures that appear in the failing set. This approach turns data validation from a gatekeeper function into a continuous improvement process.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Validated Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#conclusion",
    "href": "user-guide/sundering.html#conclusion",
    "title": "Sundering Validated Data",
    "section": "Conclusion",
    "text": "Conclusion\nData sundering provides a powerful way to separate your data based on validation results. While the concept is simple (splitting data into passing and failing sets) the feature can very useful in many data workflows. By integrating sundering into your data pipeline, you can:\n\nensure that downstream analysis only works with validated data\ncreate focused datasets for different purposes\nimprove overall data quality through systematic identification and isolation of problematic records\nbuild more robust data pipelines that explicitly handle data quality issues\n\nSo long as you’re aware of the rules and limitations of sundering, you’re likely to find it to be a simple and useful way to filter your input table on the basis of a validation plan, turning data validation from a passive reporting tool into an active component of your data processing workflow.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Validated Data"
    ]
  },
  {
    "objectID": "user-guide/validation-methods.html",
    "href": "user-guide/validation-methods.html",
    "title": "Validation Methods",
    "section": "",
    "text": "Pointblank provides a comprehensive suite of validation methods to verify different aspects of your data. Each method creates a validation step that becomes part of your validation plan.\nThese validation methods cover everything from checking column values against thresholds to validating the table structure and detecting duplicates. Combined into validation steps, they form the foundation of your data quality workflow.\nPointblank provides over 20 validation methods to handle diverse data quality requirements. These are grouped into three main categories:\nWithin each of these categories, we’ll walk through several examples showing how each validation method creates steps in your validation plan.\nAnd we’ll use the small_table dataset for all of our examples. Here’s a preview of it:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Validation Methods"
    ]
  },
  {
    "objectID": "user-guide/validation-methods.html#validation-methods-to-validation-steps",
    "href": "user-guide/validation-methods.html#validation-methods-to-validation-steps",
    "title": "Validation Methods",
    "section": "Validation Methods to Validation Steps",
    "text": "Validation Methods to Validation Steps\nIn Pointblank, validation methods become validation steps when you add them to a validation plan. Each method creates a distinct step that performs a specific check on your data.\nHere’s a simple example showing how three validation methods create three validation steps:\n\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n\n    # Step 1: Check that values in column `a` are greater than 2 ---\n    .col_vals_gt(columns=\"a\", value=2, brief=\"Values in 'a' must exceed 2.\")\n\n    # Step 2: Check that column 'date' exists in the table ---\n    .col_exists(columns=\"date\", brief=\"Column 'date' must exist.\")\n\n    # Step 3: Check that the table has exactly 13 rows ---\n    .row_count_match(count=13, brief=\"Table should have exactly 13 rows.\")\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Values in 'a' must exceed 2.\n\n        \n    a\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        Column 'date' must exist.\n\n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        Table should have exactly 13 rows.\n\n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nEach validation method produces one step in the validation report above. When combined, these steps form a complete validation plan that systematically checks different aspects of your data quality.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Validation Methods"
    ]
  },
  {
    "objectID": "user-guide/validation-methods.html#common-arguments",
    "href": "user-guide/validation-methods.html#common-arguments",
    "title": "Validation Methods",
    "section": "Common Arguments",
    "text": "Common Arguments\nMost validation methods in Pointblank share a set of common arguments that provide consistency and flexibility across different validation types:\n\ncolumns=: specifies which column(s) to validate (used in column-based validations)\npre=: allows data transformation before validation\nsegments=: enables validation across different data subsets\nthresholds=: sets acceptable failure thresholds\nactions=: defines actions to take when validations fail\nbrief=: provides a description of what the validation is checking\nactive=: determines if the validation step should be executed (default is True)\nna_pass=: controls how missing values are handled (only for column value validation methods)\n\nFor column validation methods, the na_pass= parameter determines whether missing values (NULL/None/NA) should pass validation (this parameter is covered in a later section).\nThese arguments follow a consistent pattern across validation methods, so you don’t need to memorize different parameter sets for each function. This systematic approach makes Pointblank more intuitive to work with as you build increasingly complex validation plans.\nWe’ll cover most of these common arguments in their own dedicated sections later in the User Guide, as some of them represent a deeper topic worthy of focused attention.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Validation Methods"
    ]
  },
  {
    "objectID": "user-guide/validation-methods.html#column-value-validations",
    "href": "user-guide/validation-methods.html#column-value-validations",
    "title": "Validation Methods",
    "section": "1. Column Value Validations",
    "text": "1. Column Value Validations\nThese methods check individual values within columns against specific criteria:\n\nComparison checks (col_vals_gt(), col_vals_lt(), etc.) for comparing values to thresholds or other columns\nRange checks (col_vals_between(), col_vals_outside()) for verifying values fall within or outside specific ranges\nSet membership checks (col_vals_in_set(), col_vals_not_in_set()) for validating values against predefined sets\nNull value checks (col_vals_null(), col_vals_not_null()) for testing presence or absence of null values\nPattern matching checks (col_vals_regex()) for validating text patterns with regular expressions\nCustom expression checks (col_vals_expr()) for complex validations using custom expressions\n\nNow let’s look at some key examples from select categories of column value validations.\n\nComparison Checks\nLet’s start with a simple example of how col_vals_gt() might be used to check if the values in a column are greater than a specified value.\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_gt(columns=\"a\", value=5)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    30.23\n    100.77\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nIf you’re checking data in a column that contains Null/None/NA values and you’d like to disregard those values (i.e., let them pass validation), you can use na_pass=True. The following example checks values in column c of small_table, which contains two None values:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_le(columns=\"c\", value=10, na_pass=True)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above validation table, we see that all test units passed. If we didn’t use na_pass=True there would be 2 failing test units, one for each None value in the c column.\nIt’s possible to check against column values against values in an adjacent column. To do this, supply the value= argument with the column name within the col() helper function. Here’s an example of that:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_lt(columns=\"a\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThis validation checks that values in column a are less than values in column c.\n\n\nChecking of Missing Values\nA very common thing to validate is that there are no Null/NA/missing values in a column. The col_vals_not_null() method checks for the presence of missing values:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_not_null(columns=\"a\")\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nColumn a has no missing values and the above validation proves this.\n\n\nChecking Strings with Regexes\nA regular expression (regex) validation via the col_vals_regex() validation method checks if values in a column match a specified pattern. Here’s an example with two validation steps, each checking text values in a column:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_regex(columns=\"b\", pattern=r\"^\\d-[a-z]{3}-\\d{3}$\")\n    .col_vals_regex(columns=\"f\", pattern=r\"high|low|mid\")\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^\\d-[a-z]{3}-\\d{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    f\n    high|low|mid\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\n\n\nHandling Missing Values with na_pass=\nWhen validating columns containing Null/None/NA values, you can control how these missing values are treated with the na_pass= parameter:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_le(columns=\"c\", value=10, na_pass=True)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example, column c contains two None values, but all test units pass because we set na_pass=True. Without this setting, those two values would fail the validation.\nIn summary, na_pass= works like this:\n\nna_pass=True: missing values pass validation regardless of the condition being tested\nna_pass=False (the default): missing values fail validation",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Validation Methods"
    ]
  },
  {
    "objectID": "user-guide/validation-methods.html#row-based-validations",
    "href": "user-guide/validation-methods.html#row-based-validations",
    "title": "Validation Methods",
    "section": "2. Row-based Validations",
    "text": "2. Row-based Validations\nRow-based validations focus on examining properties that span across entire rows rather than individual columns. These are essential for detecting issues that can’t be found by looking at columns in isolation:\n\nrows_distinct(): ensures no duplicate rows exist in the table\nrows_complete(): verifies that no rows contain any missing values\n\nThese row-level validations are particularly valuable for ensuring data integrity and completeness at the record level, which is crucial for many analytical and operational data applications.\n\nChecking Row Distinctness\nHere’s an example where we check for duplicate rows with rows_distinct():\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .rows_distinct()\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWe can also adapt the rows_distinct() check to use a single column or a subset of columns. To do that, we need to use the columns_subset= parameter. Here’s an example of that:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .rows_distinct(columns_subset=\"b\")\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\n\n\nChecking Row Completeness\nAnother important validation is checking for complete rows: rows that have no missing values across all columns or a specified subset of columns. The rows_complete() validation method performs this check.\nHere’s an example checking if all rows in the table are complete (have no missing values in any column):\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .rows_complete()\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_complete\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            rows_complete()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nAs the report indicates, there are some incomplete rows in the table.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Validation Methods"
    ]
  },
  {
    "objectID": "user-guide/validation-methods.html#table-structure-validations",
    "href": "user-guide/validation-methods.html#table-structure-validations",
    "title": "Validation Methods",
    "section": "3. Table Structure Validations",
    "text": "3. Table Structure Validations\nTable structure validations ensure that the overall architecture of your data meets expectations. These structural checks form a foundation for more detailed data quality assessments:\n\ncol_exists(): verifies a column exists in the table\ncol_schema_match(): ensures table matches a defined schema\ncol_count_match(): confirms the table has the expected number of columns\nrow_count_match(): verifies the table has the expected number of rows\n\nThese structural validations provide essential checks on the fundamental organization of your data tables, ensuring they have the expected dimensions and components needed for reliable data analysis.\n\nChecking Column Presence\nIf you need to check for the presence of individual columns, the col_exists() validation method is useful. In this example, we check whether the date column is present in the table:\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_exists(columns=\"date\")\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThat column is present, so the single test unit of this validation step is a passing one.\n\n\nChecking the Table Schema\nFor deeper checks of table structure, a schema validation can be performed with the col_schema_match() validation method, where the goal is to check whether the structure of a table matches an expected schema. To define an expected table schema, we need to use the Schema class. Here is a simple example that (1) prepares a schema consisting of column names, (2) using that schema object in a col_schema_match() validation step:\n\nschema = pb.Schema(columns=[\"date_time\", \"date\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe col_schema_match() validation step will only have a single test unit (signifying pass or fail). We can see in the above validation report that the column schema validation passed.\nMore often, a schema will be defined using column names and column types. We can do that by using a list of tuples in the columns= parameter of Schema. Here’s an example of that approach in action:\n\nschema = pb.Schema(\n    columns=[\n        (\"date_time\", \"Datetime(time_unit='us', time_zone=None)\"),\n        (\"date\", \"Date\"),\n        (\"a\", \"Int64\"),\n        (\"b\", \"String\"),\n        (\"c\", \"Int64\"),\n        (\"d\", \"Float64\"),\n        (\"e\", \"Boolean\"),\n        (\"f\", \"String\"),\n    ]\n)\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe col_schema_match() validation method has several boolean parameters for making the checks less stringent:\n\ncomplete=: requires exact column matching (all expected columns must exist, no extra columns allowed)\nin_order=: enforces that columns appear in the same order as defined in the schema\ncase_sensitive_colnames=: column names must match with exact letter case\ncase_sensitive_dtypes=: data type strings must match with exact letter case\n\nThese parameters all default to True, providing strict schema validation. Setting any to False relaxes the validation requirements, making the checks more flexible when exact matching isn’t necessary or practical for your use case.\n\n\nChecking Counts of Row and Columns\nRow and column count validations check the number of rows and columns in a table.\nUsing row_count_match() checks whether the number of rows in a table matches a specified count.\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .row_count_match(count=13)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe col_count_match() validation method checks if the number of columns in a table matches a specified count.\n\n(\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_count_match(count=8)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    8\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nExpectations on column and row counts can be useful in certain situations and they align nicely with schema checks.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Validation Methods"
    ]
  },
  {
    "objectID": "user-guide/validation-methods.html#conclusion",
    "href": "user-guide/validation-methods.html#conclusion",
    "title": "Validation Methods",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we’ve explored the various types of validation methods that Pointblank offers for ensuring data quality. These methods provide a framework for validating column values, checking row properties, and verifying table structures. By combining these validation methods into comprehensive plans, you can systematically test your data against business rules and quality expectations. And this all helps to ensure your data remains reliable and trustworthy.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Validation Methods"
    ]
  },
  {
    "objectID": "user-guide/briefs.html",
    "href": "user-guide/briefs.html",
    "title": "Briefs",
    "section": "",
    "text": "When validating data with Pointblank, it’s often helpful to have descriptive labels for each validation step. This is where briefs come in. A brief is a short description of what a validation step is checking and it appears in the STEP column of the validation report table. Briefs make your validation reports more readable and they help others understand what each step is verifying without needing to look at the code.\nBriefs can be set in two ways:\nUnderstanding these two approaches to adding briefs gives you flexibility in how you document your validation process. Global briefs provide consistency across all steps and save time when you want similar descriptions throughout, while step-level briefs allow for precise customization when specific validations need more detailed or unique explanations. In practice, many validation workflows will combine both approaches (i.e., setting a useful global brief template while overriding it for steps that require special attention).",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#global-briefs",
    "href": "user-guide/briefs.html#global-briefs",
    "title": "Briefs",
    "section": "Global Briefs",
    "text": "Global Briefs\nTo set a global brief that applies to all validation steps, use the Validate(brief=) parameter when creating a Validate object:\n\nimport pointblank as pb\nimport polars as pl\n\n# Sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"value\": [10, 20, 30, 40, 50],\n    \"category\": [\"A\", \"B\", \"C\", \"A\", \"B\"]\n})\n\n# Create a validation with a global brief\n(\n    pb.Validate(\n        data=data,\n        tbl_name=\"example_data\",\n        brief=\"Step {step}: {auto}\"  # Global brief template\n    )\n    .col_vals_gt(\n        columns=\"value\",\n        value=5\n    )\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"]\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:39Polarsexample_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Step 1: Expect that values in value should be &gt; 5.\n\n        \n    value\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        Step 2: Expect that values in category should be in the set of A, B, C.\n\n        \n    category\n    A, B, C\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example, every validation step will have a brief description that follows the pattern \"Step X: [auto-generated description]\".\nThis is a simple example of template-based briefs. Later in this guide, we’ll explore the full range of templating elements available for creating custom brief descriptions that precisely communicate what each validation step is checking.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#step-level-briefs",
    "href": "user-guide/briefs.html#step-level-briefs",
    "title": "Briefs",
    "section": "Step-level Briefs",
    "text": "Step-level Briefs\nYou can also set briefs for individual validation steps:\n\n(\n    pb.Validate(data=data, tbl_name=\"example_data\")\n    .col_vals_gt(\n        columns=\"value\",\n        value=5,\n        brief=\"Check if values exceed minimum threshold of 5\"\n    )\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"],\n        brief=\"Verify categories are valid\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:39Polarsexample_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Check if values exceed minimum threshold of 5\n\n        \n    value\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        Verify categories are valid\n\n        \n    category\n    A, B, C\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nLocal briefs override any global briefs that might be set.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#brief-templating",
    "href": "user-guide/briefs.html#brief-templating",
    "title": "Briefs",
    "section": "Brief Templating",
    "text": "Brief Templating\nBriefs support templating elements that get replaced with specific values:\n\n{auto}: an auto-generated description of the validation\n{step}: the step number in the validation plan\n{col}: the column name(s) being validated\n{value}: the comparison value used in the validation (when applicable)\n{thresholds}: a short summary of thresholds levels set (or unset) for the step\n{segment}, {segment_column}, {segment_value}: information on the step’s segment\n\nHere’s how to use these templates:\n\n(\n    pb.Validate(data=data, tbl_name=\"example_data\")\n    .col_vals_gt(\n        columns=\"value\",\n        value=5,\n        brief=\"Step {step}: Checking column '{col}' for values `&gt; 5`\"\n    )\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"],\n        brief=\"{auto} **(Step {step})**\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:39Polarsexample_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Step 1: Checking column 'value' for values &gt; 5\n\n        \n    value\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        Expect that values in category should be in the set of A, B, C. (Step 2)\n\n        \n    category\n    A, B, C\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThese template elements make briefs highly flexible and customizable. You can combine multiple templating elements in a single brief to create descriptive yet concise validation step descriptions. The templates help maintain consistency across your validation reports while providing enough detail to understand what each step is checking.\nNote that not all templating elements will be relevant for every validation step. For instance, {value} is only applicable to validation functions that hold a comparison value like col_vals_gt(). If you include a templating element that isn’t relevant to a particular step, it will not be replaced with a corresponding value.\nBriefs support the use of Markdown formatting, allowing you to add emphasis with bold or italic text, include inline code formatting, or other Markdown elements to make your briefs more visually distinctive and informative. This can be especially helpful when you want certain parts of your briefs to stand out in the validation report.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#automatic-briefs",
    "href": "user-guide/briefs.html#automatic-briefs",
    "title": "Briefs",
    "section": "Automatic Briefs",
    "text": "Automatic Briefs\nIf you want Pointblank to generate briefs for you automatically, you can set brief=True. Here, we’ll make that setting at the global level (by using Validate(brief=True)):\n\n(\n    pb.Validate(\n        data=data,\n        tbl_name=\"example_data\",\n        brief=True\n    )\n    .col_vals_gt(\n        columns=\"value\",\n        value=5\n    )\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"]\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:39Polarsexample_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in value should be &gt; 5.\n\n        \n    value\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        Expect that values in category should be in the set of A, B, C.\n\n        \n    category\n    A, B, C\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nAutomatic briefs are descriptive and include information about what’s being validated, including the column names and the validation conditions.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#briefs-localized-to-a-specified-language",
    "href": "user-guide/briefs.html#briefs-localized-to-a-specified-language",
    "title": "Briefs",
    "section": "Briefs Localized to a Specified Language",
    "text": "Briefs Localized to a Specified Language\nWhen using the lang= parameter in Validate(), automatically generated briefs will be created in the specified language (along with other elements of the validation report table):\n\n(\n    pb.Validate(\n        data=data,\n        tbl_name=\"example_data\",\n        lang=\"es\",  # Spanish\n        brief=True  # Auto-generate all briefs in Spanish\n    )\n    .col_vals_gt(\n        columns=\"value\",\n        value=5\n    )\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"]\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Validación de Pointblank\n  \n  \n    2025-05-23|02:19:39Polarsexample_data\n  \n\n  \n  \n  PASO\n  COLUMNAS\n  VALORES\n  TBL\n  EVAL\n  UNID.\n  PASA\n  FALLO\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Se espera que los valores en value sean &gt; 5.\n\n        \n    value\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51,00\n    00,00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        Se espera que los valores en category estén en el conjunto de A, B, C.\n\n        \n    category\n    A, B, C\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51,00\n    00,00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nWhen using the lang= parameter in combination with the {auto} templating element, the auto-generated portion of the brief will also be translated to the specified language. This makes it possible to create fully localized validation reports where both custom text and auto-generated descriptions appear in the same language.\nPointblank supports several languages for localized briefs, including French (\"fr\"), German (\"de\"), Spanish (\"es\"), Italian (\"it\"), and Portuguese (\"pt\"). For the complete list of supported languages, refer to the Validate() documentation.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#disabling-briefs",
    "href": "user-guide/briefs.html#disabling-briefs",
    "title": "Briefs",
    "section": "Disabling Briefs",
    "text": "Disabling Briefs\nIf you’ve set a global brief but want to disable it for specific validation steps, you can set brief=False:\n\n(\n    pb.Validate(\n        data=data,\n        tbl_name=\"example_data\",\n        brief=\"Step {step}: {auto}\"  # Global brief template\n    )\n    .col_vals_gt(columns=\"value\", value=5)  # Uses global brief\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"],\n        brief=False  # No brief for this step\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:39Polarsexample_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Step 1: Expect that values in value should be &gt; 5.\n\n        \n    value\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    category\n    A, B, C\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    51.00\n    00.00\n    —\n    —\n    —\n    —",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#practical-example-comprehensive-validation-with-briefs",
    "href": "user-guide/briefs.html#practical-example-comprehensive-validation-with-briefs",
    "title": "Briefs",
    "section": "Practical Example: Comprehensive Validation with Briefs",
    "text": "Practical Example: Comprehensive Validation with Briefs\nIn real-world data validation scenarios, you’ll likely work with more complex datasets and apply various types of validation checks. This final example brings together many of the brief-generating techniques we’ve covered, showing how you can mix different approaches in a single validation workflow.\n\n# Create a more complex dataset\ncomplex_data = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5, 6, 7, 8],\n    \"value\": [10, 20, 30, 40, 50, 60, 70, 80],\n    \"ratio\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n    \"category\": [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\"],\n    \"date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\",\n             \"2023-01-05\", \"2023-01-06\", \"2023-01-07\", \"2023-01-08\"]\n})\n\n# Create a comprehensive validation\n(\n    pb.Validate(\n        data=complex_data,\n        tbl_name=\"complex_data\",\n        label=\"Complex Data Validation\"\n    )\n    .col_vals_gt(\n        columns=\"value\",\n        value=0,\n        brief=\"All values must be positive.\"\n    )\n    .col_vals_between(\n        columns=\"ratio\",\n        left=0,\n        right=1,\n        brief=\"**Step {step}**: Ratios should be between `0` and `1`.\"\n    )\n    .col_vals_in_set(\n        columns=\"category\",\n        set=[\"A\", \"B\", \"C\"],\n        brief=True  # Auto-generate brief\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Complex Data ValidationPolarscomplex_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        All values must be positive.\n\n        \n    value\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    8\n    81.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        Step 2: Ratios should be between 0 and 1.\n\n        \n    ratio\n    [0, 1]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    8\n    81.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        Expect that values in category should be in the set of A, B, C.\n\n        \n    category\n    A, B, C\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    8\n    81.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe example above demonstrates:\n\nplaintext briefs with direct messages\ntemplate-based briefs with Markdown formatting\nautomatically generated briefs (brief=True)\n\nBy combining these different brief styles, you can create validation reports that are informative, consistent, and tailored to your specific data quality requirements.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#best-practices-for-using-briefs",
    "href": "user-guide/briefs.html#best-practices-for-using-briefs",
    "title": "Briefs",
    "section": "Best Practices for Using Briefs",
    "text": "Best Practices for Using Briefs\nWell-crafted briefs can significantly enhance the readability and usefulness of your validation reports. Here are some guidelines to follow:\n\nBe concise: briefs should be short and to the point; they’re meant to quickly communicate the purpose of a validation step\nBe specific: include relevant details or conditions that make the validation meaningful\nUse templates consistently: if you’re using template elements like \"{step}\" or \"{col}\", try to use them consistently across all briefs for a cleaner look\nUse auto-generated briefs as a starting point: you can start with Validate(brief=True) to see what Pointblank generates automatically, then customize as needed\nAdd custom briefs for complex validations: custom briefs are especially useful for complex validations where the purpose might not be immediately obvious from the code\n\nFollowing these best practices will help ensure your validation reports are easy to understand for everyone who needs to review them.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/briefs.html#conclusion",
    "href": "user-guide/briefs.html#conclusion",
    "title": "Briefs",
    "section": "Conclusion",
    "text": "Conclusion\nBriefs help make validation reports more readable and understandable. By using global briefs, step-level briefs, or a combination of both, you can create validation reports that clearly communicate what each validation step is checking.\nWhether you want automatically generated descriptions or precisely tailored custom messages, the brief system provides the flexibility to make your data validation work more transparent and easier to interpret for all stakeholders.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Briefs"
    ]
  },
  {
    "objectID": "user-guide/installation.html",
    "href": "user-guide/installation.html",
    "title": "Installation",
    "section": "",
    "text": "Pointblank can be installed using various package managers. The base installation gives you the core validation functionality, with optional dependencies for working with different data sources.\n\n\nYou can install Pointblank using your preferred package manager:\n\npipuvconda\n\n\npip install pointblank\n\n\nuv pip install pointblank\n\n\nconda install -c conda-forge pointblank\n\n\n\n\n\n\nPointblank requires a DataFrame library but doesn’t include one by default, giving you the flexibility to choose either Pandas or Polars:\n\nPolarsPandas\n\n\n# Using pip\npip install pointblank[pl]\n\n# Or manually\npip install polars&gt;=1.24.0\n\n\n# Using pip\npip install pointblank[pd]\n\n# Or manually\npip install pandas&gt;=2.2.3\n\n\n\nPointblank works seamlessly with both libraries, and you can choose the one that best fits your workflow and performance requirements.\n\n\n\n\n\nTo work with various database systems through Ibis, you can install additional backends:\n\npipuvconda\n\n\npip install pointblank[sqlite]      # SQLite\npip install pointblank[duckdb]      # DuckDB\npip install pointblank[postgres]    # PostgreSQL\npip install pointblank[mysql]       # MySQL\npip install pointblank[mssql]       # Microsoft SQL Server\npip install pointblank[bigquery]    # BigQuery\npip install pointblank[pyspark]     # Apache Spark\npip install pointblank[databricks]  # Databricks\npip install pointblank[snowflake]   # Snowflake\n\n# Example of installing multiple backends\npip install pointblank[duckdb,postgres,sqlite]\n\n\nuv pip install pointblank[sqlite]      # SQLite\nuv pip install pointblank[duckdb]      # DuckDB\nuv pip install pointblank[postgres]    # PostgreSQL\nuv pip install pointblank[mysql]       # MySQL\nuv pip install pointblank[mssql]       # Microsoft SQL Server\nuv pip install pointblank[bigquery]    # BigQuery\nuv pip install pointblank[pyspark]     # Apache Spark\nuv pip install pointblank[databricks]  # Databricks\nuv pip install pointblank[snowflake]   # Snowflake\n\n# Example of installing multiple backends\nuv pip install pointblank[duckdb,postgres,sqlite]\n\n\nconda install -c conda-forge pointblank-sqlite      # SQLite\nconda install -c conda-forge pointblank-duckdb      # DuckDB\nconda install -c conda-forge pointblank-postgres    # PostgreSQL\nconda install -c conda-forge pointblank-mysql       # MySQL\nconda install -c conda-forge pointblank-mssql       # Microsoft SQL Server\nconda install -c conda-forge pointblank-bigquery    # BigQuery\nconda install -c conda-forge pointblank-pyspark     # Apache Spark\nconda install -c conda-forge pointblank-databricks  # Databricks\nconda install -c conda-forge pointblank-snowflake   # Snowflake\n\n# Example of installing multiple backends\nconda install -c conda-forge pointblank-duckdb pointblank-postgres pointblank-sqlite\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven when using exclusively Ibis backends, you still need either Pandas or Polars installed since Pointblank’s reporting functionality (powered by Great Tables) requires a DataFrame library for rendering tabular reporting results.\n\n\n\n\n\nPointblank includes experimental support for AI-assisted validation plan generation:\npip install pointblank[generate]\nThis installs the necessary dependencies for working with LLM providers to help generate validation plans. See the Draft Validation article for how to create validation plans from existing data.\n\n\n\nIf you want the latest development version with the newest features, you can install directly from GitHub:\npip install git+https://github.com/posit-dev/pointblank.git\n\n\n\n\nYou can verify your installation by importing Pointblank and checking the version:\nimport pointblank as pb\nprint(pb.__version__)\n\n\n\n\nPython 3.10 or higher\nA supported DataFrame library (Pandas or Polars)\nOptional: Ibis (for database connectivity)\n\n\n\n\nNow that you’ve installed Pointblank, you’re ready to start validating your data. If you haven’t read the Introduction yet, consider starting there to learn the basic concepts.\nIf you encounter any installation issues, please open an issue on GitHub with details about your system and the specific error messages you’re seeing. The maintainers actively monitor these issues and can help troubleshoot problems.\nFor a quick test of your installation, try running a simple validation:\nimport pointblank as pb\n\n# Load a small dataset\ndata = pb.load_dataset(\"small_table\")\n\n# Create a simple validation\nvalidation = (\n    pb.Validate(data=data)\n    .col_exists(columns=[\"a\", \"b\", \"c\"])\n    .interrogate()\n)\n\n# Display the validation results\nvalidation",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "user-guide/installation.html#basic-installation",
    "href": "user-guide/installation.html#basic-installation",
    "title": "Installation",
    "section": "",
    "text": "You can install Pointblank using your preferred package manager:\n\npipuvconda\n\n\npip install pointblank\n\n\nuv pip install pointblank\n\n\nconda install -c conda-forge pointblank",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "user-guide/installation.html#dataframe-libraries",
    "href": "user-guide/installation.html#dataframe-libraries",
    "title": "Installation",
    "section": "",
    "text": "Pointblank requires a DataFrame library but doesn’t include one by default, giving you the flexibility to choose either Pandas or Polars:\n\nPolarsPandas\n\n\n# Using pip\npip install pointblank[pl]\n\n# Or manually\npip install polars&gt;=1.24.0\n\n\n# Using pip\npip install pointblank[pd]\n\n# Or manually\npip install pandas&gt;=2.2.3\n\n\n\nPointblank works seamlessly with both libraries, and you can choose the one that best fits your workflow and performance requirements.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "user-guide/installation.html#optional-dependencies",
    "href": "user-guide/installation.html#optional-dependencies",
    "title": "Installation",
    "section": "",
    "text": "To work with various database systems through Ibis, you can install additional backends:\n\npipuvconda\n\n\npip install pointblank[sqlite]      # SQLite\npip install pointblank[duckdb]      # DuckDB\npip install pointblank[postgres]    # PostgreSQL\npip install pointblank[mysql]       # MySQL\npip install pointblank[mssql]       # Microsoft SQL Server\npip install pointblank[bigquery]    # BigQuery\npip install pointblank[pyspark]     # Apache Spark\npip install pointblank[databricks]  # Databricks\npip install pointblank[snowflake]   # Snowflake\n\n# Example of installing multiple backends\npip install pointblank[duckdb,postgres,sqlite]\n\n\nuv pip install pointblank[sqlite]      # SQLite\nuv pip install pointblank[duckdb]      # DuckDB\nuv pip install pointblank[postgres]    # PostgreSQL\nuv pip install pointblank[mysql]       # MySQL\nuv pip install pointblank[mssql]       # Microsoft SQL Server\nuv pip install pointblank[bigquery]    # BigQuery\nuv pip install pointblank[pyspark]     # Apache Spark\nuv pip install pointblank[databricks]  # Databricks\nuv pip install pointblank[snowflake]   # Snowflake\n\n# Example of installing multiple backends\nuv pip install pointblank[duckdb,postgres,sqlite]\n\n\nconda install -c conda-forge pointblank-sqlite      # SQLite\nconda install -c conda-forge pointblank-duckdb      # DuckDB\nconda install -c conda-forge pointblank-postgres    # PostgreSQL\nconda install -c conda-forge pointblank-mysql       # MySQL\nconda install -c conda-forge pointblank-mssql       # Microsoft SQL Server\nconda install -c conda-forge pointblank-bigquery    # BigQuery\nconda install -c conda-forge pointblank-pyspark     # Apache Spark\nconda install -c conda-forge pointblank-databricks  # Databricks\nconda install -c conda-forge pointblank-snowflake   # Snowflake\n\n# Example of installing multiple backends\nconda install -c conda-forge pointblank-duckdb pointblank-postgres pointblank-sqlite\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven when using exclusively Ibis backends, you still need either Pandas or Polars installed since Pointblank’s reporting functionality (powered by Great Tables) requires a DataFrame library for rendering tabular reporting results.\n\n\n\n\n\nPointblank includes experimental support for AI-assisted validation plan generation:\npip install pointblank[generate]\nThis installs the necessary dependencies for working with LLM providers to help generate validation plans. See the Draft Validation article for how to create validation plans from existing data.\n\n\n\nIf you want the latest development version with the newest features, you can install directly from GitHub:\npip install git+https://github.com/posit-dev/pointblank.git",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "user-guide/installation.html#verifying-your-installation",
    "href": "user-guide/installation.html#verifying-your-installation",
    "title": "Installation",
    "section": "",
    "text": "You can verify your installation by importing Pointblank and checking the version:\nimport pointblank as pb\nprint(pb.__version__)",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "user-guide/installation.html#system-requirements",
    "href": "user-guide/installation.html#system-requirements",
    "title": "Installation",
    "section": "",
    "text": "Python 3.10 or higher\nA supported DataFrame library (Pandas or Polars)\nOptional: Ibis (for database connectivity)",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "user-guide/installation.html#next-steps",
    "href": "user-guide/installation.html#next-steps",
    "title": "Installation",
    "section": "",
    "text": "Now that you’ve installed Pointblank, you’re ready to start validating your data. If you haven’t read the Introduction yet, consider starting there to learn the basic concepts.\nIf you encounter any installation issues, please open an issue on GitHub with details about your system and the specific error messages you’re seeing. The maintainers actively monitor these issues and can help troubleshoot problems.\nFor a quick test of your installation, try running a simple validation:\nimport pointblank as pb\n\n# Load a small dataset\ndata = pb.load_dataset(\"small_table\")\n\n# Create a simple validation\nvalidation = (\n    pb.Validate(data=data)\n    .col_exists(columns=[\"a\", \"b\", \"c\"])\n    .interrogate()\n)\n\n# Display the validation results\nvalidation",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Installation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html",
    "href": "user-guide/expressions.html",
    "title": "Expression-Based Validation",
    "section": "",
    "text": "While Pointblank offers many specialized validation functions for common data quality checks, sometimes you need more flexibility for complex validation requirements. This is where expression-based validation with col_vals_expr() comes in.\nThe col_vals_expr() method allows you to:\nNow let’s explore how to use these capabilities through a collection of examples!",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html#basic-usage",
    "href": "user-guide/expressions.html#basic-usage",
    "title": "Expression-Based Validation",
    "section": "Basic Usage",
    "text": "Basic Usage\nAt its core, col_vals_expr() validates whether an expression evaluates to True for each row in your data. Here’s a simple example:\n\nimport pointblank as pb\nimport polars as pl\n\n# Load small_table dataset as a Polars DataFrame\nsmall_table_pl = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n\n(\n    pb.Validate(data=small_table_pl)\n    .col_vals_expr(\n\n        # Use Polars expression syntax ---\n        expr=pl.col(\"d\") &gt; pl.col(\"a\") * 50,\n        brief=\"Column `d` should be at least 50 times larger than `a`.\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:52Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        Column d should be at least 50 times larger than a.\n\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example, we’re validating that for each row, the value in column d is at least 50 times larger than the value in column a.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html#notes-on-expression-syntax",
    "href": "user-guide/expressions.html#notes-on-expression-syntax",
    "title": "Expression-Based Validation",
    "section": "Notes on Expression Syntax",
    "text": "Notes on Expression Syntax\nThe expression syntax depends on your table type:\n\nPolars: uses Polars expression syntax with pl.col(\"column_name\")\nPandas: uses standard Python/NumPy syntax\n\nThe expression should:\n\nevaluate to a boolean result for each row\nreference columns using the appropriate syntax for your table type\nuse standard operators (+, -, *, /, &gt;, &lt;, ==, etc.)\nnot include assignments",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html#complex-expressions",
    "href": "user-guide/expressions.html#complex-expressions",
    "title": "Expression-Based Validation",
    "section": "Complex Expressions",
    "text": "Complex Expressions\nThe real power of col_vals_expr() comes with complex expressions that would be difficult to represent using the standard validation functions:\n\n# Load game_revenue dataset as a Polars DataFrame\ngame_revenue_pl = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\")\n\n(\n    pb.Validate(data=game_revenue_pl)\n    .col_vals_expr(\n\n        # Use Polars expression syntax ---\n        expr=(pl.col(\"session_duration\") &gt; 20) | (pl.col(\"item_revenue\") &gt; 10),\n        brief=\"Sessions should be either long (&gt;20 min) or high-value (&gt;$10).\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:52Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        Sessions should be either long (&gt;20 min) or high-value (&gt;$10).\n\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    15180.76\n    4820.24\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis validates that either the session duration is longer than 20 minutes OR the item revenue is greater than $10.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html#example-multiple-conditions",
    "href": "user-guide/expressions.html#example-multiple-conditions",
    "title": "Expression-Based Validation",
    "section": "Example: Multiple Conditions",
    "text": "Example: Multiple Conditions\nYou can create sophisticated validations with multiple conditions:\n\n# Create a simple Polars DataFrame\nemployee_df = pl.DataFrame({\n    \"age\": [25, 30, 15, 40, 35],\n    \"income\": [50000, 75000, 0, 100000, 60000],\n    \"years_experience\": [3, 8, 0, 15, 7]\n})\n\n(\n    pb.Validate(data=employee_df, tbl_name=\"employee_data\")\n    .col_vals_expr(\n        # Complex condition with multiple comparisons\n        expr=(\n            (pl.col(\"age\") &gt;= 18) &\n            (pl.col(\"income\") / (pl.col(\"years_experience\") + 1) &lt;= 25000)\n        ),\n        brief=\"Adults should have reasonable income-to-experience ratios.\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:52Polarsemployee_data\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        Adults should have reasonable income-to-experience ratios.\n\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    5\n    40.80\n    10.20\n    —\n    —\n    —\n    —",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html#example-handling-null-values",
    "href": "user-guide/expressions.html#example-handling-null-values",
    "title": "Expression-Based Validation",
    "section": "Example: Handling Null Values",
    "text": "Example: Handling Null Values\nWhen working with expressions, consider how to handle null/missing values:\n\n(\n    pb.Validate(data=small_table_pl)\n    .col_vals_expr(\n        # Check for nulls before division\n        expr=(pl.col(\"c\").is_not_null()) & ((pl.col(\"c\") / pl.col(\"a\")) &gt; 1.5),\n        brief=\"Ratio of `c`/`a` should exceed 1.5 (when `c` is not null).\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:52Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        Ratio of c/a should exceed 1.5 (when c is not null).\n\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    —",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html#best-practices",
    "href": "user-guide/expressions.html#best-practices",
    "title": "Expression-Based Validation",
    "section": "Best Practices",
    "text": "Best Practices\nHere are some tips and tricks for effectively using expression-based validation with col_vals_expr().\n\nDocument Your Expressions\nAlways provide clear documentation in the brief= parameter:\n\n(\n    pb.Validate(data=small_table_pl)\n    .col_vals_expr(\n        expr=pl.col(\"d\") &gt; pl.col(\"a\") * 1.5,\n        # Document which columns are being compared\n        brief=\"Column `d` should be at least 1.5 times larger than column `a`.\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:52Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        Column d should be at least 1.5 times larger than column a.\n\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\n\n\nHandle Edge Cases\nConsider potential edge cases like division by zero or nulls:\n\n(\n    pb.Validate(data=small_table_pl)\n    .col_vals_expr(\n        # Check denominator before division\n        expr=(pl.col(\"a\") != 0) & (pl.col(\"d\") / pl.col(\"a\") &gt; 1.5),\n        brief=\"Ratio of `d`/`a` should exceed 1.5 (avoiding division by zero).\"\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:19:52Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        Ratio of d/a should exceed 1.5 (avoiding division by zero).\n\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\n\n\nTest on Small Datasets First\nWhen developing complex expressions, test on a small sample of your data first to ensure your logic is correct before applying it to large datasets.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/expressions.html#conclusion",
    "href": "user-guide/expressions.html#conclusion",
    "title": "Expression-Based Validation",
    "section": "Conclusion",
    "text": "Conclusion\nThe col_vals_expr() method provides a powerful way to implement complex validation logic in Pointblank when standard validation methods aren’t sufficient. By leveraging expressions, you can create sophisticated data quality checks tailored to your specific requirements, combining conditions across multiple columns and applying transformations as needed.\nThis flexibility makes expression-based validation an essential tool for addressing complex data quality scenarios in your validation workflows.",
    "crumbs": [
      "User Guide",
      "Advanced Validation",
      "Expression-Based Validation"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html",
    "href": "user-guide/preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "While the available validation methods can do a lot for you, there’s likewise a lot of things you can’t easily do with them. What if you wanted to validate that\nThese constitute more sophisticated validation requirements, yet such examinations are quite prevalent in practice. Rather than expanding our library to encompass every conceivable validation scenario (a pursuit that would yield an unwieldy and potentially infinite collection) we instead employ a more elegant approach. By transforming the table under examination through judicious preprocessing and exposing key metrics, we may subsequently employ the existing collection of validation methods. This compositional strategy affords us considerable analytical power while maintaining conceptual clarity and implementation parsimony.\nCentral to this approach is the idea of composability. Pointblank makes it easy to safely transform the target table for a given validation via the pre= argument. Any computed columns are available for the (short) lifetime of the validation step during interrogation. This composability means:\nThis compositional paradigm allows us to use data transformation effectively within our validation workflows, maintaining both flexibility and clarity in our data quality assessments.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#transforming-data-with-lambda-functions",
    "href": "user-guide/preprocessing.html#transforming-data-with-lambda-functions",
    "title": "Preprocessing",
    "section": "Transforming Data with Lambda Functions",
    "text": "Transforming Data with Lambda Functions\nNow, through examples, let’s look at the process of performing the validations mentioned above. We’ll use the small_table dataset for all of the examples. Here it is in its entirety:\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIn getting to grips with the basics, we’ll try to validate that string lengths in the b column are less than 10 characters. We can’t directly use the col_vals_lt() validation method with that column because it is meant to be used with a column of numeric values. Let’s just give that method what it needs and create a column with string lengths! The target table is a Polars DataFrame so we’ll provide a lambda function that uses the Polars API to add in that numeric column:\n\nimport polars as pl\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"String lengths\"\n    )\n    .col_vals_lt(\n        columns=\"string_lengths\",  # the generated column through `pre=`\n        value=10,                  # the string length value to be less than\n        pre=lambda df: df.with_columns(string_lengths=pl.col(\"b\").str.len_chars())\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    String lengthsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    string_lengths\n    10\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation was successfully constructed and we can see from the validation report table that all strings in b had lengths less than 10 characters. Also note that the icon under the TBL column is no longer a rightward-facing arrow, but one that is indicative of a transformation taking place.\nLet’s examine the transformation approach more closely. In the previous example, we’re not directly testing the b column itself. Instead, we’re validating the string_lengths column that was generated by the lambda function provided to pre=. The Polars API’s with_columns() method does the heavy lifting, creating numerical values that represent each string’s length in the original column.\nThat transformation occurs only during interrogation and only for that validation step. Any prior or subsequent steps would normally use the as-provided small_table. Having the possibility for data transformation being isolated at the step level means that you don’t have to generate separate validation plans for each form of the data, you’re free to fluidly transform the target table as necessary for perform validations on different representations of the data.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#using-custom-functions-for-preprocessing",
    "href": "user-guide/preprocessing.html#using-custom-functions-for-preprocessing",
    "title": "Preprocessing",
    "section": "Using Custom Functions for Preprocessing",
    "text": "Using Custom Functions for Preprocessing\nWhile lambda functions work well for simple transformations, custom named functions can make your validation code more organized and reusable, especially for complex preprocessing logic. Let’s implement the same string length validation using a dedicated function:\n\ndef add_string_lengths(df):\n    return df.with_columns(string_lengths=pl.col(\"b\").str.len_chars())\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"String lengths for column `b`.\"\n    )\n    .col_vals_lt(\n        columns=pb.last_n(1),\n        value=10,\n        pre=add_string_lengths\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    String lengths for column `b`.Polarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    string_lengths\n    10\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe column-generating logic was placed in the add_string_lengths() function, which is then passed to pre=. Notice we’re using pb.last_n(1) in the columns parameter. This is a convenient column selector that targets the last column in the DataFrame, which in our case is the newly created string_lengths column. This saves us from having to explicitly write out the column name, making our code more adaptable if column names change. Despite not specifying the name directly, you’ll still see the actual column name (string_lengths) displayed in the validation report.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#creating-parameterized-preprocessing-functions",
    "href": "user-guide/preprocessing.html#creating-parameterized-preprocessing-functions",
    "title": "Preprocessing",
    "section": "Creating Parameterized Preprocessing Functions",
    "text": "Creating Parameterized Preprocessing Functions\nSo far we’ve used simple functions and lambdas, but sometimes you may want to create more flexible preprocessing functions that can be configured with parameters. Let’s create a reusable function that can calculate string lengths for any column:\n\ndef string_length_calculator(column_name):\n    \"\"\"Returns a preprocessing function that calculates string lengths for the specified column.\"\"\"\n    def preprocessor(df):\n        return df.with_columns(string_lengths=pl.col(column_name).str.len_chars())\n    return preprocessor\n\n# Validate string lengths in column b\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"String lengths for column `b`.\"\n    )\n    .col_vals_lt(\n        columns=pb.last_n(1),\n        value=10,\n        pre=string_length_calculator(column_name=\"b\")\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    String lengths for column `b`.Polarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    string_lengths\n    10\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis pattern is called a function factory, which is a function that creates and returns another function. The outer function (string_length_calculator()) accepts parameters that customize the behavior of the returned preprocessing function. The inner function (preprocessor()) is what actually gets called during validation.\nThis approach offers several benefits as it:\n\ncreates reusable, configurable preprocessing functions\nkeeps your validation code DRY\nallows you to separate configuration from implementation\nenables easy application of the same transformation to different columns\n\nYou could extend this pattern to create even more sophisticated preprocessing functions with multiple parameters, default values, and complex logic.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#using-narwhals-to-preprocess-many-types-of-dataframes",
    "href": "user-guide/preprocessing.html#using-narwhals-to-preprocess-many-types-of-dataframes",
    "title": "Preprocessing",
    "section": "Using Narwhals to Preprocess Many Types of DataFrames",
    "text": "Using Narwhals to Preprocess Many Types of DataFrames\nIn this previous example we used a Polars table. You might have a situation where you perform data validation variously on Pandas and Polars DataFrames. This is where Narwhals becomes handy: it provides a single, consistent API that works across multiple DataFrame types, eliminating the need to learn and switch between different APIs depending on your data source.\nLet’s obtain small_table as a Pandas DataFrame. We’ll construct a validation step to verify that the median of column c is greater than the median in column a.\n\nimport narwhals as nw\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Median comparison.\",\n    )\n    .col_vals_gt(\n        columns=\"c\",\n        value=pb.col(\"a\"),\n        pre=lambda df: nw.from_native(df).select(nw.median(\"c\"), nw.median(\"a\"))\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Median comparison.Pandassmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    a\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe goal is to check that the median value of c is greater than the corresponding median of column a, which we set up through the columns= and value= parameters in the col_vals_gt() method.\nThere’s a bit to unpack here so let’s look at at the lambda function first. Narwhals can translate a Pandas DataFrame to a Narwhals DataFrame with its from_native() function. After that initiating step, you’re free to use the Narwhals API (which is modeled on a subset of the Polars API) to do the necessary data transformation. In this case, we are getting the medians of the c and a columns and ending up with a one-row, two-column table.\nWe should note that the transformed table is, perhaps surprisingly, a Narwhals DataFrame (we didn’t have to end with .to_native()). Pointblank is able to work directly with the Narwhals DataFrame for validation purposes, which makes the workflow more concise.\nOne more thing to note: Pointblank provides a convenient syntactic sugar for working with Narwhals. If you name the lambda parameter dfn instead of df, the system automatically applies nw.from_native() to the input DataFrame first. This lets you write more concise code without having to explicitly convert the DataFrame to a Narwhals format.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#swapping-in-a-totally-different-dataframe",
    "href": "user-guide/preprocessing.html#swapping-in-a-totally-different-dataframe",
    "title": "Preprocessing",
    "section": "Swapping in a Totally Different DataFrame",
    "text": "Swapping in a Totally Different DataFrame\nSometimes data validation requires looking at completely transformed versions of your data (such as aggregated summaries, pivoted views, or even reference tables). While this approach goes against the typical paradigm of validating a single target table, there are legitimate use cases where you might need to validate properties that only emerge after significant transformations.\nLet’s now try to prepare the final validation scenario, checking that there are at least three instances of every categorical value in column f (which contains string values in the set of \"low\", \"mid\", and \"high\"). This time, we’ll prepare the transformed table (transformed by Polars expressions) outside of the Pointblank code.\n\ndata_original = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\ndata_transformed = data_original.group_by(\"f\").len(name=\"n\")\n\ndata_transformed\n\n\nshape: (3, 2)fnstru32\"low\"5\"mid\"2\"high\"6\n\n\nThen, we’ll plug in the data_transformed DataFrame with a lambda expression in pre=:\n\n(\n    pb.Validate(\n        data=data_original,\n        tbl_name=\"small_table\",\n        label=\"Category counts.\",\n    )\n    .col_vals_ge(\n        columns=\"n\",\n        value=3,\n        pre=lambda x: data_transformed\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Category counts.Polarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    n\n    3\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    20.67\n    10.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWe can see from the validation report table that there are three test units. This corresponds to a row for each of the categorical value counts. From the report, we find that two of the three test units are passing test units (turns out there are only two instances of \"mid\" in column f).\nNote that the swapped-in table can be any table type that Pointblank supports, like a Polars DataFrame (as shown here), a Pandas DataFrame, a Narwhals DataFrame, or any other compatible format. This flexibility allows you to validate properties of your data that might only be apparent after significant reshaping or aggregation.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#conclusion",
    "href": "user-guide/preprocessing.html#conclusion",
    "title": "Preprocessing",
    "section": "Conclusion",
    "text": "Conclusion\nThe preprocessing capabilities in Pointblank provide the power and flexibility for validating complex data properties beyond what’s directly possible with the standard validation methods. Through the pre= parameter, you can:\n\ntransform your data on-the-fly with computed columns\ngenerate aggregated metrics to validate statistical properties\nwork seamlessly across different DataFrame types using Narwhals\nswap in completely different tables when validating properties that emerge only after transformation\n\nBy combining these preprocessing techniques with Pointblank’s validation methods, you can create comprehensive data quality checks that address virtually any validation scenario without needing an endless library of specialized validation functions. This composable approach keeps your validation code concise while allowing you to verify even the most complex data quality requirements.\nRemember that preprocessing happens just for the specific validation step, keeping your validation plan organized and maintaining the integrity of your original data throughout the rest of the validation process.",
    "crumbs": [
      "User Guide",
      "Validation Plan",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html",
    "href": "user-guide/col-summary-tbl.html",
    "title": "Column Summaries",
    "section": "",
    "text": "While previewing a table with preview() is undoubtedly a good thing to do, sometimes you need more. This is where summarizing a table comes in. When you view a summary of a table, the column-by-column info can quickly increase your understanding of a dataset. Plus, it allows you to quickly catch anomalies in your data (e.g., the maximum value of a column could be far outside the realm of possibility).\nPointblank provides a function to make it extremely easy to view column-level summaries in a single table. That function is called col_summary_tbl() and, just like preview() does, it supports the use of any table that Pointblank can use for validation. And no matter what the input data is, the resultant reporting table is consistent in its design and construction.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#trying-out-col_summary_tbl",
    "href": "user-guide/col-summary-tbl.html#trying-out-col_summary_tbl",
    "title": "Column Summaries",
    "section": "Trying out col_summary_tbl()",
    "text": "Trying out col_summary_tbl()\nThe function only requires a table. Let’s use the small_table dataset (a very simple table) to start us off:\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n\npb.col_summary_tbl(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    date_timeDatetime(time_unit='us', time_zone=None)\n    0 0.00\n    12 0.92\n    —\n    —\n     2016-01-04 00:32:00 – 2016-01-30 11:23:00\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    2\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dateDate\n    0 0.00\n    11 0.85\n    —\n    —\n     2016-01-04 – 2016-01-30\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    3\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    aInt64\n    0 0.00\n    7 0.54\n    3.77\n    2.09\n    1.00\n    1.60\n    2.00\n    3.00\n    4.00\n    7.40\n    8.00\n    2.00\n  \n  \n    4\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    bString\n    0 0.00\n    12 0.92\n    9.00SL\n    0.00SL\n    9SL\n    —\n    —\n    9SL\n    —\n    —\n    9SL\n    —\n  \n  \n    5\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    cInt64\n    2 0.15\n    6 0.46\n    5.73\n    2.72\n    2.00\n    2.50\n    3.00\n    7.00\n    8.00\n    9.00\n    9.00\n    5.00\n  \n  \n    6\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dFloat64\n    0 0.00\n    12 0.92\n    2305\n    2631\n    108\n    214\n    838\n    1036\n    3291\n    6335\n    10000\n    2453\n  \n  \n    7\n    \n    boolean\n    \n        \n            \n            \n                \n            \n            \n                \n            \n            \n        \n    \n\n    eBoolean\n    0 0.00\n    T 0.61F 0.39\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n  \n    8\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    fString\n    0 0.00\n    3 0.23\n    3.46SL\n    0.52SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    4SL\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe header provides the type of table we’re looking at (POLARS, since this is a Polars DataFrame) and the table dimensions. The rest of the table focuses on the column-level summaries. As such, each row represents a summary of a column in the small_table dataset. There’s a lot of information in this summary table to digest. Some of it is intuitive since this sort of table summarization isn’t all that uncommon, but other aspects of it could also give some pause. So we’ll carefully wade through how to interpret this report.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#data-categories-in-the-column-summary-table",
    "href": "user-guide/col-summary-tbl.html#data-categories-in-the-column-summary-table",
    "title": "Column Summaries",
    "section": "Data Categories in the Column Summary Table",
    "text": "Data Categories in the Column Summary Table\nOn the left side of the table are icons of different colors. These represent categories that the columns fall into. There are only five categories and columns can only be of one type. The categories (and their letter marks) are:\n\nN: numeric\nS: string-based\nD: date/datetime\nT/F: boolean\nO: object\n\nThe numeric category (N) takes data types such as floats and integers. The S category is for string-based columns. Date or datetime values are lumped into the D category. Boolean columns (T/F) have their own category and are not considered numeric (e.g., 0/1). The O category is a catchall for all other types of columns. Given the disparity of these categories and that we want them in the same table, some statistical measures will be sensible for certain column categories but not for others. Given that, we’ll explain how each category is represented in the column summary table.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#numeric-data",
    "href": "user-guide/col-summary-tbl.html#numeric-data",
    "title": "Column Summaries",
    "section": "Numeric Data",
    "text": "Numeric Data\nThree columns in small_table are numeric: a (Int64), c (Int64), and d (Float64). The common measures of the missing count/proportion (NA) and the unique value count/proportion (UQ) are provided for the numeric data type. For these two measures, the top number is the absolute count of missing values and the count of unique values. The bottom number is a proportion of the absolute count divided by the row count; this makes each proportion a value between 0 and 1 (bounds included).\nThe next two columns represent the mean (Mean) and the standard deviation (SD). The minumum (Min), maximum, (Max) and a set of quantiles occupy the next few columns (includes P5, Q1, Med for median, Q3, and P95). Finally, the interquartile range (IQR: Q3 - Q1) is the last measure provided.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#string-data",
    "href": "user-guide/col-summary-tbl.html#string-data",
    "title": "Column Summaries",
    "section": "String Data",
    "text": "String Data\nString data is present in small_table, being in columns b and f. The missing value (NA) and uniqueness (UQ) measures are accounted for here. The statistical measures are all based on string lengths, so what happens is that all strings in a column are converted to those numeric values and a subset of stats values is presented. To avoid some understandable confusion when reading the table, the stats values in each of the cells with values are annotated with the text \"SL\". It makes less sense to provide a full suite of quantile values so only the minimum (Min), median (Med), and maximum (Max) are provided.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#datedatetime-data-and-boolean-data",
    "href": "user-guide/col-summary-tbl.html#datedatetime-data-and-boolean-data",
    "title": "Column Summaries",
    "section": "Date/Datetime Data and Boolean Data",
    "text": "Date/Datetime Data and Boolean Data\nWe see that in the first two rows of our summary table there are summaries of the date_time and date columns. The summaries we provide for a date/datetime category (notice the green D to the left of the column names) are:\n\nthe missing count/proportion (NA)\nthe unique value count/proportion (UQ)\nthe minimum and maximum dates/datetimes\n\nOne column, e, is of the Boolean type. Because columns of this type could only have True, False, or missing values, we provide summary data for missingness (under NA) and proportions of True and False values (under UQ).",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Column Summaries"
    ]
  },
  {
    "objectID": "demos/using-parquet-data/index.html",
    "href": "demos/using-parquet-data/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Using Parquet Data\nA Parquet dataset can be used for data validation, thanks to Ibis.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example using a Parquet dataset.Parquet\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    item_revenue\n    200\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:14 UTC&lt; 1 s2025-05-23 02:20:14 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport ibis\n\ngame_revenue = ibis.read_parquet(\"data/game_revenue.parquet\")\n\nvalidation = (\n    pb.Validate(data=game_revenue, label=\"Example using a Parquet dataset.\")\n    .col_vals_lt(columns=\"item_revenue\", value=200)\n    .col_vals_gt(columns=\"item_revenue\", value=0)\n    .col_vals_gt(columns=\"session_duration\", value=5)\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    ParquetRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/03-data-extracts/index.html",
    "href": "demos/03-data-extracts/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Data Extracts\nPulling out data extracts that highlight rows with validation failures.\n\nValidation with failures at Step 2:\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Validation with test unit failures available as an extractPolarsgame_revenue\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19860.99\n    140.01\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-05-23 02:20:21 UTC&lt; 1 s2025-05-23 02:20:21 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\nExtract from Step 2 (which has 14 failing test units):\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows14Columns12\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1455\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-46cdjzy7\n    2015-01-17 11:25:25+00:00\n    2015-01-17 11:28:01+00:00\n    iap\n    offer4\n    13.99\n    4.6\n    2015-01-14\n    organic\n    United States\n  \n  \n    1516\n    OMCVUAIKSDTR651\n    OMCVUAIKSDTR651-yso9e1b2\n    2015-01-17 20:58:34+00:00\n    2015-01-17 21:01:34+00:00\n    iap\n    offer3\n    10.49\n    4.2\n    2015-01-07\n    other_campaign\n    United States\n  \n  \n    1517\n    OMCVUAIKSDTR651\n    OMCVUAIKSDTR651-yso9e1b2\n    2015-01-17 20:58:34+00:00\n    2015-01-17 21:02:34+00:00\n    iap\n    offer5\n    20.29\n    4.2\n    2015-01-07\n    other_campaign\n    United States\n  \n  \n    1913\n    MTCIWKOVASYP925\n    MTCIWKOVASYP925-1q3xvfmp\n    2015-01-20 12:34:43+00:00\n    2015-01-20 12:35:37+00:00\n    iap\n    offer5\n    26.09\n    3.9\n    2015-01-14\n    organic\n    Germany\n  \n  \n    1914\n    MTCIWKOVASYP925\n    MTCIWKOVASYP925-1q3xvfmp\n    2015-01-20 12:34:43+00:00\n    2015-01-20 12:37:25+00:00\n    iap\n    gold2\n    1.79\n    3.9\n    2015-01-14\n    organic\n    Germany\n  \n  \n    1919\n    BFNLURISJXTH647\n    BFNLURISJXTH647-len6vujd\n    2015-01-20 14:09:51+00:00\n    2015-01-20 14:10:03+00:00\n    iap\n    gold7\n    47.99\n    4.5\n    2015-01-10\n    organic\n    India\n  \n  \n    1920\n    BFNLURISJXTH647\n    BFNLURISJXTH647-len6vujd\n    2015-01-20 14:09:51+00:00\n    2015-01-20 14:14:21+00:00\n    iap\n    gold6\n    23.99\n    4.5\n    2015-01-10\n    organic\n    India\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\"),\n        tbl_name=\"game_revenue\",\n        label=\"Validation with test unit failures available as an extract\"\n    )\n    .col_vals_gt(columns=\"item_revenue\", value=0)      # STEP 1: no test unit failures\n    .col_vals_ge(columns=\"session_duration\", value=5)  # STEP 2: 14 test unit failures -&gt; extract\n    .interrogate()\n)\npb.preview(validation.get_data_extracts(i=2, frame=True), n_head=20, n_tail=20)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/col-vals-custom-expr/index.html",
    "href": "demos/col-vals-custom-expr/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Custom Expression for Checking Column Values\nA column expression can be used to check column values. Just use col_vals_expr() for this.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:27Pandas\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        \n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:27 UTC&lt; 1 s2025-05-23 02:20:27 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\n    )\n    .col_vals_expr(expr=lambda df: (df[\"d\"] % 1 != 0) & (df[\"a\"] &lt; 10))  # Pandas column expr\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/set-membership/index.html",
    "href": "demos/set-membership/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Set Membership\nPerform validations that check whether values are part of a set (or not part of one).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:34Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    f\n    zero, infinity\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:34 UTC&lt; 1 s2025-05-23 02:20:34 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])    # part of this set\n    .col_vals_not_in_set(columns=\"f\", set=[\"zero\", \"infinity\"])  # not part of this set\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/check-row-column-counts/index.html",
    "href": "demos/check-row-column-counts/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Verifying Row and Column Counts\nCheck the dimensions of the table with the *_count_match() validation methods.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:40DuckDB\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    ≠ 0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:40 UTC&lt; 1 s2025-05-23 02:20:40 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\")\n    )\n    .col_count_match(count=11)                       # expect 11 columns in the table\n    .row_count_match(count=2000)                     # expect 2,000 rows in the table\n    .row_count_match(count=0, inverse=True)          # expect that the table has rows\n    .col_count_match(                                # compare column count against\n        count=pb.load_dataset(                       # that of another table\n            dataset=\"game_revenue\", tbl_type=\"pandas\"\n        )\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/index.html",
    "href": "demos/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "A Selection of Examples\n\n\n\n\n\n\nStarter Validation\n\n\n\nA validation with the basics.\n\n\n\n\n\n\nAdvanced Validation\n\n\n\nA validation with a comprehensive set of rules.\n\n\n\n\n\n\nData Extracts\n\n\n\nPulling out data extracts that highlight rows with validation failures.\n\n\n\n\n\n\nSundered Data\n\n\n\nSplitting your data into ‘pass’ and ‘fail’ subsets.\n\n\n\n\n\n\nStep Reports for Column Data Checks\n\n\n\nA step report for column checks shows what went wrong.\n\n\n\n\n\n\nStep Report for a Schema Check\n\n\n\nWhen a schema doesn’t match, a step report gives you the details.\n\n\n\n\n\n\n\nNumeric Comparisons Perform comparisons of values in columns to fixed values.\nComparison Checks Across Columns Perform comparisons of values in columns to values in other columns.\nApply Validation Rules to Multiple Columns Create multiple validation steps by using a list of column names with columns=.\nChecks for Missing Values Perform validations that check whether missing/NA/Null values are present.\nExpectations with a Text Pattern With col_vals_regex(), check for conformance to a regular expression.\nSet Membership Perform validations that check whether values are part of a set (or not part of one).\nExpect No Duplicate Rows We can check for duplicate rows in the table with rows_distinct().\nChecking for Duplicate Values To check for duplicate values down a column, use rows_distinct() with a columns_subset= value.\nCustom Expression for Checking Column Values A column expression can be used to check column values. Just use col_vals_expr() for this.\nMutate the Table in a Validation Step For far more specialized validations, modify the table with the pre= argument before checking it.\nVerifying Row and Column Counts Check the dimensions of the table with the *_count_match() validation methods.\nSet Failure Threshold Levels Set threshold levels to better gauge adverse data quality.\nColumn Selector Functions: Easily Pick Columns Use column selector functions in the columns= argument to conveniently choose columns.\nCheck the Schema of a Table The schema of a table can be flexibly defined with Schema and verified with col_schema_match().\nUsing Parquet Data A Parquet dataset can be used for data validation, thanks to Ibis."
  },
  {
    "objectID": "demos/expect-text-pattern/index.html",
    "href": "demos/expect-text-pattern/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Expectations with a Text Pattern\nWith the col_vals_regex(), check for conformance to a regular expression.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:51Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^\\d-[a-z]{3}-\\d{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    f\n    high|low|mid\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:20:51 UTC&lt; 1 s2025-05-23 02:20:51 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_regex(columns=\"b\", pattern=r\"^\\d-[a-z]{3}-\\d{3}$\")  # check pattern in 'b'\n    .col_vals_regex(columns=\"f\", pattern=r\"high|low|mid\")         # check pattern in 'f'\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/numeric-comparisons/index.html",
    "href": "demos/numeric-comparisons/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Numeric Comparisons\nPerform comparisons of values in columns to fixed values.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:20:57Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    10000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    a\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    4\n    \n        \n            \n\n    col_vals_le\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    5\n    \n        \n            \n\n    col_vals_ne\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ne()\n        \n        \n        \n    a\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    6\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    c\n    [0, 15]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-05-23 02:20:57 UTC&lt; 1 s2025-05-23 02:20:57 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_gt(columns=\"d\", value=1000)            # values in 'd' &gt; 1000\n    .col_vals_lt(columns=\"d\", value=10000)           # values in 'd' &lt; 10000\n    .col_vals_ge(columns=\"a\", value=1)               # values in 'a' &gt;= 1\n    .col_vals_le(columns=\"c\", value=5)               # values in 'c' &lt;= 5\n    .col_vals_ne(columns=\"a\", value=7)               # values in 'a' not equal to 7\n    .col_vals_between(columns=\"c\", left=0, right=15) # 0 &lt;= 'c' values &lt;= 15\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/column-selector-functions/index.html",
    "href": "demos/column-selector-functions/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Column Selector Functions: Easily Pick Columns\nUse column selector functions in the columns= argument to conveniently choose columns.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:21:04Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_ge\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    ^[A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    session_id\n    ^[A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    acquisition\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    country\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    8\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    session_id\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    9\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    item_type\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    item_name\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    11\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    acquisition\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    12\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    country\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:21:04 UTC&lt; 1 s2025-05-23 02:21:04 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport narwhals.selectors as ncs\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\")\n    )\n    .col_vals_ge(\n        columns=pb.matches(\"rev|dur\"),  # check values in columns having 'rev' or 'dur' in name\n        value=0\n    )\n    .col_vals_regex(\n        columns=pb.ends_with(\"_id\"),    # check values in columns with names ending in '_id'\n        pattern=r\"^[A-Z]{12}\\d{3}\"\n    )\n    .col_vals_not_null(\n        columns=pb.last_n(2)            # check that the last two columns don't have Null values\n    )\n    .col_vals_regex(\n        columns=ncs.string(),           # check that all string columns are non-empty strings\n        pattern=r\"(.|\\s)*\\S(.|\\s)*\"\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/expect-no-duplicate-values/index.html",
    "href": "demos/expect-no-duplicate-values/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Checking for Duplicate Values\nTo check for duplicate values down a column, use rows_distinct() with a columns_subset= value.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:21:11Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-05-23 02:21:11 UTC&lt; 1 s2025-05-23 02:21:11 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .rows_distinct(columns_subset=\"b\")   # expect no duplicate values in 'b'\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/comparisons-across-columns/index.html",
    "href": "demos/comparisons-across-columns/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Comparison Checks Across Columns\nPerform comparisons of values in columns to values in other columns.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-05-23|02:21:18Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [c, 12000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-05-23 02:21:18 UTC&lt; 1 s2025-05-23 02:21:18 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_lt(columns=\"a\", value=pb.col(\"c\"))     # values in 'a' &gt; values in 'c'\n    .col_vals_between(\n        columns=\"d\",                                 # values in 'd' are between values\n        left=pb.col(\"c\"),                            # in 'c' and the fixed value of 12,000;\n        right=12000,                                 # any missing values encountered result\n        na_pass=True                                 # in a passing test unit\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  }
]