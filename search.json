[
  {
    "objectID": "user-guide/preprocessing.html",
    "href": "user-guide/preprocessing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "While the validation methods available can do a lot for you, there’s likewise a lot of things you can’t easily do with them. What if you wanted to validate:\nThese are more complicated types of validations, yet checks of this type are very commonplace. We don’t need to have a very large library of validation methods to tackle each an every case; the number of combinations indeed seems exceedingly large. Instead, let’s transform the table through a preprocessing step and expose the key values. In conjunction with that sort of table mutation, we then can use the existing library of validation methods.\nCentral to this approach is the idea of composability. Pointblank makes it easy to safely transform the input table in a given validation step via the pre= argument. Any computed columns are available for the (short) lifetime of the validation step during interrogation.\nNow, through a series of examples, let’s look at the process of performing the validations mentioned above. We’ll use the small_table dataset for all of the examples. Here it is in its entirety:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#the-basics-of-preprocessing-the-input-table",
    "href": "user-guide/preprocessing.html#the-basics-of-preprocessing-the-input-table",
    "title": "Preprocessing",
    "section": "The Basics of Preprocessing the Input Table",
    "text": "The Basics of Preprocessing the Input Table\nIn getting to grips with the basics, we’ll try to validate that string lengths in the b column are less than 10 characters. We can’t directly use the col_vals_lt() validation method with that column because it is meant to be used with a column of numeric values. Let’s just give that method what it needs and create a column with string lengths! The target table is a Polars DataFrame so we’ll provide a lambda function that uses the Polars API to add in that numeric column:\n\nimport polars as pl\n\n(\n    pb.Validate(\n        data=pb.load_dataset(\"small_table\"),\n        tbl_name=\"small_table\",\n        label=\"String lengths\"\n    )\n    .col_vals_lt(\n        columns=\"string_lengths\",  # the generated column through `pre=`\n        value=10,                  # the string length value to be less than\n        pre=lambda df: df.with_columns(string_lengths=pl.col(\"b\").str.len_chars())\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    String lengthsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    string_lengths\n    10\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation was successfully constructed and we can see from the validation report table that all strings in b had lengths less than 10 characters. Also note that the icon under the TBL column is no longer a rightward-facing arrow, but one that is indicative of transformation.\nFrom the code itself, we’re not directly testing the b column. Instead the test is of the string_lengths column generated by the lambda provided to pre=. We used Polars to do the transformation work here and that’s the piece that generates numerical values of string lengths in the computed column.\nThat transformation occurs only during interrogation and only for that validation step. Any prior or subsequent steps would normally use the as-provided small_table. Having the possibility for data transformation being isolated at the step level means that you don’t have to generate separate validation plans for each form of the data, you’re free to fluidly transform the target table as necessary for perform validations on different representations of the data.\nAside from using a lambda, you can pass in a custom function. Just make sure not to evaluate it at the pre= parameter (everything is stored lazily until interrogation time). Here’s an example of that approach:\n\ndef add_string_lengths(df):\n    return df.with_columns(string_lengths=pl.col(\"b\").str.len_chars())\n\n(\n    pb.Validate(\n        data=pb.load_dataset(\"small_table\"),\n        tbl_name=\"small_table\",\n        label=\"String lengths\"\n    )\n    .col_vals_lt(columns=pb.last_n(1), value=10, pre=add_string_lengths)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    String lengthsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    string_lengths\n    10\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe column-generating logic was placed in the add_string_lengths() function, which is then passed to pre=. We also know that the column generated will be the final one in the column series, so the last_n() column selector obviates the need to provide \"string_lengths\" here (you’ll still find the target column name echoed on the validation report table).",
    "crumbs": [
      "User Guide",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#using-narwhals-to-preprocess-many-types-of-dataframes",
    "href": "user-guide/preprocessing.html#using-narwhals-to-preprocess-many-types-of-dataframes",
    "title": "Preprocessing",
    "section": "Using Narwhals to Preprocess Many Types of DataFrames",
    "text": "Using Narwhals to Preprocess Many Types of DataFrames\nIn this previous example we used a Polars table (the load_dataset() returns a Polars DataFrame by default). You might have a situation where where you perform data validation variously on Pandas and Polars DataFrames. This is where Narwhals becomes handy.\nLet’s obtain small_table as a Pandas DataFrame. We’ll construct a validation step to verify that the median of column c is greater than the median in column a.\n\nimport narwhals as nw\n\n(\n    pb.Validate(\n        data=pb.load_dataset(\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Median comparison\",\n    )\n    .col_vals_gt(\n        columns=\"c\",\n        value=pb.col(\"a\"),\n        pre=lambda df: nw.from_native(df).select(nw.median(\"c\"), nw.median(\"a\"))\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Median comparisonPandassmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    c\n    a\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThere’s a bit to unpack here so let’s look at at the lambda function first. Narwhals can translate a Pandas DataFrame to a Narwhals DataFrame with its from_native() function. After that initiating step, you’re free to use the Narwhals API (which is modeled on a subset of the Polars API) to do the necessary data transformation. In this case, we are getting the medians of the c and a columns and ending up with a one-row, two-column table.\nThe goal is to check that the median value of c is greater than the corresponding median of column a, so we set up columns= and value= parameters in that way within the col_vals_gt() validation method call.",
    "crumbs": [
      "User Guide",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#swapping-in-different-dataframes-just-in-time",
    "href": "user-guide/preprocessing.html#swapping-in-different-dataframes-just-in-time",
    "title": "Preprocessing",
    "section": "Swapping in Different DataFrames Just in Time",
    "text": "Swapping in Different DataFrames Just in Time\nLet’s now try to prepare the final validation scenario, checking that there are at least three instances of every categorical value in column f (which contains string values in the set of \"low\", \"mid\", and \"high\"). This time, we’ll prepare the transformed table (transformed by Polars expressions) outside of the Pointblank code. Then, we’ll plug in the data_transformed DataFrame with in lambda expression in pre=:\n\ndata_original = pb.load_dataset(\"small_table\")\ndata_transformed = data_original.group_by(\"f\").len(name=\"n\")\n\nv = (\n    pb.Validate(\n        data=pb.load_dataset(\"small_table\"),\n        tbl_name=\"small_table\",\n        label=\"Category counts\",\n    )\n    .col_vals_ge(columns=\"n\", value=3, pre=lambda x: data_transformed)\n    .interrogate()\n)\n\nWe can see from the validation report table that there are three test units. This corresponds to a row for each of the categorical value counts. From the report, we find that two of the three test units are passing test units (turns out there are only two instances of \"mid\" in column f).",
    "crumbs": [
      "User Guide",
      "Preprocessing"
    ]
  },
  {
    "objectID": "user-guide/sundering.html",
    "href": "user-guide/sundering.html",
    "title": "Sundering Data",
    "section": "",
    "text": "Sundering data? First off, let’s get the correct meaning across here. Sundering is really just splitting, dividing, cutting into two pieces. And it’s a useful thing we can do in Pointblank to any data that we are validating. When you interrogate the data, you learn about which rows have test failures within them. With more validation steps, we get an even better picture of this simply by virtue of more testing.\nLet’s use the small_table in our examples to show just how sundering is done. Here’s that table:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#a-simple-example-where-data-is-torn-asunder",
    "href": "user-guide/sundering.html#a-simple-example-where-data-is-torn-asunder",
    "title": "Sundering Data",
    "section": "A Simple Example Where Data is Torn Asunder",
    "text": "A Simple Example Where Data is Torn Asunder\nWe’ll begin with a very simple validation plan, having only a single step. There will be failing test units here.\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_ge(columns=\"d\", value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWe see six failing test units in FAIL column of the above validation report table. There is a data extract (collection of failing rows) available. Let’s use the get_data_extracts() method to have a look at it.\n\nvalidation.get_data_extracts(i=1, frame=True)\n\n\nshape: (6, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr52016-01-09 12:36:002016-01-098\"3-ldm-038\"7283.94true\"low\"72016-01-15 18:46:002016-01-157\"1-knw-093\"3843.34true\"high\"92016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"102016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"112016-01-26 20:07:002016-01-264\"2-dmx-010\"7833.98true\"low\"122016-01-28 02:51:002016-01-282\"7-dmx-010\"8108.34false\"low\"\n\n\nThis is six rows of data that had failing test units in column d. Indeed we can see that all values in that column are less than 1000 (and we asserted that values should be greater than or equal to 1000). This is the ‘bad’ data, if you will. Using the get_sundered_data() method, we get the ‘good’ part:\n\nvalidation.get_sundered_data()\n\n\nshape: (7, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-04 11:00:002016-01-042\"1-bcd-345\"33423.29true\"high\"2016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"2016-01-05 13:32:002016-01-056\"8-kdg-938\"32343.23true\"high\"2016-01-06 17:23:002016-01-062\"5-jdo-903\"null3892.4false\"mid\"2016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"2016-01-17 11:27:002016-01-174\"5-boe-639\"21035.64false\"low\"2016-01-30 11:23:002016-01-301\"3-dka-303\"null2230.09true\"high\"\n\n\nThis is a Polars DataFrame of seven rows. All values in d were passing test units (i.e., fulfilled the expectation outlined in the validation step) and, in many ways, this is like a ‘good extract’.\nYou can always collect the failing rows with get_sundered_data() by using the type=\"fail\" option. Trying that here\n\nvalidation.get_sundered_data(type=\"fail\")\n\n\nshape: (6, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-09 12:36:002016-01-098\"3-ldm-038\"7283.94true\"low\"2016-01-15 18:46:002016-01-157\"1-knw-093\"3843.34true\"high\"2016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"2016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"2016-01-26 20:07:002016-01-264\"2-dmx-010\"7833.98true\"low\"2016-01-28 02:51:002016-01-282\"7-dmx-010\"8108.34false\"low\"\n\n\ngives us the same rows as in the DataFrame obtained from using validation.get_data_extracts(i=1, frame=True). Two important things to know about get_sundered_data() is that the table rows returned from type=pass (the default) and type=fail are:\n\nthe sum of rows across these returned tables will be equal to that of the original table\nthe rows in each split table are mutually exclusive (i.e., you won’t find the same row in both)\n\nYou can think of sundered data as a filtered version of the original dataset based on validation results. While the simple example illustrates how this process works on a basic level, the value of the method is better seen in a slightly more complex example.",
    "crumbs": [
      "User Guide",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#using-get_sundered_data-with-a-more-comprehensive-validation",
    "href": "user-guide/sundering.html#using-get_sundered_data-with-a-more-comprehensive-validation",
    "title": "Sundering Data",
    "section": "Using get_sundered_data() with a More Comprehensive Validation",
    "text": "Using get_sundered_data() with a More Comprehensive Validation\nThe previous example used exactly one valiation step. You’re likely to use more than that in standard practice so let’s see how get_sundered_data() works in those common situations. Here’s a validation with three steps:\n\nvalidation_2 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_ge(columns=\"d\", value=1000)\n    .col_vals_not_null(columns=\"c\")\n    .col_vals_gt(columns=\"a\", value=2)\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThere are quite a few failures here across the three validation steps. In the FAIL column of the validation report table, there are 12 failing test units if we were to tally them up. So if the input table has 13 rows in total, does this mean there would be one row in the table returned by get_sundered_data()? Not so:\n\nvalidation_2.get_sundered_data()\n\n\nshape: (4, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"2016-01-05 13:32:002016-01-056\"8-kdg-938\"32343.23true\"high\"2016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"2016-01-17 11:27:002016-01-174\"5-boe-639\"21035.64false\"low\"\n\n\nThere are four rows. This is because the different validation steps tested values in different columns of the table. Some of the failing test units had to have occurred in more than once in certain rows. The rows that didn’t have any failing test units across the three different tests (in three different columns) are the ones seen above. This brings us to the third important thing about the sundering process:\n\nthe absence of test-unit failures in a row across all validation steps means those rows are returned as the \"pass\" set, all others are placed in the \"fail\" set\n\nIn validations where many validation steps are used, we can be more confident about the level of data quality for those rows returned in the \"pass\" set. But not every type of validation step is considered within this splitting procedure. The next section will explain the rules on that.",
    "crumbs": [
      "User Guide",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#the-validation-methods-considered-when-sundering",
    "href": "user-guide/sundering.html#the-validation-methods-considered-when-sundering",
    "title": "Sundering Data",
    "section": "The Validation Methods Considered When Sundering",
    "text": "The Validation Methods Considered When Sundering\nThe sundering procedure relies on row-level validation types to be used. This makes sense as it’s impossible to judge the quality of a row when using the col_exists() validation method, for example. Luckily, we have many row-level validation methods; here’s a list:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\n\nThis is the same list of validation methods that are considered when creating data extracts.\nThere are some additional caveats though. Even if using a validation method drawn from the set above, the validation step won’t be used for sundering if:\n\nthe active= parameter for that step has been set to False\nthe pre= parameter has been used\n\nThe first one makes intuitive sense (you decided to skip this validation step entirely), the second one requires some explanation. Using pre= allows you to modify the target table, there’s no easy or practical way to compare rows in a mutated table compared to the original table (e.g., a mutation may drastically reduce the number of rows).\nSo long as you’re aware of the rules and limitations of sundering, you’ll hopefully find it to be a simple and useful way to filter your input table on the basis of a validation plan.",
    "crumbs": [
      "User Guide",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/across.html",
    "href": "user-guide/across.html",
    "title": "Across Columns",
    "section": "",
    "text": "The previous section demonstrated the use of the many validation methods. In all of the examples where column values were compared against another value, that value was always one that was fixed. These sorts of comparisons (e.g., are values in column x greater than or equal to zero?) are useful but sometimes you need more.\nFor a more dynamic type of comparison check, you can also compare a column’s values against those of another column. The name of the comparison column is provided as the value= parameter (or left= and right= as needed in the col_vals_between() and col_vals_outside() validation methods).\nLet’s see how this type of validation is made possible through a few examples using the small_table dataset shown below:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Across Columns"
    ]
  },
  {
    "objectID": "user-guide/across.html#using-col-to-specify-a-value-column",
    "href": "user-guide/across.html#using-col-to-specify-a-value-column",
    "title": "Across Columns",
    "section": "Using col() to Specify a Value Column",
    "text": "Using col() to Specify a Value Column\nWe know to use the columns= to supply the target column. Just to recap, if we wanted to check that every value in column a is greater than 2 then the validation step is written something like this:\n...\n.col_vals_gt(columns=\"a\", value=2, ...)\n...\nWhat if you have two numerical columns and have good reason to compare values across those columns? This could be a check that expects every value in a is greater than every value in x (the comparison column). That would take the form:\n...\n.col_vals_gt(columns=\"a\", value=pb.col(\"x\"), ...)\n...\nUsing the col() helper function here in value= is key. It lets Pointblank know that you’re not using a literal, fixed value for the comparison (you’re specifying a column). So long as you do this, the validation will involve checking that every value a is greater than every adjacent value in x. Here’s an example of this:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_gt(columns=\"d\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    d\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nNotice that in the COLUMNS and VALUES columns of the validation report, the two column names are shown: d and c. A thing that could be surprising is that there are two failing test units, even though values in d are consistently larger than values in column c. The reason is that there are missing values (i.e., None values) in column c and any missing value in a comparison check will result in a failing test unit.\nWhen doing a comparison against a fixed value, we only had to worry about missing values in the target column. When comparing across columns, there is potential for missing values in both columns and that could result in correspondingly more failing test units. The corrective here is to use na_pass=True. If you feel missing values (in either column) should be disregarded, this setting is a reasonable choice (and you could always use col_vals_not_null() to perform missing value checks on these columns anyway).\nLet’s take a quick look at the results when na_pass=True is used:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_gt(columns=\"d\", value=pb.col(\"c\"), na_pass=True)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    d\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nWith that change, every test unit in that single step passes validation.\nThe validation methods that accept a col() expression in their value= parameter include:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()",
    "crumbs": [
      "User Guide",
      "Across Columns"
    ]
  },
  {
    "objectID": "user-guide/across.html#using-col-in-range-checks",
    "href": "user-guide/across.html#using-col-in-range-checks",
    "title": "Across Columns",
    "section": "Using col() in Range Checks",
    "text": "Using col() in Range Checks\nTwo validation methods deal with checking values within and outside a range:\n\ncol_vals_between()\ncol_vals_outside()\n\nThese validation methods both have left= and right= arguments. You can use a mix of literal values and col()-based expressions for these parameters. Here’s an example where we check values in d to be in the range of lower-bound values in column c and a fixed upper-bound value of 10000:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_between(columns=\"d\", left=pb.col(\"c\"), right=10_000, na_pass=True)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    d\n    [c, 10000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nObserve that the range reported in the VALUES column is [c, 10000]. This is our assurance that the left bound is dependent on values in column c and that the right bound is fixed to a value of 10000. All test units passed here as we were careful about missing values (using na_pass=True) as in the previous example.",
    "crumbs": [
      "User Guide",
      "Across Columns"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html",
    "href": "user-guide/thresholds.html",
    "title": "Thresholds",
    "section": "",
    "text": "Thresholds enable you to signal failure at different severity levels. They also allow for the triggering of custom actions, a topic which is covered in the next section. For instance you might be testing a column for null/missing values. When doing so, you’d want to know when there are at least 10% missing values in the column. Alternatively, it could be the case that even a single missing value is critical to your work. Threshold settings in Pointblank give you the flexibility to devise data-failure signaling to whatever tolerances are important to you.\nLet’s start with the basics though. Here’s an example of a simple validation where threshold values are set in the col_vals_not_null() validation step (this type of validation expects that there are no null/missing values in a particular column):\nimport pointblank as pb\n\nvalidation_1 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_not_null(\n        columns=\"c\", thresholds=(1, 0.2)\n    )\n    .interrogate()\n)\n\nvalidation_1\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:53:38Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    —\n    CSV\nThe code uses thresholds=(1, 0.2) to set a ‘warning’ threshold of 1 and an ‘error’ threshold of 0.2 (which is 20%) failing test units. You might notice the following in the validation table:\nThe one final threshold, C (‘critical’), wasn’t set so appears on the validation table as a dash.",
    "crumbs": [
      "User Guide",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#two-types-of-threshold-values-proportional-and-absolute",
    "href": "user-guide/thresholds.html#two-types-of-threshold-values-proportional-and-absolute",
    "title": "Thresholds",
    "section": "Two Types of Threshold Values: Proportional and Absolute",
    "text": "Two Types of Threshold Values: Proportional and Absolute\nThreshold values can be specified in two ways:\n\nproportional: a decimal value like 0.1 is taken to mean 10% of all test units failed\nabsolute: a whole number represents a fixed number of test units failed\n\nThreshold values act as cutoffs and are inclusive. So, any value of failing test units greater than or equal to the threshold value will result in exceeding the threshold. So if a threshold is defined with a value of 5, then 5 failing test units will result in an exceedence.",
    "crumbs": [
      "User Guide",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#using-the-validationthresholds-argument",
    "href": "user-guide/thresholds.html#using-the-validationthresholds-argument",
    "title": "Thresholds",
    "section": "Using the Validation(thresholds=) Argument",
    "text": "Using the Validation(thresholds=) Argument\nWe can also define thresholds globally. This means that every validation step will re-use the same set of threshold values.\nimport pointblank as pb\n\nvalidation_2 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"), thresholds=(1, 0.1))\n    .col_vals_not_null(columns=\"a\")\n    .col_vals_gt(columns=\"b\", value=2)\n    .interrogate()\n)\n\nvalidation_2\nIn this, both the col_vals_not_null() and col_vals_gt() steps will use the thresholds= value set in the Validate call. Now, if you want to override these global threshold values for a given validation step, you can always use the threshold= argument when calling a validation method (this argument is present within every validation method).",
    "crumbs": [
      "User Guide",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#ways-to-define-thresholds",
    "href": "user-guide/thresholds.html#ways-to-define-thresholds",
    "title": "Thresholds",
    "section": "Ways to Define Thresholds",
    "text": "Ways to Define Thresholds\nThere are more than a few ways to set threshold levels. We provide this flexibility because it’s often useful to have simple shorthand methods for such a common task.\n\nUsing a Tuple or a Single Value\nThe fastest way to define a threshold is to use a tuple with positional entries for the ‘warning’, ‘error’, and ‘critical’ levels.\nthresholds_tuple = (1, 0.1, 0.25) # (warning, error, critical)\nNote that a shorter tuple is also allowed:\n\n(1, ): ‘warning’ state at 1 failing test unit\n(1, 0.1): ‘warning’ state at 1 failing test unit, error state at 10% failing test units\n\nYou can even use a scalar value (float between 0 and 1 or an integer). That single value represents the threshold value for the ‘warning’ level:\nthresholds_single = 1\nWhile using a tuple or a scalar can be very succinct, a problem that arises is that the ordering of values always begins at the ‘warning’ level. This means you cannot define a threshold level for just the ‘error’ level, for example. This is fine for many cases, however, there are other ways to express thresholds without these constraints.\n\n\nUsing the Thresholds Class\nUsing the Thresholds class lets you define the threshold levels using the warning=, error=, and critical= arguments. And unlike the method of setting thresholds with a tuple, any of the threshold levels can be unset. Here’s an example where you might want to set the ‘error’ and ‘critical’ levels (leaving the ‘warning’ level unset):\nthresholds_class = pb.Thresholds(error=0.3, critical=0.5)\n\n\nUsing a Dictionary to Set Thresholds\nA specially-crafted dictionary is acceptable as input to any thresholds= argument. You need to ensure that the keys are named using either \"warning\", \"error\", or \"critical\". Any combination of keys is fine, but be careful to use only the aforementioned names (otherwise, you’ll receive a ValueError). Here’s an example that sets the ‘warning’ and ‘critical’ levels:\nthresholds_dict = {\"warning\": 1, \"critical\": 0.1}",
    "crumbs": [
      "User Guide",
      "Thresholds"
    ]
  },
  {
    "objectID": "user-guide/types.html",
    "href": "user-guide/types.html",
    "title": "Validation Types",
    "section": "",
    "text": "The collection of validation methods in Pointblank allows you to express all sorts of checks on your DataFrames and database tables. We’ll use the small_table dataset for all of the examples shown here. Here’s a preview of it:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Validation Types"
    ]
  },
  {
    "objectID": "user-guide/types.html#more-information",
    "href": "user-guide/types.html#more-information",
    "title": "Validation Types",
    "section": "More Information",
    "text": "More Information\nThese are just a few examples of the many validation methods available in Pointblank. For more detailed information, check out the individual reference pages in the API Reference.",
    "crumbs": [
      "User Guide",
      "Validation Types"
    ]
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html",
    "href": "reference/Validate.get_sundered_data.html",
    "title": "Validate.get_sundered_data",
    "section": "",
    "text": "Validate.get_sundered_data(type='pass')\nGet the data that passed or failed the validation steps.\nValidation of the data is one thing but, sometimes, you want to use the best part of the input dataset for something else. The get_sundered_data() method works with a Validate object that has been interrogated (i.e., the interrogate() method was used). We can get either the ‘pass’ data piece (rows with no failing test units across all row-based validation functions), or, the ‘fail’ data piece (rows with at least one failing test unit across the same series of validations)."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#details",
    "href": "reference/Validate.get_sundered_data.html#details",
    "title": "Validate.get_sundered_data",
    "section": "Details",
    "text": "Details\nThere are some caveats to sundering. The validation steps considered for this splitting will only involve steps where:\n\nof certain check types, where test units are cells checked row-by-row (e.g., the col_vals_*() methods)\nactive= is not set to False\npre= has not been given an expression for modifying the input table\n\nSo long as these conditions are met, the data will be split into two constituent tables: one with the rows that passed all validation steps and another with the rows that failed at least one validation step."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#parameters",
    "href": "reference/Validate.get_sundered_data.html#parameters",
    "title": "Validate.get_sundered_data",
    "section": "Parameters",
    "text": "Parameters\n\ntype :  = 'pass'\n\nThe type of data to return. Options are \"pass\" or \"fail\", where the former returns a table only containing rows where test units always passed validation steps, and the latter returns a table only containing rows had test units that failed in at least one validation step."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#returns",
    "href": "reference/Validate.get_sundered_data.html#returns",
    "title": "Validate.get_sundered_data",
    "section": "Returns",
    "text": "Returns\n\n : FrameT\n\nA table containing the data that passed or failed the validation steps."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#examples",
    "href": "reference/Validate.get_sundered_data.html#examples",
    "title": "Validate.get_sundered_data",
    "section": "Examples",
    "text": "Examples\nLet’s create a Validate object with three validation steps and then interrogate the data.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 6, 9, 7, 3, 2],\n        \"b\": [9, 8, 10, 5, 10, 6],\n        \"c\": [\"c\", \"d\", \"a\", \"b\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:53:25Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-03-06 23:53:25 UTC&lt; 1 s2025-03-06 23:53:25 UTC\n  \n\n\n\n\n\n\n        \n\n\nFrom the validation table, we can see that the first and second steps each had 4 passing test units. A failing test unit will mark the entire row as failing in the context of the get_sundered_data() method. We can use this method to get the rows of data that passed the during interrogation.\n\npb.preview(validation.get_sundered_data())\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cString\n\n\n\n  \n    1\n    9\n    10\n    a\n  \n  \n    2\n    7\n    5\n    b\n  \n\n\n\n\n\n\n        \n\n\nThe returned DataFrame contains the rows that passed all validation steps (we passed this object to preview() to show it in an HTML view). From the six-row input DataFrame, the first two rows and the last two rows had test units that failed validation. Thus the middle two rows are the only ones that passed all validation steps and that’s what we see in the returned DataFrame."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html",
    "href": "reference/Validate.col_vals_between.html",
    "title": "Validate.col_vals_between",
    "section": "",
    "text": "Validate.col_vals_between(\n    columns,\n    left,\n    right,\n    inclusive=(True, True),\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo column data lie between two specified values or data in other columns?\nThe col_vals_between() validation method checks whether column values in a table fall within a range. The range is specified with three arguments: left=, right=, and inclusive=. The left= and right= values specify the lower and upper bounds. These bounds can be specified as literal values or as column names provided within col(). The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#parameters",
    "href": "reference/Validate.col_vals_between.html#parameters",
    "title": "Validate.col_vals_between",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nleft : float | int | Column\n\nThe lower bound of the range. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison for the lower bound.\n\nright : float | int | Column\n\nThe upper bound of the range. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison for the upper bound.\n\ninclusive : tuple[bool, bool] = (True, True)\n\nA tuple of two boolean values indicating whether the comparison should be inclusive. The position of the boolean values correspond to the left= and right= values, respectively. By default, both values are True.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#returns",
    "href": "reference/Validate.col_vals_between.html#returns",
    "title": "Validate.col_vals_between",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#examples",
    "href": "reference/Validate.col_vals_between.html#examples",
    "title": "Validate.col_vals_between",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [2, 3, 2, 4, 3, 4],\n        \"b\": [5, 6, 1, 6, 8, 5],\n        \"c\": [9, 8, 8, 7, 7, 8],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    2\n    5\n    9\n  \n  \n    2\n    3\n    6\n    8\n  \n  \n    3\n    2\n    1\n    8\n  \n  \n    4\n    4\n    6\n    7\n  \n  \n    5\n    3\n    8\n    7\n  \n  \n    6\n    4\n    5\n    8\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all between the fixed boundary values of 1 and 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_between(columns=\"a\", left=1, right=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    a\n    [1, 5]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_between(). All test units passed, and there are no failing test units.\nAside from checking a column against two literal values representing the lower and upper bounds, we can also provide column names to the left= and/or right= arguments (by using the helper function col()). In this way, we can perform three additional comparison types:\n\nleft=column, right=column\nleft=literal, right=column\nleft=column, right=literal\n\nFor the next example, we’ll use col_vals_between() to check whether the values in column b are between than corresponding values in columns a (lower bound) and c (upper bound).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_between(columns=\"b\", left=pb.col(\"a\"), right=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    b\n    [a, c]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 2: b is 1 but the bounds are 2 (a) and 8 (c).\nRow 4: b is 8 but the bounds are 3 (a) and 7 (c)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html",
    "href": "reference/Validate.rows_distinct.html",
    "title": "Validate.rows_distinct",
    "section": "",
    "text": "Validate.rows_distinct(\n    columns_subset=None,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether rows in the table are distinct.\nThe rows_distinct() method checks whether rows in the table are distinct. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#parameters",
    "href": "reference/Validate.rows_distinct.html#parameters",
    "title": "Validate.rows_distinct",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns_subset : str | list[str] | None = None\n\nA single column or a list of columns to use as a subset for the distinct comparison. If None, then all columns in the table will be used for the comparison. If multiple columns are supplied, the distinct comparison will be made over the combination of values in those columns.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#returns",
    "href": "reference/Validate.rows_distinct.html#returns",
    "title": "Validate.rows_distinct",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#examples",
    "href": "reference/Validate.rows_distinct.html#examples",
    "title": "Validate.rows_distinct",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three string columns (col_1, col_2, and col_3). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"col_1\": [\"a\", \"b\", \"c\", \"d\"],\n        \"col_2\": [\"a\", \"a\", \"c\", \"d\"],\n        \"col_3\": [\"a\", \"a\", \"d\", \"e\"],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  col_1String\n  col_2String\n  col_3String\n\n\n\n  \n    1\n    a\n    a\n    a\n  \n  \n    2\n    b\n    a\n    a\n  \n  \n    3\n    c\n    c\n    d\n  \n  \n    4\n    d\n    d\n    e\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the rows in the table are distinct with rows_distinct(). We’ll determine if this validation had any failing test units (there are four test units, one for each row). A failing test units means that a given row is not distinct from every other row.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n         rows_distinct()\n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom this validation table we see that there are no failing test units. All rows in the table are distinct from one another.\nWe can also use a subset of columns to determine distinctness. Let’s specify the subset using columns col_2 and col_3 for the next validation.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_distinct(columns_subset=[\"col_2\", \"col_3\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n         rows_distinct()\n        \n    col_2, col_3\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The first and second rows are duplicated when considering only the values in columns col_2 and col_3. There’s only one set of duplicates but there are two failing test units since each row is compared to all others."
  },
  {
    "objectID": "reference/DraftValidation.html",
    "href": "reference/DraftValidation.html",
    "title": "DraftValidation",
    "section": "",
    "text": "DraftValidation(self, data, model, api_key=None)\nDraft a validation plan for a given table using an LLM.\nBy using a large language model (LLM) to draft a validation plan, you can quickly generate a starting point for validating a table. This can be useful when you have a new table and you want to get a sense of how to validate it (and adjustments could always be made later). The DraftValidation class uses the chatlas package to draft a validation plan for a given table using an LLM from either the \"anthropic\", \"openai\", \"ollama\" or \"bedrock\" provider. You can install all requirements for the class through an optional ‘generate’ install of Pointblank via pip install pointblank[generate].\n\n\n\n\n\n\nWarning\n\n\n\nThe DraftValidation() class is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\ndata : FrameT | Any\n\nThe data to be used for drafting a validation plan.\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\napi_key : str | None = None\n\nThe API key to be used for the model.\n\n\n\n\n\n\n : str\n\nThe drafted validation plan.\n\n\n\n\n\nThe model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names.\n\n\n\nProviding a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way.\n\n\n\nThe data sent to the model provider is a JSON summary of the table. This data summary is generated internally by DraftValidation using the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information to draft a validation plan. As such, even very large tables can be used with the DraftValidation class since the contents of the table are not sent to the model provider.\nThe Amazon Bedrock is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally, though only a few models are capable enough to perform the task of drafting a validation plan.\n\n\n\nLet’s look at how the DraftValidation class can be used to draft a validation plan for a table. The table to be used is \"nycflights\", which is available here via the load_dataset() function. The model to be used is \"anthropic:claude-3-5-sonnet-latest\" (which performs very well compared to other LLMs). The example assumes that the API key is stored in an .env file as ANTHROPIC_API_KEY.\nimport pointblank as pb\n\n# Load the \"nycflights\" dataset as a DuckDB table\ndata = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\n# Draft a validation plan for the \"nycflights\" table\npb.DraftValidation(data=nycflights, model=\"anthropic:claude-3-5-sonnet-latest\")\nThe output will be a drafted validation plan for the \"nycflights\" table and this will appear in the console.\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"year\", \"int64\"),\n    (\"month\", \"int64\"),\n    (\"day\", \"int64\"),\n    (\"dep_time\", \"int64\"),\n    (\"sched_dep_time\", \"int64\"),\n    (\"dep_delay\", \"int64\"),\n    (\"arr_time\", \"int64\"),\n    (\"sched_arr_time\", \"int64\"),\n    (\"arr_delay\", \"int64\"),\n    (\"carrier\", \"string\"),\n    (\"flight\", \"int64\"),\n    (\"tailnum\", \"string\"),\n    (\"origin\", \"string\"),\n    (\"dest\", \"string\"),\n    (\"air_time\", \"int64\"),\n    (\"distance\", \"int64\"),\n    (\"hour\", \"int64\"),\n    (\"minute\", \"int64\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"year\", \"month\", \"day\", \"sched_dep_time\", \"carrier\", \"flight\",\n        \"origin\", \"dest\", \"distance\", \"hour\", \"minute\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12)\n    .col_vals_between(columns=\"day\", left=1, right=31)\n    .col_vals_between(columns=\"sched_dep_time\", left=106, right=2359)\n    .col_vals_between(columns=\"dep_delay\", left=-43, right=1301, na_pass=True)\n    .col_vals_between(columns=\"air_time\", left=20, right=695, na_pass=True)\n    .col_vals_between(columns=\"distance\", left=17, right=4983)\n    .col_vals_between(columns=\"hour\", left=1, right=23)\n    .col_vals_between(columns=\"minute\", left=0, right=59)\n    .col_vals_in_set(columns=\"origin\", set=[\"EWR\", \"LGA\", \"JFK\"])\n    .col_count_match(count=18)\n    .row_count_match(count=336776)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\nThe drafted validation plan can be copied and pasted into a Python script or notebook for further use. In other words, the generated plan can be adjusted as needed to suit the specific requirements of the table being validated.\nNote that the output does not know how the data was obtained, so it uses the placeholder your_data in the data= argument of the Validate class. When adapted for use, this should be replaced with the actual data variable."
  },
  {
    "objectID": "reference/DraftValidation.html#parameters",
    "href": "reference/DraftValidation.html#parameters",
    "title": "DraftValidation",
    "section": "",
    "text": "data : FrameT | Any\n\nThe data to be used for drafting a validation plan.\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\napi_key : str | None = None\n\nThe API key to be used for the model."
  },
  {
    "objectID": "reference/DraftValidation.html#returns",
    "href": "reference/DraftValidation.html#returns",
    "title": "DraftValidation",
    "section": "",
    "text": ": str\n\nThe drafted validation plan."
  },
  {
    "objectID": "reference/DraftValidation.html#constructing-the-model-argument",
    "href": "reference/DraftValidation.html#constructing-the-model-argument",
    "title": "DraftValidation",
    "section": "",
    "text": "The model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names."
  },
  {
    "objectID": "reference/DraftValidation.html#notes-on-authentication",
    "href": "reference/DraftValidation.html#notes-on-authentication",
    "title": "DraftValidation",
    "section": "",
    "text": "Providing a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way."
  },
  {
    "objectID": "reference/DraftValidation.html#notes-on-data-sent-to-the-model-provider",
    "href": "reference/DraftValidation.html#notes-on-data-sent-to-the-model-provider",
    "title": "DraftValidation",
    "section": "",
    "text": "The data sent to the model provider is a JSON summary of the table. This data summary is generated internally by DraftValidation using the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information to draft a validation plan. As such, even very large tables can be used with the DraftValidation class since the contents of the table are not sent to the model provider.\nThe Amazon Bedrock is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally, though only a few models are capable enough to perform the task of drafting a validation plan."
  },
  {
    "objectID": "reference/DraftValidation.html#examples",
    "href": "reference/DraftValidation.html#examples",
    "title": "DraftValidation",
    "section": "",
    "text": "Let’s look at how the DraftValidation class can be used to draft a validation plan for a table. The table to be used is \"nycflights\", which is available here via the load_dataset() function. The model to be used is \"anthropic:claude-3-5-sonnet-latest\" (which performs very well compared to other LLMs). The example assumes that the API key is stored in an .env file as ANTHROPIC_API_KEY.\nimport pointblank as pb\n\n# Load the \"nycflights\" dataset as a DuckDB table\ndata = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\n# Draft a validation plan for the \"nycflights\" table\npb.DraftValidation(data=nycflights, model=\"anthropic:claude-3-5-sonnet-latest\")\nThe output will be a drafted validation plan for the \"nycflights\" table and this will appear in the console.\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"year\", \"int64\"),\n    (\"month\", \"int64\"),\n    (\"day\", \"int64\"),\n    (\"dep_time\", \"int64\"),\n    (\"sched_dep_time\", \"int64\"),\n    (\"dep_delay\", \"int64\"),\n    (\"arr_time\", \"int64\"),\n    (\"sched_arr_time\", \"int64\"),\n    (\"arr_delay\", \"int64\"),\n    (\"carrier\", \"string\"),\n    (\"flight\", \"int64\"),\n    (\"tailnum\", \"string\"),\n    (\"origin\", \"string\"),\n    (\"dest\", \"string\"),\n    (\"air_time\", \"int64\"),\n    (\"distance\", \"int64\"),\n    (\"hour\", \"int64\"),\n    (\"minute\", \"int64\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"year\", \"month\", \"day\", \"sched_dep_time\", \"carrier\", \"flight\",\n        \"origin\", \"dest\", \"distance\", \"hour\", \"minute\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12)\n    .col_vals_between(columns=\"day\", left=1, right=31)\n    .col_vals_between(columns=\"sched_dep_time\", left=106, right=2359)\n    .col_vals_between(columns=\"dep_delay\", left=-43, right=1301, na_pass=True)\n    .col_vals_between(columns=\"air_time\", left=20, right=695, na_pass=True)\n    .col_vals_between(columns=\"distance\", left=17, right=4983)\n    .col_vals_between(columns=\"hour\", left=1, right=23)\n    .col_vals_between(columns=\"minute\", left=0, right=59)\n    .col_vals_in_set(columns=\"origin\", set=[\"EWR\", \"LGA\", \"JFK\"])\n    .col_count_match(count=18)\n    .row_count_match(count=336776)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\nThe drafted validation plan can be copied and pasted into a Python script or notebook for further use. In other words, the generated plan can be adjusted as needed to suit the specific requirements of the table being validated.\nNote that the output does not know how the data was obtained, so it uses the placeholder your_data in the data= argument of the Validate class. When adapted for use, this should be replaced with the actual data variable."
  },
  {
    "objectID": "reference/contains.html",
    "href": "reference/contains.html",
    "title": "contains",
    "section": "",
    "text": "contains(text, case_sensitive=False)\nSelect columns that contain specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The contains() selector function can be used to select one or more columns that contain some specified text. So if the set of table columns consists of\n[profit, conv_first, conv_last, highest_conv, age]\nand you want to validate columns that have \"conv\" in the name, you can use columns=contains(\"conv\"). This will select the conv_first, conv_last, and highest_conv columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using contains() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/contains.html#parameters",
    "href": "reference/contains.html#parameters",
    "title": "contains",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should contain.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/contains.html#returns",
    "href": "reference/contains.html#returns",
    "title": "contains",
    "section": "Returns",
    "text": "Returns\n\n : Contains\n\nA Contains object, which can be used to select columns that contain the specified text."
  },
  {
    "objectID": "reference/contains.html#relevant-validation-methods-where-contains-can-be-used",
    "href": "reference/contains.html#relevant-validation-methods-where-contains-can-be-used",
    "title": "contains",
    "section": "Relevant Validation Methods where contains() can be Used",
    "text": "Relevant Validation Methods where contains() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe contains() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/contains.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/contains.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "contains",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe contains() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that have the text \"_n\" and start with \"item\", you can use the contains() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(contains(\"_n\") & starts_with(\"item\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/contains.html#examples",
    "href": "reference/contains.html#examples",
    "title": "contains",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, 2021_pay_total, 2022_pay_total, and person_id and we’d like to validate that the values in columns having \"pay\" in the name are greater than 10. We can use the contains() column selector function to specify the column names that contain \"pay\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2021_pay_total\": [16.32, 16.25, 15.75],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.contains(\"pay\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2021_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2022_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2021_pay_total and one for 2022_pay_total. The values in both columns were all greater than 10.\nWe can also use the contains() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that contain \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.contains(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2023_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2024_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay_total and one for 2024_pay_total."
  },
  {
    "objectID": "reference/config.html",
    "href": "reference/config.html",
    "title": "config",
    "section": "",
    "text": "config(\n    report_incl_header=True,\n    report_incl_footer=True,\n    preview_incl_header=True,\n)\nConfiguration settings for the pointblank library.\n\n\n\nreport_incl_header : bool = True\n\nThis controls whether the header should be present in the validation table report. The header contains the table name, label information, and might contain global failure threshold levels (if set).\n\nreport_incl_footer : bool = True\n\nShould the footer of the validation table report be displayed? The footer contains the starting and ending times of the interrogation.\n\npreview_incl_header : bool = True\n\nWhether the header should be present in any preview table (generated via the preview() function).\n\n\n\n\n\n\n : PointblankConfig\n\nA PointblankConfig object with the specified configuration settings."
  },
  {
    "objectID": "reference/config.html#parameters",
    "href": "reference/config.html#parameters",
    "title": "config",
    "section": "",
    "text": "report_incl_header : bool = True\n\nThis controls whether the header should be present in the validation table report. The header contains the table name, label information, and might contain global failure threshold levels (if set).\n\nreport_incl_footer : bool = True\n\nShould the footer of the validation table report be displayed? The footer contains the starting and ending times of the interrogation.\n\npreview_incl_header : bool = True\n\nWhether the header should be present in any preview table (generated via the preview() function)."
  },
  {
    "objectID": "reference/config.html#returns",
    "href": "reference/config.html#returns",
    "title": "config",
    "section": "",
    "text": ": PointblankConfig\n\nA PointblankConfig object with the specified configuration settings."
  },
  {
    "objectID": "reference/Validate.n_passed.html",
    "href": "reference/Validate.n_passed.html",
    "title": "Validate.n_passed",
    "section": "",
    "text": "Validate.n_passed(i=None, scalar=False)\nProvides a dictionary of the number of test units that passed for each validation step.\nThe n_passed() method provides the number of test units that passed for each validation step. This is the number of test units that passed in the the validation step. It is always some integer value between 0 and the total number of test units.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of passing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the n_passed() method (i.e., n - n_failed)."
  },
  {
    "objectID": "reference/Validate.n_passed.html#parameters",
    "href": "reference/Validate.n_passed.html#parameters",
    "title": "Validate.n_passed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of passing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n_passed.html#returns",
    "href": "reference/Validate.n_passed.html#returns",
    "title": "Validate.n_passed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of passing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n_passed.html#examples",
    "href": "reference/Validate.n_passed.html#examples",
    "title": "Validate.n_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps and, as it turns out, all of them will have failing test units. After interrogation, the n_passed() method is used to determine the number of passing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12],\n        \"b\": [9, 8, 10, 5, 10],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.n_passed()\n\n{1: 4, 2: 3, 3: 4}\n\n\nThe returned dictionary shows that all validation steps had no passing test units (each value was less than 5, which is the total number of test units for each step).\nIf we wanted to check the number of passing test units for a single validation step, we can provide the step number. Also, we could forego the dictionary and get a scalar value by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.n_passed(i=1)\n\n{1: 4}\n\n\nThe returned value of 4 is the number of passing test units for the first validation step."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html",
    "href": "reference/Validate.get_tabular_report.html",
    "title": "Validate.get_tabular_report",
    "section": "",
    "text": "Validate.get_tabular_report(\n    title=':default:',\n    incl_header=None,\n    incl_footer=None,\n)\nValidation report as a GT table.\nThe get_tabular_report() method returns a GT table object that represents the validation report. This validation table provides a summary of the validation results, including the validation steps, the number of test units, the number of failing test units, and the fraction of failing test units. The table also includes status indicators for the ‘warning’, ‘error’, and ‘critical’ levels.\nYou could simply display the validation table without the use of the get_tabular_report() method. However, the method provides a way to customize the title of the report. In the future this method may provide additional options for customizing the report."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#parameters",
    "href": "reference/Validate.get_tabular_report.html#parameters",
    "title": "Validate.get_tabular_report",
    "section": "Parameters",
    "text": "Parameters\n\ntitle : str | None = ':default:'\n\nOptions for customizing the title of the report. The default is the \":default:\" value which produces a generic title. Another option is \":tbl_name:\", and that presents the name of the table as the title for the report. If no title is wanted, then \":none:\" can be used. Aside from keyword options, text can be provided for the title. This will be interpreted as Markdown text and transformed internally to HTML."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#returns",
    "href": "reference/Validate.get_tabular_report.html#returns",
    "title": "Validate.get_tabular_report",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT table object that represents the validation report."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#examples",
    "href": "reference/Validate.get_tabular_report.html#examples",
    "title": "Validate.get_tabular_report",
    "section": "Examples",
    "text": "Examples\nLet’s create a Validate object with a few validation steps and then interrogate the data table to see how it performs against the validation plan. We can then generate a tabular report to get a summary of the results.\n\nimport pointblank as pb\nimport polars as pl\n\n# Create a Polars DataFrame\ntbl_pl = pl.DataFrame({\"x\": [1, 2, 3, 4], \"y\": [4, 5, 6, 7]})\n\n# Validate data using Polars DataFrame\nvalidation = (\n    pb.Validate(data=tbl_pl, tbl_name=\"tbl_xy\", thresholds=(2, 3, 4))\n    .col_vals_gt(columns=\"x\", value=1)\n    .col_vals_lt(columns=\"x\", value=3)\n    .col_vals_le(columns=\"y\", value=7)\n    .interrogate()\n)\n\n# Look at the validation table\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:52:47Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:52:47 UTC&lt; 1 s2025-03-06 23:52:47 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe validation table is displayed with a default title (‘Validation Report’). We can use the get_tabular_report() method to customize the title of the report. For example, we can set the title to the name of the table by using the title=\":tbl_name:\" option. This will use the string provided in the tbl_name= argument of the Validate object.\n\nvalidation.get_tabular_report(title=\":tbl_name:\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    tbl_xy\n  \n  \n    2025-03-06|23:52:47Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:52:47 UTC&lt; 1 s2025-03-06 23:52:47 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe title of the report is now set to the name of the table, which is ‘tbl_xy’. This can be useful if you have multiple tables and want to keep track of which table the validation report is for.\nAlternatively, you can provide your own title for the report.\n\nvalidation.get_tabular_report(title=\"Report for Table XY\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Table XY\n\n  \n  \n    2025-03-06|23:52:47Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:52:47 UTC&lt; 1 s2025-03-06 23:52:47 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe title of the report is now set to ‘Report for Table XY’. This can be useful if you want to provide a more descriptive title for the report."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html",
    "href": "reference/Validate.col_vals_in_set.html",
    "title": "Validate.col_vals_in_set",
    "section": "",
    "text": "Validate.col_vals_in_set(\n    columns,\n    set,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values are in a set of values.\nThe col_vals_in_set() validation method checks whether column values in a table are part of a specified set= of values. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#parameters",
    "href": "reference/Validate.col_vals_in_set.html#parameters",
    "title": "Validate.col_vals_in_set",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nset : list[float | int]\n\nA list of values to compare against.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#returns",
    "href": "reference/Validate.col_vals_in_set.html#returns",
    "title": "Validate.col_vals_in_set",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#examples",
    "href": "reference/Validate.col_vals_in_set.html#examples",
    "title": "Validate.col_vals_in_set",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 2, 4, 6, 2, 5],\n        \"b\": [5, 8, 2, 6, 5, 1],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    2\n    8\n  \n  \n    3\n    4\n    2\n  \n  \n    4\n    6\n    6\n  \n  \n    5\n    2\n    5\n  \n  \n    6\n    5\n    1\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all in the set of [2, 3, 4, 5, 6]. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_in_set(columns=\"a\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    a\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_in_set(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_in_set(columns=\"b\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    b\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the column b values of 8 and 1, which are not in the set of [2, 3, 4, 5, 6]."
  },
  {
    "objectID": "reference/ends_with.html",
    "href": "reference/ends_with.html",
    "title": "ends_with",
    "section": "",
    "text": "ends_with(text, case_sensitive=False)\nSelect columns that end with specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The ends_with() selector function can be used to select one or more columns that end with some specified text. So if the set of table columns consists of\n[first_name, last_name, age, address]\nand you want to validate columns that end with \"name\", you can use columns=ends_with(\"name\"). This will select the first_name and last_name columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using ends_with() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/ends_with.html#parameters",
    "href": "reference/ends_with.html#parameters",
    "title": "ends_with",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should end with.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/ends_with.html#returns",
    "href": "reference/ends_with.html#returns",
    "title": "ends_with",
    "section": "Returns",
    "text": "Returns\n\n : EndsWith\n\nAn EndsWith object, which can be used to select columns that end with the specified text."
  },
  {
    "objectID": "reference/ends_with.html#relevant-validation-methods-where-ends_with-can-be-used",
    "href": "reference/ends_with.html#relevant-validation-methods-where-ends_with-can-be-used",
    "title": "ends_with",
    "section": "Relevant Validation Methods where ends_with() can be Used",
    "text": "Relevant Validation Methods where ends_with() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe ends_with() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/ends_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/ends_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "ends_with",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe ends_with() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that end with \"e\" and start with \"a\", you can use the ends_with() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(ends_with(\"e\") & starts_with(\"a\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/ends_with.html#examples",
    "href": "reference/ends_with.html#examples",
    "title": "ends_with",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, 2021_pay, 2022_pay, and person_id and we’d like to validate that the values in columns that end with \"pay\" are greater than 10. We can use the ends_with() column selector function to specify the columns that end with \"pay\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2021_pay\": [16.32, 16.25, 15.75],\n        \"2022_pay\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.ends_with(\"pay\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2021_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2022_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2021_pay and one for 2022_pay. The values in both columns were all greater than 10.\nWe can also use the ends_with() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that end with \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay\": [18.62, 16.95, 18.25],\n        \"2023_pay\": [19.29, 17.75, 18.35],\n        \"2024_pay\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.ends_with(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2023_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2024_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay and one for 2024_pay."
  },
  {
    "objectID": "reference/Validate.f_failed.html",
    "href": "reference/Validate.f_failed.html",
    "title": "Validate.f_failed",
    "section": "",
    "text": "Validate.f_failed(i=None, scalar=False)\nProvides a dictionary of the fraction of test units that failed for each validation step.\nA measure of the fraction of test units that failed is provided by the f_failed attribute. This is the fraction of test units that failed the validation step over the total number of test units. Given this is a fractional value, it will always be in the range of 0 to 1.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThis method provides a dictionary of the fraction of failing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the f_passed() method (i.e., 1 - f_passed())."
  },
  {
    "objectID": "reference/Validate.f_failed.html#parameters",
    "href": "reference/Validate.f_failed.html#parameters",
    "title": "Validate.f_failed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the fraction of failing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.f_failed.html#returns",
    "href": "reference/Validate.f_failed.html#returns",
    "title": "Validate.f_failed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, float] | float\n\nA dictionary of the fraction of failing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.f_failed.html#examples",
    "href": "reference/Validate.f_failed.html#examples",
    "title": "Validate.f_failed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, all having some failing test units. After interrogation, the f_failed() method is used to determine the fraction of failing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\", \"d\", \"c\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.f_failed()\n\n{1: 0.2857142857142857, 2: 0.42857142857142855, 3: 0.42857142857142855}\n\n\nThe returned dictionary shows the fraction of failing test units for each validation step. The values are all greater than 0 since there were failing test units in each step.\nIf we wanted to check the fraction of failing test units for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.f_failed(i=1)\n\n{1: 0.2857142857142857}\n\n\nThe returned value is the proportion of failing test units for the first validation step (2 failing test units out of 7 total test units)."
  },
  {
    "objectID": "reference/Validate.n.html",
    "href": "reference/Validate.n.html",
    "title": "Validate.n",
    "section": "",
    "text": "Validate.n(i=None, scalar=False)\nProvides a dictionary of the number of test units for each validation step.\nThe n() method provides the number of test units for each validation step. This is the total number of test units that were evaluated in the validation step. It is always an integer value.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. The total number of test units for a validation step is the sum of the number of passing and failing test units (i.e., n = n_passed + n_failed)."
  },
  {
    "objectID": "reference/Validate.n.html#parameters",
    "href": "reference/Validate.n.html#parameters",
    "title": "Validate.n",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n.html#returns",
    "href": "reference/Validate.n.html#returns",
    "title": "Validate.n",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n.html#examples",
    "href": "reference/Validate.n.html#examples",
    "title": "Validate.n",
    "section": "Examples",
    "text": "Examples\nDifferent types of validation steps can have different numbers of test units. In the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the number of test units for each step will be a little bit different.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 9, 5],\n        \"b\": [5, 6, 10, 3],\n        \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_exists(columns=\"b\")\n    .col_vals_lt(columns=\"b\", value=9, pre=lambda df: df.filter(pl.col(\"a\") &gt; 1))\n    .interrogate()\n)\n\nThe first validation step checks that all values in column a are greater than 0. Let’s use the n() method to determine the number of test units this validation step.\n\nvalidation.n(i=1, scalar=True)\n\n4\n\n\nThe returned value of 4 is the number of test units for the first validation step. This value is the same as the number of rows in the table.\nThe second validation step checks for the existence of column b. Using the n() method we can get the number of test units for this the second step.\n\nvalidation.n(i=2, scalar=True)\n\n1\n\n\nThere’s a single test unit here because the validation step is checking for the presence of a single column.\nThe third validation step checks that all values in column b are less than 9 after filtering the table to only include rows where the value in column a is greater than 1. Because the table is filtered, the number of test units will be less than the total number of rows in the input table. Let’s prove this by using the n() method.\n\nvalidation.n(i=3, scalar=True)\n\n3\n\n\nThe returned value of 3 is the number of test units for the third validation step. When using the pre= argument, the input table can be mutated before performing the validation. The n() method is a good way to determine whether the mutation performed as expected.\nIn all of these examples, the scalar=True argument was used to return the value as a scalar integer value. If scalar=False, the method will return a dictionary with an entry for the validation step number (from the i= argument) and the number of test units. Futhermore, leaving out the i= argument altogether will return a dictionary with filled with the number of test units for each validation step. Here’s what that looks like:\n\nvalidation.n()\n\n{1: 4, 2: 1, 3: 3}"
  },
  {
    "objectID": "reference/Thresholds.html",
    "href": "reference/Thresholds.html",
    "title": "Thresholds",
    "section": "",
    "text": "Thresholds(self, warning=None, error=None, critical=None)\nDefinition of threshold values.\nThresholds are used to set limits on the number of failing test units at different levels. The levels are ‘warning’, ‘error’, and ‘critical’. These levels correspond to different levels of severity when a threshold is reached. The threshold values can be set as absolute counts or as fractions of the total number of test units. When a threshold is reached, an action can be taken (e.g., displaying a message or calling a function) if there is an associated action defined for that level (defined through the Actions class)."
  },
  {
    "objectID": "reference/Thresholds.html#parameters",
    "href": "reference/Thresholds.html#parameters",
    "title": "Thresholds",
    "section": "Parameters",
    "text": "Parameters\n\nwarning : int | float | bool | None = None\n\nThe threshold for the ‘warning’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1.\n\nerror : int | float | bool | None = None\n\nThe threshold for the ‘error’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1.\n\ncritical : int | float | bool | None = None\n\nThe threshold for the ‘critical’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1."
  },
  {
    "objectID": "reference/Thresholds.html#returns",
    "href": "reference/Thresholds.html#returns",
    "title": "Thresholds",
    "section": "Returns",
    "text": "Returns\n\n : Thresholds\n\nA Thresholds object. This can be used when using the Validate class (to set thresholds globally) or when defining validation steps like col_vals_gt() (so that threshold values are scoped to individual validation steps, overriding any global thresholds)."
  },
  {
    "objectID": "reference/Thresholds.html#examples",
    "href": "reference/Thresholds.html#examples",
    "title": "Thresholds",
    "section": "Examples",
    "text": "Examples\nIn a data validation workflow, you can set thresholds for the number of failing test units at different levels. For example, you can set a threshold for the ‘warning’ level when the number of failing test units exceeds 10% of the total number of test units:\n\nthresholds_1 = pb.Thresholds(warning=0.1)\n\nYou can also set thresholds for the ‘error’ and ‘critical’ levels:\n\nthresholds_2 = pb.Thresholds(warning=0.1, error=0.2, critical=0.05)\n\nThresholds can also be set as absolute counts. Here’s an example where the ‘warning’ level is set to 5 failing test units:\n\nthresholds_3 = pb.Thresholds(warning=5)\n\nThe thresholds object can be used to set global thresholds for all validation steps. Or, you can set thresholds for individual validation steps, which will override the global thresholds. Here’s a data validation workflow example where we set global thresholds and then override with different thresholds at the col_vals_gt() step:\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        label=\"Example Validation\",\n        thresholds=pb.Thresholds(warning=0.1, error=0.2, critical=0.3)\n    )\n    .col_vals_not_null(columns=[\"c\", \"d\"])\n    .col_vals_gt(columns=\"a\", value=3, thresholds=pb.Thresholds(warning=5))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolarsWARNING0.1ERROR0.2CRITICAL0.3\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    d\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #AAAAAA\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    ●\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nAs can be seen, the last step (col_vals_gt()) has its own thresholds, which override the global thresholds set at the beginning of the validation workflow (in the Validate class)."
  },
  {
    "objectID": "reference/Validate.f_passed.html",
    "href": "reference/Validate.f_passed.html",
    "title": "Validate.f_passed",
    "section": "",
    "text": "Validate.f_passed(i=None, scalar=False)\nProvides a dictionary of the fraction of test units that passed for each validation step.\nA measure of the fraction of test units that passed is provided by the f_passed attribute. This is the fraction of test units that passed the validation step over the total number of test units. Given this is a fractional value, it will always be in the range of 0 to 1.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThis method provides a dictionary of the fraction of passing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the f_failed() method (i.e., 1 - f_failed())."
  },
  {
    "objectID": "reference/Validate.f_passed.html#parameters",
    "href": "reference/Validate.f_passed.html#parameters",
    "title": "Validate.f_passed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the fraction of passing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.f_passed.html#returns",
    "href": "reference/Validate.f_passed.html#returns",
    "title": "Validate.f_passed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, float] | float\n\nA dictionary of the fraction of passing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.f_passed.html#examples",
    "href": "reference/Validate.f_passed.html#examples",
    "title": "Validate.f_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, all having some failing test units. After interrogation, the f_passed() method is used to determine the fraction of passing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\", \"d\", \"c\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.f_passed()\n\n{1: 0.7142857142857143, 2: 0.5714285714285714, 3: 0.5714285714285714}\n\n\nThe returned dictionary shows the fraction of passing test units for each validation step. The values are all less than 1 since there were failing test units in each step.\nIf we wanted to check the fraction of passing test units for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.f_passed(i=1)\n\n{1: 0.7142857142857143}\n\n\nThe returned value is the proportion of passing test units for the first validation step (5 passing test units out of 7 total test units)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html",
    "href": "reference/Validate.col_vals_ne.html",
    "title": "Validate.col_vals_ne",
    "section": "",
    "text": "Validate.col_vals_ne(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data not equal to a fixed value or data in another column?\nThe col_vals_ne() validation method checks whether column values in a table are not equal to a specified value= (the exact comparison used in this function is col_val != value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#parameters",
    "href": "reference/Validate.col_vals_ne.html#parameters",
    "title": "Validate.col_vals_ne",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#returns",
    "href": "reference/Validate.col_vals_ne.html#returns",
    "title": "Validate.col_vals_ne",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#examples",
    "href": "reference/Validate.col_vals_ne.html#examples",
    "title": "Validate.col_vals_ne",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 5, 5, 5, 5, 5],\n        \"b\": [5, 6, 3, 6, 5, 8],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    5\n    6\n  \n  \n    3\n    5\n    3\n  \n  \n    4\n    5\n    6\n  \n  \n    5\n    5\n    5\n  \n  \n    6\n    5\n    8\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are not equal to the value of 3. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ne(columns=\"a\", value=3)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_equal\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ne()\n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_ne(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col()) to perform a column-column comparison. For the next example, we’ll use col_vals_ne() to check whether the values in column a aren’t equal to the values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ne(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_not_equal\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ne()\n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are in rows 0 and 4, where a is 5 and b is 5 in both cases (i.e., they are equal to each other)."
  },
  {
    "objectID": "reference/everything.html",
    "href": "reference/everything.html",
    "title": "everything",
    "section": "",
    "text": "everything()\nSelect all columns.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The everything() selector function can be used to select every column in the table. If you have a table with six columns and they’re all suitable for a specific type of validation, you can use columns=everything()) and all six columns will be selected for validation."
  },
  {
    "objectID": "reference/everything.html#returns",
    "href": "reference/everything.html#returns",
    "title": "everything",
    "section": "Returns",
    "text": "Returns\n\n : Everything\n\nAn Everything object, which can be used to select all columns."
  },
  {
    "objectID": "reference/everything.html#relevant-validation-methods-where-everything-can-be-used",
    "href": "reference/everything.html#relevant-validation-methods-where-everything-can-be-used",
    "title": "everything",
    "section": "Relevant Validation Methods where everything() can be Used",
    "text": "Relevant Validation Methods where everything() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe everything() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/everything.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/everything.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "everything",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe everything() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names except those having starting with “id_”, you can use the everything() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(everything() - starts_with(\"id_\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/everything.html#examples",
    "href": "reference/everything.html#examples",
    "title": "everything",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with several numeric columns and we’d like to validate that all these columns have less than 1000. We can use the everything() column selector function to select all columns for validation.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.everything(), value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2023_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2024_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2023_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2024_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps, one each column in the table. The values in every column were all lower than 1000.\nWe can also use the everything() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select every column except those that begin with \"2023\" we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.col(pb.everything() - pb.starts_with(\"2023\")), value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2024_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2024_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2024_hours and one for 2024_pay_total."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html",
    "href": "reference/Validate.col_vals_outside.html",
    "title": "Validate.col_vals_outside",
    "section": "",
    "text": "Validate.col_vals_outside(\n    columns,\n    left,\n    right,\n    inclusive=(True, True),\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo column data lie outside of two specified values or data in other columns?\nThe col_vals_between() validation method checks whether column values in a table do not fall within a certain range. The range is specified with three arguments: left=, right=, and inclusive=. The left= and right= values specify the lower and upper bounds. These bounds can be specified as literal values or as column names provided within col(). The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#parameters",
    "href": "reference/Validate.col_vals_outside.html#parameters",
    "title": "Validate.col_vals_outside",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nleft : float | int | Column\n\nThe lower bound of the range. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison for the lower bound.\n\nright : float | int | Column\n\nThe upper bound of the range. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison for the upper bound.\n\ninclusive : tuple[bool, bool] = (True, True)\n\nA tuple of two boolean values indicating whether the comparison should be inclusive. The position of the boolean values correspond to the left= and right= values, respectively. By default, both values are True.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#returns",
    "href": "reference/Validate.col_vals_outside.html#returns",
    "title": "Validate.col_vals_outside",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#examples",
    "href": "reference/Validate.col_vals_outside.html#examples",
    "title": "Validate.col_vals_outside",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 5, 5],\n        \"b\": [2, 3, 6, 4, 3, 6],\n        \"c\": [9, 8, 8, 9, 9, 7],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    2\n    9\n  \n  \n    2\n    6\n    3\n    8\n  \n  \n    3\n    5\n    6\n    8\n  \n  \n    4\n    7\n    4\n    9\n  \n  \n    5\n    5\n    3\n    9\n  \n  \n    6\n    5\n    6\n    7\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all outside the fixed boundary values of 1 and 4. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_outside(columns=\"a\", left=1, right=4)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_between\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n         col_vals_outside()\n        \n    a\n    [1, 4]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_outside(). All test units passed, and there are no failing test units.\nAside from checking a column against two literal values representing the lower and upper bounds, we can also provide column names to the left= and/or right= arguments (by using the helper function col()). In this way, we can perform three additional comparison types:\n\nleft=column, right=column\nleft=literal, right=column\nleft=column, right=literal\n\nFor the next example, we’ll use col_vals_outside() to check whether the values in column b are outside of the range formed by the corresponding values in columns a (lower bound) and c (upper bound).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_outside(columns=\"b\", left=pb.col(\"a\"), right=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_not_between\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n         col_vals_outside()\n        \n    b\n    [a, c]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 2: b is 6 and the bounds are 5 (a) and 8 (c).\nRow 5: b is 6 and the bounds are 5 (a) and 7 (c)."
  },
  {
    "objectID": "reference/Validate.assert_passing.html",
    "href": "reference/Validate.assert_passing.html",
    "title": "Validate.assert_passing",
    "section": "",
    "text": "Validate.assert_passing()\nRaise an AssertionError if all tests are not passing.\nThe assert_passing() method will raise an AssertionError if a test does not pass. This method simply wraps all_passed for more ready use in test suites. The step number and assertion made is printed in the AssertionError message if a failure occurs, ensuring some details are preserved."
  },
  {
    "objectID": "reference/Validate.assert_passing.html#raises",
    "href": "reference/Validate.assert_passing.html#raises",
    "title": "Validate.assert_passing",
    "section": "Raises",
    "text": "Raises\n\n: AssertionError\n\nIf any validation step has failing test units."
  },
  {
    "objectID": "reference/Validate.assert_passing.html#examples",
    "href": "reference/Validate.assert_passing.html#examples",
    "title": "Validate.assert_passing",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the second step will have a failing test unit (the value 10 isn’t less than 9). After interrogation, the assert_passing() method is used to assert that all validation steps passed perfectly.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n    \"a\": [1, 2, 9, 5],\n    \"b\": [5, 6, 10, 3],\n    \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_vals_lt(columns=\"b\", value=9) # this assertion is false\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.assert_passing()\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[1], line 20\n      4 tbl = pl.DataFrame(\n      5     {\n      6     \"a\": [1, 2, 9, 5],\n   (...)\n      9     }\n     10 )\n     12 validation = (\n     13     pb.Validate(data=tbl)\n     14     .col_vals_gt(columns=\"a\", value=0)\n   (...)\n     17     .interrogate()\n     18 )\n---&gt; 20 validation.assert_passing()\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pointblank/validate.py:5448, in Validate.assert_passing(self)\n   5440 failed_steps = [\n   5441     (i, str(step.autobrief))\n   5442     for i, step in enumerate(self.validation_info)\n   5443     if step.n_failed &gt; 0\n   5444 ]\n   5445 msg = \"The following assertions failed:\\n\" + \"\\n\".join(\n   5446     [f\"- Step {i + 1}: {autobrief}\" for i, autobrief in failed_steps]\n   5447 )\n-&gt; 5448 raise AssertionError(msg)\n\nAssertionError: The following assertions failed:\n- Step 2: Expect that values in `b` should be &lt; `9`."
  },
  {
    "objectID": "reference/Validate.get_step_report.html",
    "href": "reference/Validate.get_step_report.html",
    "title": "Validate.get_step_report",
    "section": "",
    "text": "Validate.get_step_report(i)\nGet a detailed report for a single validation step.\nThe get_step_report() method returns a report of what went well, or what failed spectacularly, for a given validation step. The report includes a summary of the validation step and a detailed breakdown of the interrogation results. The report is presented as a GT table object, which can be displayed in a notebook or exported to an HTML file."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#parameters",
    "href": "reference/Validate.get_step_report.html#parameters",
    "title": "Validate.get_step_report",
    "section": "Parameters",
    "text": "Parameters\n\ni : int\n\nThe step number for which to get a detailed report."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#returns",
    "href": "reference/Validate.get_step_report.html#returns",
    "title": "Validate.get_step_report",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT table object that represents the detailed report for the validation step."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#examples",
    "href": "reference/Validate.get_step_report.html#examples",
    "title": "Validate.get_step_report",
    "section": "Examples",
    "text": "Examples\nLet’s create a validation plan with a few validation steps and interrogate the data. With that, we’ll have a look at the validation reporting table for the entire collection of steps and what went well or what failed.\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Example for the get_step_report() method\",\n        thresholds=(1, 0.20, 0.40)\n    )\n    .col_vals_lt(columns=\"d\", value=3500)\n    .col_vals_between(columns=\"c\", left=1, right=8)\n    .col_vals_gt(columns=\"a\", value=3)\n    .col_vals_regex(columns=\"b\", pattern=r\"\\d-[a-z]{3}-\\d{3}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    d\n    3500\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #EBBC14\n    2\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    c\n    [1, 8]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #FF3300\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    b\n    \\d-[a-z]{3}-\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThere were four validation steps performed, where the first three steps had failing test units and the last step had no failures. Let’s get a detailed report for the first step by using the get_step_report() method.\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1\n  \n  \n    ASSERTION d &lt; 35002 / 13 TEST UNIT FAILURES IN COLUMN 6EXTRACT OF 2 ROWS WITH TEST UNIT FAILURES IN RED:\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    \n    3892.4\n    False\n    mid\n  \n\n\n\n\n\n\n        \n\n\nThe report for the first step is displayed. The report includes a summary of the validation step and a detailed breakdown of the interrogation results. The report provides details on what the validation step was checking, the extent to which the test units failed, and a table that shows the failing rows of the data with the column of interest highlighted.\nThe second and third steps also had failing test units. Reports for those steps can be viewed by using get_step_report(i=2) and get_step_report(i=3) respectively.\nThe final step did not have any failing test units. A report for the final step can still be viewed by using get_step_report(i=4). The report will indicate that every test unit passed and a prview of the target table will be provided.\n\nvalidation.get_step_report(i=4)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 4 ✓\n  \n  \n    ASSERTION b matches regex \\d-[a-z]{3}-\\d{3}13 TEST UNITS ALL PASSED IN COLUMN 4PREVIEW OF TARGET TABLE:\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "reference/Validate.warning.html",
    "href": "reference/Validate.warning.html",
    "title": "Validate.warning",
    "section": "",
    "text": "Validate.warning(i=None, scalar=False)\nGet the ‘warning’ level status for each validation step.\nThe ‘warning’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the ‘warning’ level. Otherwise, the status is False.\nThe ascribed name of ‘warning’ is semantic and does not imply that a warning message is generated, it is simply a status indicator that could be used to trigger some action to be taken. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the ‘warning’ status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.warning.html#parameters",
    "href": "reference/Validate.warning.html#parameters",
    "title": "Validate.warning",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the ‘warning’ status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.warning.html#returns",
    "href": "reference/Validate.warning.html#returns",
    "title": "Validate.warning",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the ‘warning’ status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.warning.html#examples",
    "href": "reference/Validate.warning.html#examples",
    "title": "Validate.warning",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have some failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the warning() method is used to determine the ‘warning’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.warning()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘warning’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘warning’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘warning’ level.\nWe can also visually inspect the ‘warning’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:51:39PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    50.71\n    20.29\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:51:39 UTC&lt; 1 s2025-03-06 23:51:39 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there’s a filled gray circle in the first step (look to the far right side, in the W column) indicating that the ‘warning’ threshold was met. The other steps have empty gray circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘warning’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.warning(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had met the ‘warning’ threshold."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html",
    "href": "reference/Validate.col_vals_gt.html",
    "title": "Validate.col_vals_gt",
    "section": "",
    "text": "Validate.col_vals_gt(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data greater than a fixed value or data in another column?\nThe col_vals_gt() validation method checks whether column values in a table are greater than a specified value= (the exact comparison used in this function is col_val &gt; value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#parameters",
    "href": "reference/Validate.col_vals_gt.html#parameters",
    "title": "Validate.col_vals_gt",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#returns",
    "href": "reference/Validate.col_vals_gt.html#returns",
    "title": "Validate.col_vals_gt",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#examples",
    "href": "reference/Validate.col_vals_gt.html#examples",
    "title": "Validate.col_vals_gt",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 6, 5],\n        \"b\": [1, 2, 1, 2, 2, 2],\n        \"c\": [2, 1, 2, 2, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    2\n    1\n  \n  \n    3\n    5\n    1\n    2\n  \n  \n    4\n    7\n    2\n    2\n  \n  \n    5\n    6\n    2\n    3\n  \n  \n    6\n    5\n    2\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all greater than the value of 4. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=4)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_gt(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col()) to perform a column-column comparison. For the next example, we’ll use col_vals_gt() to check whether the values in column c are greater than values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"c\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    c\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 1: c is 1 and b is 2.\nRow 3: c is 2 and b is 2."
  },
  {
    "objectID": "reference/Validate.get_json_report.html",
    "href": "reference/Validate.get_json_report.html",
    "title": "Validate.get_json_report",
    "section": "",
    "text": "Validate.get_json_report(use_fields=None, exclude_fields=None)\nGet a report of the validation results as a JSON-formatted string.\n\n\n\nuse_fields : list[str] | None = None\n\nA list of fields to include in the report. If None, all fields are included.\n\nexclude_fields : list[str] | None = None\n\nA list of fields to exclude from the report. If None, no fields are excluded.\n\n\n\n\n\n\n : str\n\nA JSON-formatted string representing the validation report."
  },
  {
    "objectID": "reference/Validate.get_json_report.html#parameters",
    "href": "reference/Validate.get_json_report.html#parameters",
    "title": "Validate.get_json_report",
    "section": "",
    "text": "use_fields : list[str] | None = None\n\nA list of fields to include in the report. If None, all fields are included.\n\nexclude_fields : list[str] | None = None\n\nA list of fields to exclude from the report. If None, no fields are excluded."
  },
  {
    "objectID": "reference/Validate.get_json_report.html#returns",
    "href": "reference/Validate.get_json_report.html#returns",
    "title": "Validate.get_json_report",
    "section": "",
    "text": ": str\n\nA JSON-formatted string representing the validation report."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html",
    "href": "reference/Validate.col_vals_eq.html",
    "title": "Validate.col_vals_eq",
    "section": "",
    "text": "Validate.col_vals_eq(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data equal to a fixed value or data in another column?\nThe col_vals_eq() validation method checks whether column values in a table are equal to a specified value= (the exact comparison used in this function is col_val == value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#parameters",
    "href": "reference/Validate.col_vals_eq.html#parameters",
    "title": "Validate.col_vals_eq",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#returns",
    "href": "reference/Validate.col_vals_eq.html#returns",
    "title": "Validate.col_vals_eq",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#examples",
    "href": "reference/Validate.col_vals_eq.html#examples",
    "title": "Validate.col_vals_eq",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 5, 5, 5, 5, 5],\n        \"b\": [5, 5, 5, 6, 5, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    5\n    5\n  \n  \n    3\n    5\n    5\n  \n  \n    4\n    5\n    6\n  \n  \n    5\n    5\n    5\n  \n  \n    6\n    5\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all equal to the value of 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_eq(columns=\"a\", value=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_equal\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_eq()\n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_eq(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col()) to perform a column-column comparison. For the next example, we’ll use col_vals_eq() to check whether the values in column a are equal to the values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_eq(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_equal\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_eq()\n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 3: a is 5 and b is 6.\nRow 5: a is 5 and b is 4."
  },
  {
    "objectID": "reference/Validate.col_count_match.html",
    "href": "reference/Validate.col_count_match.html",
    "title": "Validate.col_count_match",
    "section": "",
    "text": "Validate.col_count_match(\n    count,\n    inverse=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether the column count of the table matches a specified count.\nThe col_count_match() method checks whether the column count of the target table matches a specified count. This validation will operate over a single test unit, which is whether the column count matches the specified count.\nWe also have the option to invert the validation step by setting inverse=True. This will make the expectation that column row count of the target table does not match the specified count."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#parameters",
    "href": "reference/Validate.col_count_match.html#parameters",
    "title": "Validate.col_count_match",
    "section": "Parameters",
    "text": "Parameters\n\ncount : int | FrameT | Any\n\nThe expected column count of the table. This can be an integer value, a Polars or Pandas DataFrame object, or an Ibis backend table. If a DataFrame/table is provided, the column count of that object will be used as the expected count.\n\ninverse : bool = False\n\nShould the validation step be inverted? If True, then the expectation is that the column count of the target table should not match the specified count= value.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#returns",
    "href": "reference/Validate.col_count_match.html#returns",
    "title": "Validate.col_count_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#examples",
    "href": "reference/Validate.col_count_match.html#examples",
    "title": "Validate.col_count_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use the built in dataset \"game_revenue\". The table can be obtained by calling load_dataset(\"game_revenue\").\n\nimport pointblank as pb\n\ngame_revenue = pb.load_dataset(\"game_revenue\")\n\npb.preview(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the number of columns in the table matches a fixed value. In this case, we will use the value 11 as the expected column count.\n\nvalidation = (\n    pb.Validate(data=game_revenue)\n    .col_count_match(count=11)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_count_match()\n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the expectation value of 11 matches the actual count of columns in the target table. So, the single test unit passed."
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "load_dataset(dataset='small_table', tbl_type='polars')\nLoad a dataset hosted in the library as specified table type.\nThe Pointblank library includes several datasets that can be loaded using the load_dataset() function. The datasets can be loaded as a Polars DataFrame, a Pandas DataFrame, or as a DuckDB table (which uses the Ibis library backend). These datasets are used throughout the documentation’s examples to demonstrate the functionality of the library. They’re also useful for experimenting with the library and trying out different validation scenarios."
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\ndataset : Literal['small_table', 'game_revenue', 'nycflights'] = 'small_table'\n\nThe name of the dataset to load. Current options are \"small_table\", \"game_revenue\", and \"nycflights\".\n\ntbl_type : Literal['polars', 'pandas', 'duckdb'] = 'polars'\n\nThe type of table to generate from the dataset. The named options are \"polars\", \"pandas\", and \"duckdb\"."
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n : FrameT | Any\n\nThe dataset for the Validate object. This could be a Polars DataFrame, a Pandas DataFrame, or a DuckDB table as an Ibis table."
  },
  {
    "objectID": "reference/load_dataset.html#included-datasets",
    "href": "reference/load_dataset.html#included-datasets",
    "title": "load_dataset",
    "section": "Included Datasets",
    "text": "Included Datasets\nThere are three included datasets that can be loaded using the load_dataset() function:\n\n\"small_table\": A small dataset with 13 rows and 8 columns. This dataset is useful for testing and demonstration purposes.\n\"game_revenue\": A dataset with 2000 rows and 11 columns. Provides revenue data for a game development company. For the particular game, there are records of player sessions, the items they purchased, ads viewed, and the revenue generated.\n\"nycflights\": A dataset with 336,776 rows and 18 columns. This dataset provides information about flights departing from New York City airports (JFK, LGA, or EWR) in 2013."
  },
  {
    "objectID": "reference/load_dataset.html#supported-dataframe-types",
    "href": "reference/load_dataset.html#supported-dataframe-types",
    "title": "load_dataset",
    "section": "Supported DataFrame Types",
    "text": "Supported DataFrame Types\nThe tbl_type= parameter can be set to one of the following:\n\n\"polars\": A Polars DataFrame.\n\"pandas\": A Pandas DataFrame.\n\"duckdb\": An Ibis table for a DuckDB database."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\nLoad the \"small_table\" dataset as a Polars DataFrame by calling load_dataset() with its defaults:\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset()\n\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nNote that the \"small_table\" dataset is a simple Polars DataFrame and using the preview() function will display the table in an HTML viewing environment.\nThe \"game_revenue\" dataset can be loaded as a Pandas DataFrame by specifying the dataset name and setting tbl_type=\"pandas\":\n\ngame_revenue = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"pandas\")\n\npb.preview(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  player_idobject\n  session_idobject\n  session_startdatetime64[ns, UTC]\n  timedatetime64[ns, UTC]\n  item_typeobject\n  item_nameobject\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydatetime64[ns]\n  acquisitionobject\n  countryobject\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11 00:00:00\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11 00:00:00\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10 00:00:00\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10 00:00:00\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14 00:00:00\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nThe \"game_revenue\" dataset is a more real-world dataset with a mix of data types, and it’s significantly larger than the small_table dataset at 2000 rows and 11 columns.\nThe \"nycflights\" dataset can be loaded as a DuckDB table by specifying the dataset name and setting tbl_type=\"duckdb\":\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\npb.preview(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows336,776Columns18\n  \n\n  \n  yearint64\n  monthint64\n  dayint64\n  dep_timeint64\n  sched_dep_timeint64\n  dep_delayint64\n  arr_timeint64\n  sched_arr_timeint64\n  arr_delayint64\n  carrierstring\n  flightint64\n  tailnumstring\n  originstring\n  deststring\n  air_timeint64\n  distanceint64\n  hourint64\n  minuteint64\n\n\n\n  \n    1\n    2013\n    1\n    1\n    517\n    515\n    2\n    830\n    819\n    11\n    UA\n    1545\n    N14228\n    EWR\n    IAH\n    227\n    1400\n    5\n    15\n  \n  \n    2\n    2013\n    1\n    1\n    533\n    529\n    4\n    850\n    830\n    20\n    UA\n    1714\n    N24211\n    LGA\n    IAH\n    227\n    1416\n    5\n    29\n  \n  \n    3\n    2013\n    1\n    1\n    542\n    540\n    2\n    923\n    850\n    33\n    AA\n    1141\n    N619AA\n    JFK\n    MIA\n    160\n    1089\n    5\n    40\n  \n  \n    4\n    2013\n    1\n    1\n    544\n    545\n    -1\n    1004\n    1022\n    -18\n    B6\n    725\n    N804JB\n    JFK\n    BQN\n    183\n    1576\n    5\n    45\n  \n  \n    5\n    2013\n    1\n    1\n    554\n    600\n    -6\n    812\n    837\n    -25\n    DL\n    461\n    N668DN\n    LGA\n    ATL\n    116\n    762\n    6\n    0\n  \n  \n    336772\n    2013\n    9\n    30\n    NULL\n    1455\n    NULL\n    NULL\n    1634\n    NULL\n    9E\n    3393\n    NULL\n    JFK\n    DCA\n    NULL\n    213\n    14\n    55\n  \n  \n    336773\n    2013\n    9\n    30\n    NULL\n    2200\n    NULL\n    NULL\n    2312\n    NULL\n    9E\n    3525\n    NULL\n    LGA\n    SYR\n    NULL\n    198\n    22\n    0\n  \n  \n    336774\n    2013\n    9\n    30\n    NULL\n    1210\n    NULL\n    NULL\n    1330\n    NULL\n    MQ\n    3461\n    N535MQ\n    LGA\n    BNA\n    NULL\n    764\n    12\n    10\n  \n  \n    336775\n    2013\n    9\n    30\n    NULL\n    1159\n    NULL\n    NULL\n    1344\n    NULL\n    MQ\n    3572\n    N511MQ\n    LGA\n    CLE\n    NULL\n    419\n    11\n    59\n  \n  \n    336776\n    2013\n    9\n    30\n    NULL\n    840\n    NULL\n    NULL\n    1020\n    NULL\n    MQ\n    3531\n    N839MQ\n    LGA\n    RDU\n    NULL\n    431\n    8\n    40\n  \n\n\n\n\n\n\n        \n\n\nThe \"nycflights\" dataset is a large dataset with 336,776 rows and 18 columns. This dataset is truly a real-world dataset and provides information about flights originating from New York City airports in 2013."
  },
  {
    "objectID": "reference/last_n.html",
    "href": "reference/last_n.html",
    "title": "last_n",
    "section": "",
    "text": "last_n(n, offset=0)\nSelect the last n columns in the column list.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The last_n() selector function can be used to select n columns positioned at the end of the column list. So if the set of table columns consists of\n[age, rev_01, rev_02, profit_01, profit_02]\nand you want to validate the last two columns, you can use columns=last_n(2). This will select the profit_01 and profit_02 columns and a validation step will be created for each.\nThe offset= parameter can be used to skip a certain number of columns from the end of the column list. So if you want to select the third and fourth columns from the end, you can use columns=last_n(2, offset=2)."
  },
  {
    "objectID": "reference/last_n.html#parameters",
    "href": "reference/last_n.html#parameters",
    "title": "last_n",
    "section": "Parameters",
    "text": "Parameters\n\nn : int\n\nThe number of columns to select from the end of the column list. Should be a positive integer value. If n is greater than the number of columns in the table, all columns will be selected.\n\noffset : int = 0\n\nThe offset from the end of the column list. The default is 0. If offset is greater than the number of columns in the table, no columns will be selected."
  },
  {
    "objectID": "reference/last_n.html#returns",
    "href": "reference/last_n.html#returns",
    "title": "last_n",
    "section": "Returns",
    "text": "Returns\n\n : LastN\n\nA LastN object, which can be used to select the last n columns."
  },
  {
    "objectID": "reference/last_n.html#relevant-validation-methods-where-last_n-can-be-used",
    "href": "reference/last_n.html#relevant-validation-methods-where-last_n-can-be-used",
    "title": "last_n",
    "section": "Relevant Validation Methods where last_n() can be Used",
    "text": "Relevant Validation Methods where last_n() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe last_n() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/last_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/last_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "last_n",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe last_n() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names starting with “rev” along with the last two columns, you can use the last_n() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(last_n(2) | starts_with(\"rev\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/last_n.html#examples",
    "href": "reference/last_n.html#examples",
    "title": "last_n",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, paid_2021, paid_2022, paid_2023, and paid_2024 and we’d like to validate that the values in the last four columns are greater than 10. We can use the last_n() column selector function to specify that the last four columns in the table are the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.last_n(4), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps. The values in all those columns were all greater than 10.\nWe can also use the last_n() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select the last four columns but also omit those columns that end with \"2023\", we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.last_n(4) - pb.ends_with(\"2023\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get three validation steps, one for paid_2021, paid_2022, and paid_2024."
  },
  {
    "objectID": "reference/DataScan.html",
    "href": "reference/DataScan.html",
    "title": "DataScan",
    "section": "",
    "text": "DataScan(self, data, tbl_name=None)\nGet a summary of a dataset.\nThe DataScan class provides a way to get a summary of a dataset. The summary includes the following information:\n\nthe name of the table (if provided)\nthe type of the table (e.g., \"polars\", \"pandas\", etc.)\nthe number of rows and columns in the table\ncolumn-level information, including:\n\nthe column name\nthe column type\nthe number of missing values\nthe number of unique values\na sample of the data\nstatistics (if the column contains numbers, strings, or datetimes)\n\n\nTo obtain a dictionary representation of the summary, you can use the get_profile() method. To get a JSON representation of the summary, you can use the to_json() method. To save the JSON text to a file, the save_to_json() method could be used.\n\n\n\n\n\n\nWarning\n\n\n\nThe DataScan() class is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\ndata : FrameT | Any\n\nThe data to scan and summarize.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name.\n\n\n\n\n\n\n : DataScan\n\nA DataScan object."
  },
  {
    "objectID": "reference/DataScan.html#parameters",
    "href": "reference/DataScan.html#parameters",
    "title": "DataScan",
    "section": "",
    "text": "data : FrameT | Any\n\nThe data to scan and summarize.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name."
  },
  {
    "objectID": "reference/DataScan.html#returns",
    "href": "reference/DataScan.html#returns",
    "title": "DataScan",
    "section": "",
    "text": ": DataScan\n\nA DataScan object."
  },
  {
    "objectID": "reference/Schema.html",
    "href": "reference/Schema.html",
    "title": "Schema",
    "section": "",
    "text": "Schema(self, columns=None, tbl=None, **kwargs)\nDefinition of a schema object.\nThe schema object defines the structure of a table. Once it is defined, the object can be used in a validation workflow, using Validate and its methods, to ensure that the structure of a table matches the expected schema. The validation method that works with the schema object is called col_schema_match().\nA schema for a table can be constructed with the Schema class in a number of ways:\nThe schema object can also be constructed by providing a DataFrame or Ibis table object (using the tbl= parameter) and the schema will be collected from either type of object. The schema object can be printed to display the column names and dtypes. Note that if tbl= is provided then there shouldn’t be any other inputs provided through either columns= or **kwargs."
  },
  {
    "objectID": "reference/Schema.html#parameters",
    "href": "reference/Schema.html#parameters",
    "title": "Schema",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | list[tuple[str, str]] | list[tuple[str]] | dict[str, str] | None = None\n\nA list of strings (representing column names), a list of tuples (for column names and column dtypes), or a dictionary containing column and dtype information. If any of these inputs are provided here, it will take precedence over any column arguments provided via **kwargs.\n\ntbl : any | None = None\n\nA DataFrame (Polars or Pandas) or an Ibis table object from which the schema will be collected. Read the Supported Input Table Types section for details on the supported table types.\n\n****kwargs** :  = {}\n\nIndividual column arguments that are in the form of column=dtype or column=[dtype1, dtype2, ...]. These will be ignored if the columns= parameter is not None."
  },
  {
    "objectID": "reference/Schema.html#returns",
    "href": "reference/Schema.html#returns",
    "title": "Schema",
    "section": "Returns",
    "text": "Returns\n\n : Schema\n\nA schema object."
  },
  {
    "objectID": "reference/Schema.html#supported-input-table-types",
    "href": "reference/Schema.html#supported-input-table-types",
    "title": "Schema",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe tbl= parameter, if used, can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using Schema(tbl=) with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/Schema.html#additional-notes-on-schema-construction",
    "href": "reference/Schema.html#additional-notes-on-schema-construction",
    "title": "Schema",
    "section": "Additional Notes on Schema Construction",
    "text": "Additional Notes on Schema Construction\nWhile there is flexibility in how a schema can be constructed, there is the potential for some confusion. So let’s go through each of the methods of constructing a schema in more detail and single out some important points.\nWhen providing a list of column names to columns=, a col_schema_match() validation step will only check the column names. Any arguments pertaining to dtypes will be ignored.\nWhen using a list of tuples in columns=, the tuples could contain the column name and dtype or just the column name. This construction allows for more flexibility in constructing the schema as some columns will be checked for dtypes and others will not. This method is the only way to have mixed checks of column names and dtypes in col_schema_match().\nWhen providing a dictionary to columns=, the keys are the column names and the values are the dtypes. This method of input is useful in those cases where you might already have a dictionary of column names and dtypes that you want to use as the schema.\nIf using individual column arguments in the form of keyword arguments, the column names are the keyword arguments and the dtypes are the values. This method emphasizes readability and is perhaps more convenient when manually constructing a schema with a small number of columns.\nFinally, multiple dtypes can be provided for a single column by providing a list or tuple of dtypes in place of a scalar string value. Having multiple dtypes for a column allows for the dtype check via col_schema_match() to make multiple attempts at matching the column dtype. Should any of the dtypes match the column dtype, that part of the schema check will pass. Here are some examples of how you could provide single and multiple dtypes for a column:\n# list of tuples\nschema_1 = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", [\"Float64\", \"Int64\"])])\n\n# dictionary\nschema_2 = pb.Schema(columns={\"name\": \"String\", \"age\": [\"Float64\", \"Int64\"]})\n\n# keyword arguments\nschema_3 = pb.Schema(name=\"String\", age=[\"Float64\", \"Int64\"])\nAll of the above examples will construct the same schema object."
  },
  {
    "objectID": "reference/Schema.html#examples",
    "href": "reference/Schema.html#examples",
    "title": "Schema",
    "section": "Examples",
    "text": "Examples\nA schema can be constructed via the Schema class in multiple ways. Let’s use the following Polars DataFrame as a basis for constructing a schema:\n\nimport pointblank as pb\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"height\": [5.6, 6.0, 5.8]\n})\n\nYou could provide Schema(columns=) a list of tuples containing column names and data types:\n\nschema = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", \"Int64\"), (\"height\", \"Float64\")])\n\nAlternatively, a dictionary containing column names and dtypes also works:\n\nschema = pb.Schema(columns={\"name\": \"String\", \"age\": \"Int64\", \"height\": \"Float64\"})\n\nAnother input method involves using individual column arguments in the form of keyword arguments:\n\nschema = pb.Schema(name=\"String\", age=\"Int64\", height=\"Float64\")\n\nFinally, could also provide a DataFrame (Polars and Pandas) or an Ibis table object to tbl= and the schema will be collected:\nschema = pb.Schema(tbl=df)\nWhichever method you choose, you can verify the schema inputs by printing the schema object:\n\nprint(schema)\n\nPointblank Schema\n  name: String\n  age: Int64\n  height: Float64\n\n\nThe Schema object can be used to validate the structure of a table against the schema. The relevant Validate method for this is col_schema_match(). In a validation workflow, you’ll have a target table (defined at the beginning of the workflow) and you might want to ensure that your expectations of the table structure are met. The col_schema_match() method works with a Schema object to validate the structure of the table. Here’s an example of how you could use col_schema_match() in a validation workflow:\n\n# Define the schema\nschema = pb.Schema(name=\"String\", age=\"Int64\", height=\"Float64\")\n\n# Define a validation that checks the schema against the table (`df`)\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\n# Display the validation results\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_schema_match()\n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe col_schema_match() validation method will validate the structure of the table against the schema during interrogation. If the structure of the table does not match the schema, the single test unit will fail. In this case, the defined schema matched the structure of the table, so the validation passed.\nWe can also choose to check only the column names of the target table. This can be done by providing a simplified Schema object, which is given a list of column names:\n\nschema = pb.Schema(columns=[\"name\", \"age\", \"height\"])\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_schema_match()\n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the schema only checks the column names of the table against the schema during interrogation. If the column names of the table do not match the schema, the single test unit will fail. In this case, the defined schema matched the column names of the table, so the validation passed.\nIf you wanted to check column names and dtypes only for a subset of columns (and just the column names for the rest), you could use a list of mixed one- or two-item tuples in columns=:\n\nschema = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", ), (\"height\", )])\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_schema_match()\n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nNot specifying a dtype for a column (as is the case for the age and height columns in the above example) will only check the column name.\nThere may also be the case where you want to check the column names and specify multiple dtypes for a column to have several attempts at matching the dtype. This can be done by providing a list of dtypes where there would normally be a single dtype:\n\nschema = pb.Schema(\n  columns=[(\"name\", \"String\"), (\"age\", [\"Float64\", \"Int64\"]), (\"height\", \"Float64\")]\n)\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_schema_match()\n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFor the age column, the schema will check for both Float64 and Int64 dtypes. If either of these dtypes is found in the column, the portion of the schema check will succeed."
  },
  {
    "objectID": "reference/Schema.html#see-also",
    "href": "reference/Schema.html#see-also",
    "title": "Schema",
    "section": "See Also",
    "text": "See Also\nThe col_schema_match() validation method, where a Schema object is used in a validation workflow."
  },
  {
    "objectID": "demos/using-parquet-data/index.html",
    "href": "demos/using-parquet-data/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Using Parquet Data\nA Parquet dataset can be used for data validation, thanks to Ibis.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example using a Parquet dataset.Parquet\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    item_revenue\n    200\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:54 UTC&lt; 1 s2025-03-06 23:50:54 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport ibis\n\ngame_revenue = ibis.read_parquet(\"data/game_revenue.parquet\")\n\nvalidation = (\n    pb.Validate(data=game_revenue, label=\"Example using a Parquet dataset.\")\n    .col_vals_lt(columns=\"item_revenue\", value=200)\n    .col_vals_gt(columns=\"item_revenue\", value=0)\n    .col_vals_gt(columns=\"session_duration\", value=5)\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    ParquetRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/set-membership/index.html",
    "href": "demos/set-membership/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Set Membership\nPerform validations that check whether values are part of a set (or not part of one).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:48Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_in_set()\n        \n    f\n    zero, infinity\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:48 UTC&lt; 1 s2025-03-06 23:50:48 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])    # part of this set\n    .col_vals_not_in_set(columns=\"f\", set=[\"zero\", \"infinity\"])  # not part of this set\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/failure-thresholds/index.html",
    "href": "demos/failure-thresholds/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Set Failure Threshold Levels\nSet threshold levels to better gauge adverse data quality.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:42DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #AAAAAA\n    4\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19931.00\n    70.00\n    ●\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    5\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    end_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    ●\n    ●\n    ●\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:42 UTC&lt; 1 s2025-03-06 23:50:42 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(  # setting relative threshold defaults for all steps\n            warning=0.05,          # 5% failing test units: warning threshold (gray)\n            error=0.10,            # 10% failed test units: error threshold (yellow)\n            critical=0.15          # 15% failed test units: critical threshold (red)\n        ),\n    )\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=4,\n        thresholds=(5, 10, 20)     # setting absolute thresholds for *this* step (W, E, C)\n    )\n    .col_exists(columns=\"end_day\")\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/checks-for-missing/index.html",
    "href": "demos/checks-for-missing/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Checks for Missing Values\nPerform validations that check whether missing/NA/Null values are present.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:35Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    d\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    5\n    \n        \n        \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_null()\n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    00.00\n    131.00\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-03-06 23:50:35 UTC&lt; 1 s2025-03-06 23:50:35 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_not_null(columns=\"a\")                  # expect no Null values\n    .col_vals_not_null(columns=\"b\")                  # \"\" \"\"\n    .col_vals_not_null(columns=\"c\")                  # \"\" \"\"\n    .col_vals_not_null(columns=\"d\")                  # \"\" \"\"\n    .col_vals_null(columns=\"a\")                      # expect all values to be Null\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/03-data-extracts/index.html",
    "href": "demos/03-data-extracts/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Data Extracts\nPulling out data extracts that highlight rows with validation failures.\n\nValidation with failures at Step 2:\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Validation with test unit failures available as an extractPolarsgame_revenue\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19860.99\n    140.01\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-03-06 23:50:29 UTC&lt; 1 s2025-03-06 23:50:29 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\nExtract from Step 2 (which has 14 failing test units):\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows14Columns12\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1455\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-46cdjzy7\n    2015-01-17 11:25:25+00:00\n    2015-01-17 11:28:01+00:00\n    iap\n    offer4\n    13.99\n    4.6\n    2015-01-14\n    organic\n    United States\n  \n  \n    1516\n    OMCVUAIKSDTR651\n    OMCVUAIKSDTR651-yso9e1b2\n    2015-01-17 20:58:34+00:00\n    2015-01-17 21:01:34+00:00\n    iap\n    offer3\n    10.49\n    4.2\n    2015-01-07\n    other_campaign\n    United States\n  \n  \n    1517\n    OMCVUAIKSDTR651\n    OMCVUAIKSDTR651-yso9e1b2\n    2015-01-17 20:58:34+00:00\n    2015-01-17 21:02:34+00:00\n    iap\n    offer5\n    20.29\n    4.2\n    2015-01-07\n    other_campaign\n    United States\n  \n  \n    1913\n    MTCIWKOVASYP925\n    MTCIWKOVASYP925-1q3xvfmp\n    2015-01-20 12:34:43+00:00\n    2015-01-20 12:35:37+00:00\n    iap\n    offer5\n    26.09\n    3.9\n    2015-01-14\n    organic\n    Germany\n  \n  \n    1914\n    MTCIWKOVASYP925\n    MTCIWKOVASYP925-1q3xvfmp\n    2015-01-20 12:34:43+00:00\n    2015-01-20 12:37:25+00:00\n    iap\n    gold2\n    1.79\n    3.9\n    2015-01-14\n    organic\n    Germany\n  \n  \n    1919\n    BFNLURISJXTH647\n    BFNLURISJXTH647-len6vujd\n    2015-01-20 14:09:51+00:00\n    2015-01-20 14:10:03+00:00\n    iap\n    gold7\n    47.99\n    4.5\n    2015-01-10\n    organic\n    India\n  \n  \n    1920\n    BFNLURISJXTH647\n    BFNLURISJXTH647-len6vujd\n    2015-01-20 14:09:51+00:00\n    2015-01-20 14:14:21+00:00\n    iap\n    gold6\n    23.99\n    4.5\n    2015-01-10\n    organic\n    India\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\"),\n        tbl_name=\"game_revenue\",\n        label=\"Validation with test unit failures available as an extract\"\n    )\n    .col_vals_gt(columns=\"item_revenue\", value=0)      # STEP 1: no test unit failures\n    .col_vals_ge(columns=\"session_duration\", value=5)  # STEP 2: 14 test unit failures -&gt; extract\n    .interrogate()\n)\npb.preview(validation.get_data_extracts(i=2, frame=True), n_head=20, n_tail=20)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/expect-text-pattern/index.html",
    "href": "demos/expect-text-pattern/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Expectations with a Text Pattern\nWith the col_vals_regex(), check for conformance to a regular expression.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:24Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    b\n    ^\\d-[a-z]{3}-\\d{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    f\n    high|low|mid\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:24 UTC&lt; 1 s2025-03-06 23:50:24 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_regex(columns=\"b\", pattern=r\"^\\d-[a-z]{3}-\\d{3}$\")  # check pattern in 'b'\n    .col_vals_regex(columns=\"f\", pattern=r\"high|low|mid\")         # check pattern in 'f'\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/comparisons-across-columns/index.html",
    "href": "demos/comparisons-across-columns/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Comparison Checks Across Columns\nPerform comparisons of values in columns to values in other columns.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:17Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    a\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    d\n    [c, 12000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:17 UTC&lt; 1 s2025-03-06 23:50:17 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_lt(columns=\"a\", value=pb.col(\"c\"))     # values in 'a' &gt; values in 'c'\n    .col_vals_between(\n        columns=\"d\",                                 # values in 'd' are between values\n        left=pb.col(\"c\"),                            # in 'c' and the fixed value of 12,000;\n        right=12000,                                 # any missing values encountered result\n        na_pass=True                                 # in a passing test unit\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/05-step-report-column-check/index.html",
    "href": "demos/05-step-report-column-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Step Report: Column Data Checks\nA step report for column checks shows what went wrong.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Step reports for column data checksPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    c\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    b\n    \\d-[a-z]{3}-\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:12 UTC&lt; 1 s2025-03-06 23:50:12 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1\n  \n  \n    ASSERTION c ≥ 44 / 13 TEST UNIT FAILURES IN COLUMN 5EXTRACT OF 4 ROWS WITH TEST UNIT FAILURES IN RED:\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2 ✓\n  \n  \n    ASSERTION b matches regex \\d-[a-z]{3}-\\d{3}13 TEST UNITS ALL PASSED IN COLUMN 4PREVIEW OF TARGET TABLE:\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        tbl_name=\"small_table\",\n        label=\"Step reports for column data checks\"\n    )\n    .col_vals_ge(columns=\"c\", value=4, na_pass=True)                # has failing test units\n    .col_vals_regex(columns=\"b\", pattern=r\"\\d-[a-z]{3}-\\d{3}\")      # no failing test units\n    .interrogate()\n)\n\nvalidation\nvalidation.get_step_report(i=1)\nvalidation.get_step_report(i=2)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/col-vals-custom-expr/index.html",
    "href": "demos/col-vals-custom-expr/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Custom Expression for Checking Column Values\nA column expression can be used to check column values. Just use col_vals_expr() for this.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:06Pandas\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_expr()\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:06 UTC&lt; 1 s2025-03-06 23:50:06 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\n    )\n    .col_vals_expr(expr=lambda df: (df[\"d\"] % 1 != 0) & (df[\"a\"] &lt; 10))  # Pandas column expr\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/numeric-comparisons/index.html",
    "href": "demos/numeric-comparisons/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Numeric Comparisons\nPerform comparisons of values in columns to fixed values.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:00Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    d\n    10000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    a\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    4\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    5\n    \n        \n        \n\n    col_vals_not_equal\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ne()\n        \n    a\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    6\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    c\n    [0, 15]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-03-06 23:50:00 UTC&lt; 1 s2025-03-06 23:50:00 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_gt(columns=\"d\", value=1000)            # values in 'd' &gt; 1000\n    .col_vals_lt(columns=\"d\", value=10000)           # values in 'd' &lt; 10000\n    .col_vals_ge(columns=\"a\", value=1)               # values in 'a' &gt;= 1\n    .col_vals_le(columns=\"c\", value=5)               # values in 'c' &lt;= 5\n    .col_vals_ne(columns=\"a\", value=7)               # values in 'a' not equal to 7\n    .col_vals_between(columns=\"c\", left=0, right=15) # 0 &lt;= 'c' values &lt;= 15\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/expect-no-duplicate-rows/index.html",
    "href": "demos/expect-no-duplicate-rows/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Expect No Duplicate Rows\nWe can check for duplicate rows in the table with rows_distinct().\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:49:56Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n         rows_distinct()\n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:49:56 UTC&lt; 1 s2025-03-06 23:49:56 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .rows_distinct()    # expect no duplicate rows\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Find out if your data is what you think it is\nPointblank is a table validation and testing library for Python. It helps you ensure that your tabular data meets certain expectations and constraints and it presents the results in a beautiful validation report table."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Pointblank",
    "section": "Getting Started",
    "text": "Getting Started\nLet’s take a Polars DataFrame and validate it against a set of constraints. We do that by using the Validate class along with adding validation steps:\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\")) # Use Validate() to start\n    .col_vals_gt(columns=\"d\", value=100)       # STEP 1      |\n    .col_vals_le(columns=\"c\", value=5)         # STEP 2      | &lt;-- Build up a validation plan\n    .col_exists(columns=[\"date\", \"date_time\"]) # STEPS 3 & 4 |\n    .interrogate() # This will execute all validation steps and collect intel\n)\n\nvalidation\n\n\n\nThe rows in the validation report table correspond to each of the validation steps. One of the key concepts is that validation steps can be broken down into atomic test cases (test units), where each of these test units is given either of pass/fail status based on the validation constraints. You’ll see these tallied up in the reporting table (in the UNITS, PASS, and FAIL columns).\nThe tabular reporting view is just one way to see the results. You can also obtain fine-grained results of the interrogation as individual step reports or via methods that provide key metrics. It’s also possible to use the validation results for downstream processing, such as filtering the input table based on the pass/fail status of the rows.\nOn the input side, we can use the following types of tables:\n\nPolars DataFrame\nPandas DataFrame\nDuckDB table\nMySQL table\nPostgreSQL table\nSQLite table\nParquet\n\nTo make this all work seamlessly, we use Narwhals to work with Polars and Pandas DataFrames. We also integrate with Ibis to enable the use of DuckDB, MySQL, PostgreSQL, SQLite, Parquet, and more! In doing all of this, we can provide an ergonomic and consistent API for validating tabular data from various sources."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Pointblank",
    "section": "Features",
    "text": "Features\nHere’s a short list of what we think makes Pointblank a great tool for data validation:\n\nFlexible: We support tables from Polars, Pandas, Duckdb, MySQL, PostgreSQL, SQLite, and Parquet\nBeautiful Reports: Generate beautiful HTML table reports of your data validation results\nFunctional Output: Easily pull the specific data validation outputs you need for further processing\nEasy to Use: Get started quickly with a straightforward API and clear documentation examples\nPowerful: You can make complex data validation rules with flexible options for composition"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Pointblank",
    "section": "Installation",
    "text": "Installation\nYou can install Pointblank using pip:\npip install pointblank"
  },
  {
    "objectID": "index.html#getting-in-touch",
    "href": "index.html#getting-in-touch",
    "title": "Pointblank",
    "section": "Getting in Touch",
    "text": "Getting in Touch\nIf you encounter a bug, have usage questions, or want to share ideas to make this package better, please feel free to file an issue.\nWanna talk about data validation in a more relaxed setting? Join our Discord server! This is a great option for asking about the development of Pointblank, pitching ideas that may become features, and just sharing your ideas!"
  },
  {
    "objectID": "index.html#contributing-to-pointblank",
    "href": "index.html#contributing-to-pointblank",
    "title": "Pointblank",
    "section": "Contributing to Pointblank",
    "text": "Contributing to Pointblank\nThere are many ways to contribute to the ongoing development of Pointblank. Some contributions can be simple (like fixing typos, improving documentation, filing issues for feature requests or problems, etc.) and others might take more time and care (like answering questions and submitting PRs with code changes). Just know that anything you can do to help would be very much appreciated!\nPlease read over the contributing guidelines for information on how to get started."
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "Pointblank",
    "section": "Roadmap",
    "text": "Roadmap\nThere is much to do to make Pointblank a dependable and useful tool for data validation. To that end, we have a roadmap that will serve as a guide for the development of the library. Here are some of the things we are working on or plan to work on in the near future:\n\nmore validation methods to cover a wider range of data validation needs\neasy-to-use but powerful logging functionality\nmessaging actions (e.g., Slack, emailing, etc.) to better react to threshold exceedences\nadditional functionality for building more complex validations via LLMs (extension of ideas from the current DraftValidation class)\na feature for quickly obtaining summary information on any dataset (tying together existing and future dataset summary-generation pieces)\nensuring there are text/dict/JSON/HTML versions of all reports\nsupporting the writing and reading of YAML validation config files\na cli utility for Pointblank that can be used to run validations from the command line\ncomplete testing of validations across all compatible backends (for certification of those backends as fully supported)\ncompletion of the User Guide in the project website\nfunctionality for creating and publishing data dictionaries, which could: (a) use LLMs to more quickly draft column-level descriptions, and (b) incorporate templating features to make it easier to keep descriptions consistent and up to date\n\nIf you have any ideas for features or improvements, don’t hesitate to share them with us! We are always looking for ways to make Pointblank better."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Pointblank",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that the Pointblank project is released with a contributor code of conduct. By participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Pointblank",
    "section": "📄 License",
    "text": "📄 License\nPointblank is licensed under the MIT license.\n© Posit Software, PBC."
  },
  {
    "objectID": "index.html#governance",
    "href": "index.html#governance",
    "title": "Pointblank",
    "section": "🏛️ Governance",
    "text": "🏛️ Governance\nThis project is primarily maintained by Rich Iannone. Other authors may occasionally assist with some of these duties."
  },
  {
    "objectID": "demos/index.html",
    "href": "demos/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "A Selection of Examples\n\n\n\n\n\n\nStarter Validation\n\n\n\nA validation with the basics.\n\n\n\n\n\n\nAdvanced Validation\n\n\n\nA validation with a comprehensive set of rules.\n\n\n\n\n\n\nData Extracts\n\n\n\nPulling out data extracts that highlight rows with validation failures.\n\n\n\n\n\n\nSundered Data\n\n\n\nSplitting your data into ‘pass’ and ‘fail’ subsets.\n\n\n\n\n\n\nStep Reports for Column Data Checks\n\n\n\nA step report for column checks shows what went wrong.\n\n\n\n\n\n\nStep Report for a Schema Check\n\n\n\nWhen a schema doesn’t match, a step report gives you the details.\n\n\n\n\n\n\n\nNumeric Comparisons Perform comparisons of values in columns to fixed values.\nComparison Checks Across Columns Perform comparisons of values in columns to values in other columns.\nApply Validation Rules to Multiple Columns Create multiple validation steps by using a list of column names with columns=.\nChecks for Missing Values Perform validations that check whether missing/NA/Null values are present.\nExpectations with a Text Pattern With col_vals_regex(), check for conformance to a regular expression.\nSet Membership Perform validations that check whether values are part of a set (or not part of one).\nExpect No Duplicate Rows We can check for duplicate rows in the table with rows_distinct().\nChecking for Duplicate Values To check for duplicate values down a column, use rows_distinct() with a columns_subset= value.\nCustom Expression for Checking Column Values A column expression can be used to check column values. Just use col_vals_expr() for this.\nMutate the Table in a Validation Step For far more specialized validations, modify the table with the pre= argument before checking it.\nVerifying Row and Column Counts Check the dimensions of the table with the *_count_match() validation methods.\nSet Failure Threshold Levels Set threshold levels to better gauge adverse data quality.\nColumn Selector Functions: Easily Pick Columns Use column selector functions in the columns= argument to conveniently choose columns.\nCheck the Schema of a Table The schema of a table can be flexibly defined with Schema and verified with col_schema_match().\nUsing Parquet Data A Parquet dataset can be used for data validation, thanks to Ibis."
  },
  {
    "objectID": "demos/01-starter/index.html",
    "href": "demos/01-starter/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Starter Validation\nA validation with the basics.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    A starter validationPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:03 UTC&lt; 1 s2025-03-06 23:50:03 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate( # Use pb.Validate to start\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"A starter validation\"\n    )\n    .col_vals_gt(columns=\"d\", value=1000)       # STEP 1 |\n    .col_vals_le(columns=\"c\", value=5)          # STEP 2 | &lt;-- Build up a validation plan\n    .col_exists(columns=[\"date\", \"date_time\"])  # STEP 3 |\n    .interrogate()  # This will execute all validation steps and collect intel\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/mutate-table-in-step/index.html",
    "href": "demos/mutate-table-in-step/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Mutate the Table in a Validation Step\nFor far more specialized validations, modify the table with the pre= argument before checking it.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:09Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    a\n    [3, 6]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_equal\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_eq()\n        \n    b_len\n    9\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:09 UTC&lt; 1 s2025-03-06 23:50:09 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\nimport narwhals as nw\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_between(\n        columns=\"a\",\n        left=3, right=6,\n        pre=lambda df: df.select(pl.median(\"a\"))    # Use a Polars expression to aggregate\n    )\n    .col_vals_eq(\n        columns=\"b_len\",\n        value=9,\n        pre=lambda dfn: dfn.with_columns(           # Use a Narwhals expression, identified\n            b_len=nw.col(\"b\").str.len_chars()       # by the 'dfn' here\n        )\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/expect-no-duplicate-values/index.html",
    "href": "demos/expect-no-duplicate-values/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Checking for Duplicate Values\nTo check for duplicate values down a column, use rows_distinct() with a columns_subset= value.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:15Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n         rows_distinct()\n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:15 UTC&lt; 1 s2025-03-06 23:50:15 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .rows_distinct(columns_subset=\"b\")   # expect no duplicate values in 'b'\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/06-step-report-schema-check/index.html",
    "href": "demos/06-step-report-schema-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Step Report: Schema Check\nWhen a schema doesn’t match, a step report gives you the details.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Step report for a schema checkDuckDBsmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_schema_match()\n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:20 UTC&lt; 1 s2025-03-06 23:50:20 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✗\n  \n  \n    COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DTYPE\n  \n  COLUMN\n  \n  DTYPE\n  \n\n\n\n  \n    1\n    date_time\n    timestamp(6)\n    1\n    date_time\n    ✓\n    timestamp\n    ✗\n  \n  \n    2\n    date\n    date\n    2\n    dates\n    ✗\n    date\n    —\n  \n  \n    3\n    a\n    int64\n    3\n    a\n    ✓\n    int64\n    ✓\n  \n  \n    4\n    b\n    string\n    4\n    b\n    ✓\n    —\n    \n  \n  \n    5\n    c\n    int64\n    5\n    c\n    ✓\n    —\n    \n  \n  \n    6\n    d\n    float64\n    6\n    d\n    ✓\n    float64\n    ✓\n  \n  \n    7\n    e\n    boolean\n    7\n    e\n    ✓\n    bool | boolean\n    ✓\n  \n  \n    8\n    f\n    string\n    8\n    f\n    ✓\n    str\n    ✗\n  \n\n  \n  \n  \n    Supplied Column Schema:[('date_time', 'timestamp'), ('dates', 'date'), ('a', 'int64'), ('b',), ('c',), ('d', 'float64'), ('e', ['bool', 'boolean']), ('f', 'str')]\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\n# Create a schema for the target table (`small_table` as a DuckDB table)\nschema = pb.Schema(\n    columns=[\n        (\"date_time\", \"timestamp\"),     # this dtype doesn't match\n        (\"dates\", \"date\"),              # this column name doesn't match\n        (\"a\", \"int64\"),\n        (\"b\",),                         # omit dtype to not check for it\n        (\"c\",),                         # \"\"   \"\"   \"\"  \"\"\n        (\"d\", \"float64\"),\n        (\"e\", [\"bool\", \"boolean\"]),     # try several dtypes (second one matches)\n        (\"f\", \"str\"),                   # this dtype doesn't match\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform a schema check\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"duckdb\"),\n        tbl_name=\"small_table\",\n        label=\"Step report for a schema check\"\n    )\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\nvalidation.get_step_report(i=1)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows13Columns8\n  \n\n  \n  date_timetimestamp\n  datedate\n  aint64\n  bstring\n  cint64\n  dfloat64\n  eboolean\n  fstring\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    NULL\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    NULL\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/schema-check/index.html",
    "href": "demos/schema-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Check the Schema of a Table\nThe schema of a table can be flexibly defined with Schema and verified with col_schema_match().\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:26Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_schema_match()\n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:26 UTC&lt; 1 s2025-03-06 23:50:26 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n        \"c\": [1.1, 2.2, 3.3, 4.4],\n    }\n)\n\n# Use the Schema class to define the column schema as loosely or rigorously as required\nschema = pb.Schema(\n    columns=[\n        (\"a\", \"String\"),          # Column 'a' has dtype 'String'\n        (\"b\", [\"Int\", \"Int64\"]),  # Column 'b' has dtype 'Int' or 'Int64'\n        (\"c\", )                   # Column 'c' follows 'b' but we don't specify a dtype here\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform the schema check\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation"
  },
  {
    "objectID": "demos/check-row-column-counts/index.html",
    "href": "demos/check-row-column-counts/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Verifying Row and Column Counts\nCheck the dimensions of the table with the *_count_match() validation methods.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:32DuckDB\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_count_match()\n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         row_count_match()\n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         row_count_match()\n        \n    —\n    ≠ 0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_count_match()\n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:32 UTC&lt; 1 s2025-03-06 23:50:32 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\")\n    )\n    .col_count_match(count=11)                       # expect 11 columns in the table\n    .row_count_match(count=2000)                     # expect 2,000 rows in the table\n    .row_count_match(count=0, inverse=True)          # expect that the table has rows\n    .col_count_match(                                # compare column count against\n        count=pb.load_dataset(                       # that of another table\n            dataset=\"game_revenue\", tbl_type=\"pandas\"\n        )\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/02-advanced/index.html",
    "href": "demos/02-advanced/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Advanced Validation\nA validation with a comprehensive set of rules.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Comprehensive validation examplePolarsgame_revenueWARNING0.1ERROR0.25CRITICAL0.35\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    ^[A-Z]{12}[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    item_revenue\n    0.02\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19410.97\n    590.03\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    5\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    acquisition\n    google, facebook, organic, crosspromo, other_campaign\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19750.99\n    250.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    6\n    \n        \n        \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_in_set()\n        \n    country\n    Mongolia, Germany\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17750.89\n    2250.11\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    7\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    session_duration\n    [10, 50]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    8\n    \n        \n        \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n         rows_distinct()\n        \n    player_id, session_id, time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19780.99\n    220.01\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    9\n    \n        \n        \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         row_count_match()\n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n        \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_count_match()\n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    11\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    12\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    13\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    14\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:38 UTC&lt; 1 s2025-03-06 23:50:38 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\nimport narwhals as nw\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\"),\n        tbl_name=\"game_revenue\",\n        label=\"Comprehensive validation example\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"^[A-Z]{12}[0-9]{3}$\")        # STEP 1\n    .col_vals_gt(columns=\"session_duration\", value=5)                           # STEP 2\n    .col_vals_ge(columns=\"item_revenue\", value=0.02)                            # STEP 3\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])                    # STEP 4\n    .col_vals_in_set(                                                           # STEP 5\n        columns=\"acquisition\",\n        set=[\"google\", \"facebook\", \"organic\", \"crosspromo\", \"other_campaign\"]\n    )\n    .col_vals_not_in_set(columns=\"country\", set=[\"Mongolia\", \"Germany\"])        # STEP 6\n    .col_vals_between(                                                          # STEP 7\n        columns=\"session_duration\",\n        left=10, right=50,\n        pre = lambda df: df.select(pl.median(\"session_duration\"))\n    )\n    .rows_distinct(columns_subset=[\"player_id\", \"session_id\", \"time\"])          # STEP 8\n    .row_count_match(count=2000)                                                # STEP 9\n    .col_count_match(count=11)                                                  # STEP 10\n    .col_vals_not_null(columns=pb.starts_with(\"item\"))                          # STEPS 11-13\n    .col_exists(columns=\"start_day\")                                            # STEP 14\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    6\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:08:56+00:00\n    ad\n    ad_10sec\n    0.07\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    7\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:14:08+00:00\n    ad\n    ad_10sec\n    0.08\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    8\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:21:44+00:00\n    ad\n    ad_30sec\n    1.17\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    9\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:24:20+00:00\n    ad\n    ad_10sec\n    0.14\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    10\n    FXWUORGYNJAE271\n    FXWUORGYNJAE271-et7bs639\n    2015-01-01 15:17:18+00:00\n    2015-01-01 15:19:36+00:00\n    ad\n    ad_5sec\n    0.08\n    30.7\n    2015-01-01\n    organic\n    Canada\n  \n  \n    1991\n    VPNRYLMBKJGT925\n    VPNRYLMBKJGT925-vt26q9gb\n    2015-01-21 01:07:24+00:00\n    2015-01-21 01:26:12+00:00\n    ad\n    ad_survey\n    0.72\n    24.9\n    2015-01-21\n    other_campaign\n    Germany\n  \n  \n    1992\n    JVBZCPKXHFMU491\n    JVBZCPKXHFMU491-wvi6hs2t\n    2015-01-21 01:49:36+00:00\n    2015-01-21 01:53:36+00:00\n    iap\n    gold6\n    41.99\n    7.1\n    2015-01-07\n    organic\n    United States\n  \n  \n    1993\n    JVBZCPKXHFMU491\n    JVBZCPKXHFMU491-wvi6hs2t\n    2015-01-21 01:49:36+00:00\n    2015-01-21 01:55:42+00:00\n    iap\n    gems3\n    17.49\n    7.1\n    2015-01-07\n    organic\n    United States\n  \n  \n    1994\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:01:20+00:00\n    ad\n    ad_playable\n    1.116\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1995\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:14+00:00\n    ad\n    ad_15sec\n    0.225\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/column-selector-functions/index.html",
    "href": "demos/column-selector-functions/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Column Selector Functions: Easily Pick Columns\nUse column selector functions in the columns= argument to conveniently choose columns.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:45Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    ^[A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    session_id\n    ^[A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    acquisition\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    country\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    7\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    8\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    session_id\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    9\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    item_type\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    item_name\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    11\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    acquisition\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    12\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    country\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:45 UTC&lt; 1 s2025-03-06 23:50:45 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport narwhals.selectors as ncs\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\")\n    )\n    .col_vals_ge(\n        columns=pb.matches(\"rev|dur\"),  # check values in columns having 'rev' or 'dur' in name\n        value=0\n    )\n    .col_vals_regex(\n        columns=pb.ends_with(\"_id\"),    # check values in columns with names ending in '_id'\n        pattern=r\"^[A-Z]{12}\\d{3}\"\n    )\n    .col_vals_not_null(\n        columns=pb.last_n(2)            # check that the last two columns don't have Null values\n    )\n    .col_vals_regex(\n        columns=ncs.string(),           # check that all string columns are non-empty strings\n        pattern=r\"(.|\\s)*\\S(.|\\s)*\"\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/apply-checks-to-several-columns/index.html",
    "href": "demos/apply-checks-to-several-columns/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Apply Validation Rules to Multiple Columns\nCreate multiple validation steps by using a list of column names with columns=.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:50:50Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    a\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    c\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    d\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:50:50 UTC&lt; 1 s2025-03-06 23:50:50 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_ge(columns=[\"a\", \"c\", \"d\"], value=0)   # check values in 'a', 'c', and 'd'\n    .col_exists(columns=[\"date_time\", \"date\"])       # check for the existence of two columns\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/04-sundered-data/index.html",
    "href": "demos/04-sundered-data/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Sundered Data\nSplitting your data into ‘pass’ and ‘fail’ subsets.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Sundering DataPandassmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-03-06 23:50:57 UTC&lt; 1 s2025-03-06 23:50:57 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows4Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    3\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    4\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Sundering Data\"\n    )\n    .col_vals_gt(columns=\"d\", value=1000)\n    .col_vals_le(columns=\"c\", value=5)\n    .interrogate()\n)\n\nvalidation\npb.preview(validation.get_sundered_data(type=\"pass\"))\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html",
    "href": "reference/Validate.col_vals_lt.html",
    "title": "Validate.col_vals_lt",
    "section": "",
    "text": "Validate.col_vals_lt(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data less than a fixed value or data in another column?\nThe col_vals_lt() validation method checks whether column values in a table are less than a specified value= (the exact comparison used in this function is col_val &lt; value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#parameters",
    "href": "reference/Validate.col_vals_lt.html#parameters",
    "title": "Validate.col_vals_lt",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#returns",
    "href": "reference/Validate.col_vals_lt.html#returns",
    "title": "Validate.col_vals_lt",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#examples",
    "href": "reference/Validate.col_vals_lt.html#examples",
    "title": "Validate.col_vals_lt",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [1, 2, 1, 2, 2, 2],\n        \"c\": [2, 1, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    2\n    1\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    2\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    2\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all less than the value of 10. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=\"a\", value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_lt(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col()) to perform a column-column comparison. For the next example, we’ll use col_vals_lt() to check whether the values in column b are less than values in column c.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=\"b\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    b\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 1: b is 2 and c is 1.\nRow 2: b is 1 and c is 1."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html",
    "href": "reference/Validate.col_schema_match.html",
    "title": "Validate.col_schema_match",
    "section": "",
    "text": "Validate.col_schema_match(\n    schema,\n    complete=True,\n    in_order=True,\n    case_sensitive_colnames=True,\n    case_sensitive_dtypes=True,\n    full_match_dtypes=True,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo columns in the table (and their types) match a predefined schema?\nThe col_schema_match() method works in conjunction with an object generated by the Schema class. That class object is the expectation for the actual schema of the target table. The validation step operates over a single test unit, which is whether the schema matches that of the table (within the constraints enforced by the complete=, and in_order= options)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#parameters",
    "href": "reference/Validate.col_schema_match.html#parameters",
    "title": "Validate.col_schema_match",
    "section": "Parameters",
    "text": "Parameters\n\nschema : Schema\n\nA Schema object that represents the expected schema of the table. This object is generated by the Schema class.\n\ncomplete : bool = True\n\nShould the schema match be complete? If True, then the target table must have all columns specified in the schema. If False, then the table can have additional columns not in the schema (i.e., the schema is a subset of the target table’s columns).\n\nin_order : bool = True\n\nShould the schema match be in order? If True, then the columns in the schema must appear in the same order as they do in the target table. If False, then the order of columns in the schema and the target table can differ.\n\ncase_sensitive_colnames : bool = True\n\nShould the schema match be case-sensitive with regard to column names? If True, then the column names in the schema and the target table must match exactly. If False, then the column names are compared in a case-insensitive manner.\n\ncase_sensitive_dtypes : bool = True\n\nShould the schema match be case-sensitive with regard to column data types? If True, then the column data types in the schema and the target table must match exactly. If False, then the column data types are compared in a case-insensitive manner.\n\nfull_match_dtypes : bool = True\n\nShould the schema match require a full match of data types? If True, then the column data types in the schema and the target table must match exactly. If False then substring matches are allowed, so a schema data type of Int would match a target table data type of Int64.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#returns",
    "href": "reference/Validate.col_schema_match.html#returns",
    "title": "Validate.col_schema_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#examples",
    "href": "reference/Validate.col_schema_match.html#examples",
    "title": "Validate.col_schema_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three columns (string, integer, and float). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n        \"c\": [1.1, 2.2, 3.3, 4.4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bInt64\n  cFloat64\n\n\n\n  \n    1\n    apple\n    1\n    1.1\n  \n  \n    2\n    banana\n    6\n    2.2\n  \n  \n    3\n    cherry\n    3\n    3.3\n  \n  \n    4\n    date\n    5\n    4.4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the columns in the table match a predefined schema. A schema can be defined using the Schema class.\n\nschema = pb.Schema(\n    columns=[(\"a\", \"String\"), (\"b\", \"Int64\"), (\"c\", \"Float64\")]\n)\n\nYou can print the schema object to verify that the expected schema is as intended.\n\nprint(schema)\n\nPointblank Schema\n  a: String\n  b: Int64\n  c: Float64\n\n\nNow, we’ll use the col_schema_match() method to validate the table against the expected schema object. There is a single test unit for this validation step (whether the schema matches the table or not).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         col_schema_match()\n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the schema matches the table. The single test unit passed since the table columns and their types match the schema."
  },
  {
    "objectID": "reference/Validate.error.html",
    "href": "reference/Validate.error.html",
    "title": "Validate.error",
    "section": "",
    "text": "Validate.error(i=None, scalar=False)\nGet the ‘error’ level status for each validation step.\nThe ‘error’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the ‘error’ level. Otherwise, the status is False.\nThe ascribed name of ‘error’ is semantic and does not imply that the validation process is halted, it is simply a status indicator that could be used to trigger some action to be taken. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the ‘error’ status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.error.html#parameters",
    "href": "reference/Validate.error.html#parameters",
    "title": "Validate.error",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the ‘error’ status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.error.html#returns",
    "href": "reference/Validate.error.html#returns",
    "title": "Validate.error",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the ‘error’ status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.error.html#examples",
    "href": "reference/Validate.error.html#examples",
    "title": "Validate.error",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have some failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the error() method is used to determine the ‘error’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [3, 4, 9, 7, 2, 3, 8],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.error()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘error’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘error’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘error’ level.\nWe can also visually inspect the ‘error’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:51:12PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    30.43\n    40.57\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:51:12 UTC&lt; 1 s2025-03-06 23:51:12 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there are filled gray and yellow circles in the first step (far right side, in the W and E columns) indicating that the ‘warning’ and ‘error’ thresholds were met. The other steps have empty gray and yellow circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘error’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.error(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had the ‘error’ threshold met."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "When peforming data validation, you’ll need the Validate class to get the process started. It’s given the target table and you can optionally provide some metadata and/or failure thresholds (using the Thresholds class or through shorthands for this task). The Validate class has numerous methods for defining validation steps and for obtaining post-interrogation metrics and data.\n\n\n\nValidate\nWorkflow for defining a set of validations on a table and interrogating for results.\n\n\nThresholds\nDefinition of threshold values.\n\n\nActions\nDefinition of action values.\n\n\nSchema\nDefinition of a schema object.\n\n\nDraftValidation\nDraft a validation plan for a given table using an LLM.\n\n\n\n\n\n\nValidation steps can be thought of as sequential validations on the target data. We call Validate’s validation methods to build up a validation plan: a collection of steps that, in the aggregate, provides good validation coverage.\n\n\n\nValidate.col_vals_gt\nAre column data greater than a fixed value or data in another column?\n\n\nValidate.col_vals_lt\nAre column data less than a fixed value or data in another column?\n\n\nValidate.col_vals_ge\nAre column data greater than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_le\nAre column data less than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_eq\nAre column data equal to a fixed value or data in another column?\n\n\nValidate.col_vals_ne\nAre column data not equal to a fixed value or data in another column?\n\n\nValidate.col_vals_between\nDo column data lie between two specified values or data in other columns?\n\n\nValidate.col_vals_outside\nDo column data lie outside of two specified values or data in other columns?\n\n\nValidate.col_vals_in_set\nValidate whether column values are in a set of values.\n\n\nValidate.col_vals_not_in_set\nValidate whether column values are not in a set of values.\n\n\nValidate.col_vals_null\nValidate whether values in a column are NULL.\n\n\nValidate.col_vals_not_null\nValidate whether values in a column are not NULL.\n\n\nValidate.col_vals_regex\nValidate whether column values match a regular expression pattern.\n\n\nValidate.col_vals_expr\nValidate column values using a custom expression.\n\n\nValidate.col_exists\nValidate whether one or more columns exist in the table.\n\n\nValidate.rows_distinct\nValidate whether rows in the table are distinct.\n\n\nValidate.col_schema_match\nDo columns in the table (and their types) match a predefined schema?\n\n\nValidate.row_count_match\nValidate whether the row count of the table matches a specified count.\n\n\nValidate.col_count_match\nValidate whether the column count of the table matches a specified count.\n\n\n\n\n\n\nA flexible way to select columns for validation is to use the col() function along with column selection helper functions. A combination of col() + starts_with(), matches(), etc., allows for the selection of multiple target columns (mapping a validation across many steps). Furthermore, the col() function can be used to declare a comparison column (e.g., for the value= argument in many col_vals_*() methods) when you can’t use a fixed value for comparison.\n\n\n\ncol\nHelper function for referencing a column in the input table.\n\n\nstarts_with\nSelect columns that start with specified text.\n\n\nends_with\nSelect columns that end with specified text.\n\n\ncontains\nSelect columns that contain specified text.\n\n\nmatches\nSelect columns that match a specified regular expression pattern.\n\n\neverything\nSelect all columns.\n\n\nfirst_n\nSelect the first n columns in the column list.\n\n\nlast_n\nSelect the last n columns in the column list.\n\n\n\n\n\n\nThe validation plan is put into action when interrogate() is called. The workflow for performing a comprehensive validation is then: (1) Validate(), (2) adding validation steps, (3) interrogate(). After interrogation of the data, we can view a validation report table (by printing the object or using get_tabular_report()), extract key metrics, or we can split the data based on the validation results (with get_sundered_data()).\n\n\n\nValidate.interrogate\nExecute each validation step against the table and store the results.\n\n\nValidate.get_tabular_report\nValidation report as a GT table.\n\n\nValidate.get_step_report\nGet a detailed report for a single validation step.\n\n\nValidate.get_json_report\nGet a report of the validation results as a JSON-formatted string.\n\n\nValidate.get_sundered_data\nGet the data that passed or failed the validation steps.\n\n\nValidate.get_data_extracts\nGet the rows that failed for each validation step.\n\n\nValidate.all_passed\nDetermine if every validation step passed perfectly, with no failing test units.\n\n\nValidate.assert_passing\nRaise an AssertionError if all tests are not passing.\n\n\nValidate.n\nProvides a dictionary of the number of test units for each validation step.\n\n\nValidate.n_passed\nProvides a dictionary of the number of test units that passed for each validation step.\n\n\nValidate.n_failed\nProvides a dictionary of the number of test units that failed for each validation step.\n\n\nValidate.f_passed\nProvides a dictionary of the fraction of test units that passed for each validation step.\n\n\nValidate.f_failed\nProvides a dictionary of the fraction of test units that failed for each validation step.\n\n\nValidate.warning\nGet the ‘warning’ level status for each validation step.\n\n\nValidate.error\nGet the ‘error’ level status for each validation step.\n\n\nValidate.critical\nGet the ‘critical’ level status for each validation step.\n\n\n\n\n\n\nThe Inspect group contains functions that are helpful for getting to grips on a new data table. Use the DataScan class to get a quick overview of the data, preview() to see the first and last few rows of a table, missing_vals_tbl() to see where there are missing values in a table, and get_column_count()/get_row_count() to get the number of columns and rows in a table. Several datasets included in the package can be accessed via the load_dataset() function. Finally, the config() utility lets us set global configuration parameters.\n\n\n\nDataScan\nGet a summary of a dataset.\n\n\npreview\nDisplay a table preview that shows some rows from the top, some from the bottom.\n\n\nmissing_vals_tbl\nDisplay a table that shows the missing values in the input table.\n\n\nget_column_count\nGet the number of columns in a table.\n\n\nget_row_count\nGet the number of rows in a table.\n\n\nload_dataset\nLoad a dataset hosted in the library as specified table type.\n\n\nconfig\nConfiguration settings for the pointblank library."
  },
  {
    "objectID": "reference/index.html#validate",
    "href": "reference/index.html#validate",
    "title": "API Reference",
    "section": "",
    "text": "When peforming data validation, you’ll need the Validate class to get the process started. It’s given the target table and you can optionally provide some metadata and/or failure thresholds (using the Thresholds class or through shorthands for this task). The Validate class has numerous methods for defining validation steps and for obtaining post-interrogation metrics and data.\n\n\n\nValidate\nWorkflow for defining a set of validations on a table and interrogating for results.\n\n\nThresholds\nDefinition of threshold values.\n\n\nActions\nDefinition of action values.\n\n\nSchema\nDefinition of a schema object.\n\n\nDraftValidation\nDraft a validation plan for a given table using an LLM."
  },
  {
    "objectID": "reference/index.html#validation-steps",
    "href": "reference/index.html#validation-steps",
    "title": "API Reference",
    "section": "",
    "text": "Validation steps can be thought of as sequential validations on the target data. We call Validate’s validation methods to build up a validation plan: a collection of steps that, in the aggregate, provides good validation coverage.\n\n\n\nValidate.col_vals_gt\nAre column data greater than a fixed value or data in another column?\n\n\nValidate.col_vals_lt\nAre column data less than a fixed value or data in another column?\n\n\nValidate.col_vals_ge\nAre column data greater than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_le\nAre column data less than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_eq\nAre column data equal to a fixed value or data in another column?\n\n\nValidate.col_vals_ne\nAre column data not equal to a fixed value or data in another column?\n\n\nValidate.col_vals_between\nDo column data lie between two specified values or data in other columns?\n\n\nValidate.col_vals_outside\nDo column data lie outside of two specified values or data in other columns?\n\n\nValidate.col_vals_in_set\nValidate whether column values are in a set of values.\n\n\nValidate.col_vals_not_in_set\nValidate whether column values are not in a set of values.\n\n\nValidate.col_vals_null\nValidate whether values in a column are NULL.\n\n\nValidate.col_vals_not_null\nValidate whether values in a column are not NULL.\n\n\nValidate.col_vals_regex\nValidate whether column values match a regular expression pattern.\n\n\nValidate.col_vals_expr\nValidate column values using a custom expression.\n\n\nValidate.col_exists\nValidate whether one or more columns exist in the table.\n\n\nValidate.rows_distinct\nValidate whether rows in the table are distinct.\n\n\nValidate.col_schema_match\nDo columns in the table (and their types) match a predefined schema?\n\n\nValidate.row_count_match\nValidate whether the row count of the table matches a specified count.\n\n\nValidate.col_count_match\nValidate whether the column count of the table matches a specified count."
  },
  {
    "objectID": "reference/index.html#column-selection",
    "href": "reference/index.html#column-selection",
    "title": "API Reference",
    "section": "",
    "text": "A flexible way to select columns for validation is to use the col() function along with column selection helper functions. A combination of col() + starts_with(), matches(), etc., allows for the selection of multiple target columns (mapping a validation across many steps). Furthermore, the col() function can be used to declare a comparison column (e.g., for the value= argument in many col_vals_*() methods) when you can’t use a fixed value for comparison.\n\n\n\ncol\nHelper function for referencing a column in the input table.\n\n\nstarts_with\nSelect columns that start with specified text.\n\n\nends_with\nSelect columns that end with specified text.\n\n\ncontains\nSelect columns that contain specified text.\n\n\nmatches\nSelect columns that match a specified regular expression pattern.\n\n\neverything\nSelect all columns.\n\n\nfirst_n\nSelect the first n columns in the column list.\n\n\nlast_n\nSelect the last n columns in the column list."
  },
  {
    "objectID": "reference/index.html#interrogation-and-reporting",
    "href": "reference/index.html#interrogation-and-reporting",
    "title": "API Reference",
    "section": "",
    "text": "The validation plan is put into action when interrogate() is called. The workflow for performing a comprehensive validation is then: (1) Validate(), (2) adding validation steps, (3) interrogate(). After interrogation of the data, we can view a validation report table (by printing the object or using get_tabular_report()), extract key metrics, or we can split the data based on the validation results (with get_sundered_data()).\n\n\n\nValidate.interrogate\nExecute each validation step against the table and store the results.\n\n\nValidate.get_tabular_report\nValidation report as a GT table.\n\n\nValidate.get_step_report\nGet a detailed report for a single validation step.\n\n\nValidate.get_json_report\nGet a report of the validation results as a JSON-formatted string.\n\n\nValidate.get_sundered_data\nGet the data that passed or failed the validation steps.\n\n\nValidate.get_data_extracts\nGet the rows that failed for each validation step.\n\n\nValidate.all_passed\nDetermine if every validation step passed perfectly, with no failing test units.\n\n\nValidate.assert_passing\nRaise an AssertionError if all tests are not passing.\n\n\nValidate.n\nProvides a dictionary of the number of test units for each validation step.\n\n\nValidate.n_passed\nProvides a dictionary of the number of test units that passed for each validation step.\n\n\nValidate.n_failed\nProvides a dictionary of the number of test units that failed for each validation step.\n\n\nValidate.f_passed\nProvides a dictionary of the fraction of test units that passed for each validation step.\n\n\nValidate.f_failed\nProvides a dictionary of the fraction of test units that failed for each validation step.\n\n\nValidate.warning\nGet the ‘warning’ level status for each validation step.\n\n\nValidate.error\nGet the ‘error’ level status for each validation step.\n\n\nValidate.critical\nGet the ‘critical’ level status for each validation step."
  },
  {
    "objectID": "reference/index.html#inspect",
    "href": "reference/index.html#inspect",
    "title": "API Reference",
    "section": "",
    "text": "The Inspect group contains functions that are helpful for getting to grips on a new data table. Use the DataScan class to get a quick overview of the data, preview() to see the first and last few rows of a table, missing_vals_tbl() to see where there are missing values in a table, and get_column_count()/get_row_count() to get the number of columns and rows in a table. Several datasets included in the package can be accessed via the load_dataset() function. Finally, the config() utility lets us set global configuration parameters.\n\n\n\nDataScan\nGet a summary of a dataset.\n\n\npreview\nDisplay a table preview that shows some rows from the top, some from the bottom.\n\n\nmissing_vals_tbl\nDisplay a table that shows the missing values in the input table.\n\n\nget_column_count\nGet the number of columns in a table.\n\n\nget_row_count\nGet the number of rows in a table.\n\n\nload_dataset\nLoad a dataset hosted in the library as specified table type.\n\n\nconfig\nConfiguration settings for the pointblank library."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html",
    "href": "reference/Validate.col_vals_regex.html",
    "title": "Validate.col_vals_regex",
    "section": "",
    "text": "Validate.col_vals_regex(\n    columns,\n    pattern,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values match a regular expression pattern.\nThe col_vals_regex() validation method checks whether column values in a table correspond to a pattern= matching expression. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#parameters",
    "href": "reference/Validate.col_vals_regex.html#parameters",
    "title": "Validate.col_vals_regex",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npattern : str\n\nA regular expression pattern to compare against.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#returns",
    "href": "reference/Validate.col_vals_regex.html#returns",
    "title": "Validate.col_vals_regex",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#examples",
    "href": "reference/Validate.col_vals_regex.html#examples",
    "title": "Validate.col_vals_regex",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two string columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"rb-0343\", \"ra-0232\", \"ry-0954\", \"rc-1343\"],\n        \"b\": [\"ra-0628\", \"ra-583\", \"rya-0826\", \"rb-0735\"],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bString\n\n\n\n  \n    1\n    rb-0343\n    ra-0628\n  \n  \n    2\n    ra-0232\n    ra-583\n  \n  \n    3\n    ry-0954\n    rya-0826\n  \n  \n    4\n    rc-1343\n    rb-0735\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that all of the values in column a match a particular regex pattern. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=\"a\", pattern=r\"r[a-z]-[0-9]{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    a\n    r[a-z]-[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_regex(). All test units passed, and there are no failing test units.\nNow, let’s use the same regex for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=\"b\", pattern=r\"r[a-z]-[0-9]{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    b\n    r[a-z]-[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the string values of rows 1 and 2 in column b."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html",
    "href": "reference/Validate.col_vals_null.html",
    "title": "Validate.col_vals_null",
    "section": "",
    "text": "Validate.col_vals_null(\n    columns,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether values in a column are NULL.\nThe col_vals_null() validation method checks whether column values in a table are NULL. This validation will operate over the number of test units that is equal to the number of rows in the table."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#parameters",
    "href": "reference/Validate.col_vals_null.html#parameters",
    "title": "Validate.col_vals_null",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#returns",
    "href": "reference/Validate.col_vals_null.html#returns",
    "title": "Validate.col_vals_null",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#examples",
    "href": "reference/Validate.col_vals_null.html#examples",
    "title": "Validate.col_vals_null",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [None, None, None, None],\n        \"b\": [None, 2, None, 9],\n    }\n).with_columns(pl.col(\"a\").cast(pl.Int64))\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    None\n    None\n  \n  \n    2\n    None\n    2\n  \n  \n    3\n    None\n    None\n  \n  \n    4\n    None\n    9\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all Null values. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_null(columns=\"a\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_null()\n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_null(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_null(columns=\"b\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_null()\n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the two non-Null values in column b."
  },
  {
    "objectID": "reference/Validate.col_exists.html",
    "href": "reference/Validate.col_exists.html",
    "title": "Validate.col_exists",
    "section": "",
    "text": "Validate.col_exists(\n    columns,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether one or more columns exist in the table.\nThe col_exists() method checks whether one or more columns exist in the target table. The only requirement is specification of the column names. Each validation step or expectation will operate over a single test unit, which is whether the column exists or not."
  },
  {
    "objectID": "reference/Validate.col_exists.html#parameters",
    "href": "reference/Validate.col_exists.html#parameters",
    "title": "Validate.col_exists",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_exists.html#returns",
    "href": "reference/Validate.col_exists.html#returns",
    "title": "Validate.col_exists",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_exists.html#examples",
    "href": "reference/Validate.col_exists.html#examples",
    "title": "Validate.col_exists",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with a string columns (a) and a numeric column (b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bInt64\n\n\n\n  \n    1\n    apple\n    1\n  \n  \n    2\n    banana\n    6\n  \n  \n    3\n    cherry\n    3\n  \n  \n    4\n    date\n    5\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the columns a and b actually exist in the table. We’ll determine if this validation had any failing test units (each validation will have a single test unit).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_exists(columns=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows two entries (one check per column) generated by the col_exists() validation step. Both steps passed since both columns provided in columns= are present in the table.\nNow, let’s check for the existence of a different set of columns.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_exists(columns=[\"b\", \"c\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports one passing validation step (the check for column b) and one failing validation step (the check for column c, which doesn’t exist)."
  },
  {
    "objectID": "reference/Validate.row_count_match.html",
    "href": "reference/Validate.row_count_match.html",
    "title": "Validate.row_count_match",
    "section": "",
    "text": "Validate.row_count_match(\n    count,\n    tol=0,\n    inverse=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether the row count of the table matches a specified count.\nThe row_count_match() method checks whether the row count of the target table matches a specified count. This validation will operate over a single test unit, which is whether the row count matches the specified count.\nWe also have the option to invert the validation step by setting inverse=True. This will make the expectation that the row count of the target table does not match the specified count."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#parameters",
    "href": "reference/Validate.row_count_match.html#parameters",
    "title": "Validate.row_count_match",
    "section": "Parameters",
    "text": "Parameters\n\ncount : int | FrameT | Any\n\nThe expected row count of the table. This can be an integer value, a Polars or Pandas DataFrame object, or an Ibis backend table. If a DataFrame/table is provided, the row count of that object will be used as the expected count.\n\ntol : Tolerance = 0\n\nThe tolerance allowable for the row count match. This can be specified as a single numeric value (integer or float) or as a tuple of two integers representing the lower and upper bounds of the tolerance range. If a single integer value (greater than 1) is provided, it represents the absolute bounds of the tolerance, ie. plus or minus the value. If a float value (between 0-1) is provided, it represents the relative tolerance, ie. plus or minus the relative percentage of the target. If a tuple is provided, it represents the lower and upper absolute bounds of the tolerance range. See the examples for more.\n\ninverse : bool = False\n\nShould the validation step be inverted? If True, then the expectation is that the row count of the target table should not match the specified count= value.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#returns",
    "href": "reference/Validate.row_count_match.html#returns",
    "title": "Validate.row_count_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#examples",
    "href": "reference/Validate.row_count_match.html#examples",
    "title": "Validate.row_count_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use the built in dataset \"small_table\". The table can be obtained by calling load_dataset(\"small_table\").\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(\"small_table\")\n\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the number of rows in the table matches a fixed value. In this case, we will use the value 13 as the expected row count.\n\nvalidation = (\n    pb.Validate(data=small_table)\n    .row_count_match(count=13)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         row_count_match()\n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the expectation value of 13 matches the actual count of rows in the target table. So, the single test unit passed.\nLet’s modify our example to show the different ways we can allow some tolerance to our validation by using the tol argument.\n\nsmaller_small_table = small_table.sample(n = 12) # within the lower bound\nvalidation = (\n    pb.Validate(data=smaller_small_table)\n    .row_count_match(count=13,tol=(2, 0)) # minus 2 but plus 0, ie. 11-13\n    .interrogate()\n)\n\nvalidation\n\nvalidation = (\n    pb.Validate(data=smaller_small_table)\n    .row_count_match(count=13,tol=.05) # .05% tolerance of 13\n    .interrogate()\n)\n\neven_smaller_table = small_table.sample(n = 2)\nvalidation = (\n    pb.Validate(data=even_smaller_table)\n    .row_count_match(count=13,tol=5) # plus or minus 5; this test will fail\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n         row_count_match()\n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —"
  },
  {
    "objectID": "reference/Validate.interrogate.html",
    "href": "reference/Validate.interrogate.html",
    "title": "Validate.interrogate",
    "section": "",
    "text": "Validate.interrogate(\n    collect_extracts=True,\n    collect_tbl_checked=True,\n    get_first_n=None,\n    sample_n=None,\n    sample_frac=None,\n    sample_limit=5000,\n)\nExecute each validation step against the table and store the results.\nWhen a validation plan has been set with a series of validation steps, the interrogation process through interrogate() should then be invoked. Interrogation will evaluate each validation step against the table and store the results.\nThe interrogation process will collect extracts of failing rows if the collect_extracts= option is set to True (the default). We can control the number of rows collected using the get_first_n=, sample_n=, and sample_frac= options. The sample_limit= option will enforce a hard limit on the number of rows collected when using the sample_frac= option.\nAfter interrogation is complete, the Validate object will have gathered information, and we can use methods like n_passed(), f_failed(), etc., to understand how the table performed against the validation plan. A visual representation of the validation results can be viewed by printing theValidate` object; this will display the validation table in an HTML viewing environment."
  },
  {
    "objectID": "reference/Validate.interrogate.html#parameters",
    "href": "reference/Validate.interrogate.html#parameters",
    "title": "Validate.interrogate",
    "section": "Parameters",
    "text": "Parameters\n\ncollect_extracts : bool = True\n\nAn option to collect rows of the input table that didn’t pass a particular validation step. The default is True and further options (i.e., get_first_n=, sample_*=) allow for fine control of how these rows are collected.\n\ncollect_tbl_checked : bool = True\n\nThe processed data frames produced by executing the validation steps is collected and stored in the Validate object if collect_tbl_checked=True. This information is necessary for some methods (e.g., get_sundered_data()), but it potentially makes the object grow to a large size. To opt out of attaching this data, set this argument to False.\n\nget_first_n : int | None = None\n\nIf the option to collect rows where test units is chosen, there is the option here to collect the first n rows. Supply an integer number of rows to extract from the top of subset table containing non-passing rows (the ordering of data from the original table is retained).\n\nsample_n : int | None = None\n\nIf the option to collect non-passing rows is chosen, this option allows for the sampling of n rows. Supply an integer number of rows to sample from the subset table. If n happens to be greater than the number of non-passing rows, then all such rows will be returned.\n\nsample_frac : int | float | None = None\n\nIf the option to collect non-passing rows is chosen, this option allows for the sampling of a fraction of those rows. Provide a number in the range of 0 and 1. The number of rows to return could be very large, however, the sample_limit= option will apply a hard limit to the returned rows.\n\nsample_limit : int = 5000\n\nA value that limits the possible number of rows returned when sampling non-passing rows using the sample_frac= option."
  },
  {
    "objectID": "reference/Validate.interrogate.html#returns",
    "href": "reference/Validate.interrogate.html#returns",
    "title": "Validate.interrogate",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the results of the interrogation."
  },
  {
    "objectID": "reference/Validate.interrogate.html#examples",
    "href": "reference/Validate.interrogate.html#examples",
    "title": "Validate.interrogate",
    "section": "Examples",
    "text": "Examples\nLet’s use a built-in dataset (\"game_revenue\") to demonstrate some of the options of the interrogation process. A series of validation steps will populate our validation plan. After setting up the plan, the next step is to interrogate the table and see how well it aligns with our expectations. We’ll use the get_first_n= option so that any extracts of failing rows are limited to the first n rows.\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"game_revenue\"))\n    .col_vals_lt(columns=\"item_revenue\", value=200)\n    .col_vals_gt(columns=\"item_revenue\", value=0)\n    .col_vals_gt(columns=\"session_duration\", value=5)\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n)\n\nvalidation.interrogate(get_first_n=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:51:42Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    item_revenue\n    200\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    [A-Z]{12}[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:51:42 UTC&lt; 1 s2025-03-06 23:51:42 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that step 3 (checking for session_duration greater than 5) has 18 failing test units. This means that 18 rows in the table are problematic. We’d like to see the rows that failed this validation step and we can do that with the get_data_extracts() method.\n\npb.preview(validation.get_data_extracts(i=3, frame=True))\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows10Columns12\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    620\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:25:18+00:00\n    iap\n    offer4\n    17.991\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    621\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:26:24+00:00\n    iap\n    offer5\n    26.09\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    622\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:28:36+00:00\n    ad\n    ad_15sec\n    0.53\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n\n\n\n\n\n\n        \n\n\nThe get_data_extracts() method will return a Polars DataFrame here with the first 10 rows that failed the validation step (we passed that into the preview() function for a better display). There are actually 18 rows that failed but we limited the collection of extracts with get_first_n=10."
  },
  {
    "objectID": "reference/Validate.html",
    "href": "reference/Validate.html",
    "title": "Validate",
    "section": "",
    "text": "Validate(\n    self,\n    data,\n    tbl_name=None,\n    label=None,\n    thresholds=None,\n    actions=None,\n    lang=None,\n    locale=None,\n)\nWorkflow for defining a set of validations on a table and interrogating for results.\nThe Validate class is used for defining a set of validation steps on a table and interrogating the table with the validation plan. This class is the main entry point for the data quality reporting workflow. The overall aim of this workflow is to generate comprehensive reporting information to assess the level of data quality for a target table.\nWe can supply as many validation steps as needed, and having a large number of them should increase the validation coverage for a given table. The validation methods (e.g., col_vals_gt(), col_vals_between(), etc.) translate to discrete validation steps, where each step will be sequentially numbered (useful when viewing the reporting data). This process of calling validation methods is known as developing a validation plan.\nThe validation methods, when called, are merely instructions up to the point the concluding interrogate() method is called. That kicks off the process of acting on the validation plan by querying the target table getting reporting results for each step. Once the interrogation process is complete, we can say that the workflow now has reporting information. We can then extract useful information from the reporting data to understand the quality of the table. Printing the Validate object (or using the get_tabular_report() method) will return a table with the results of the interrogation and get_sundered_data() allows for the splitting of the table based on passing and failing rows."
  },
  {
    "objectID": "reference/Validate.html#parameters",
    "href": "reference/Validate.html#parameters",
    "title": "Validate",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to validate, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str | None = None\n\nA optional name to assign to the input table object. If no value is provided, a name will be generated based on whatever information is available. This table name will be displayed in the header area of the tabular report.\n\nlabel : str | None = None\n\nAn optional label for the validation plan. If no value is provided, a label will be generated based on the current system date and time. Markdown can be used here to make the label more visually appealing (it will appear in the header area of the tabular report).\n\nthresholds : int | float | bool | tuple | dict | Thresholds | None = None\n\nGenerate threshold failure levels so that all validation steps can report and react accordingly when exceeding the set levels. This is to be created using one of several valid input schemes: (1) single integer/float denoting absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, (3) a dictionary of 1-3 entries, or a Thresholds object.\n\nactions : Actions | None = None\n\nThe actions to take when validation steps meet or exceed any set threshold levels. This should be provided in the form of an Actions object. If None then no default actions will be set.\n\nlang : str | None = None\n\nThe language to use for automatic creation of briefs (short descriptions for each validation step). By default, None will create English (\"en\") text. Other options include French (\"fr\"), German (\"de\"), Italian (\"it\"), Spanish (\"es\"), Portuguese (\"pt\"), Turkish (\"tr\"), Chinese (\"zh\"), Russian (\"ru\"), Polish (\"pl\"), Danish (\"da\"), Swedish (\"sv\"), and Dutch (\"nl\").\n\nlocale : str | None = None\n\nAn optional locale ID to use for formatting values in the reporting table according the locale’s rules. Examples include \"en-US\" for English (United States) and \"fr-FR\" for French (France). More simply, this can be a language identifier without a designation of territory, like \"es\" for Spanish."
  },
  {
    "objectID": "reference/Validate.html#returns",
    "href": "reference/Validate.html#returns",
    "title": "Validate",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nA Validate object with the table and validations to be performed."
  },
  {
    "objectID": "reference/Validate.html#supported-input-table-types",
    "href": "reference/Validate.html#supported-input-table-types",
    "title": "Validate",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, the use of Validate with such tables requires the Ibis library v9.5.0 and above to be installed. If the input table is a Polars or Pandas DataFrame, the Ibis library is not required."
  },
  {
    "objectID": "reference/Validate.html#examples",
    "href": "reference/Validate.html#examples",
    "title": "Validate",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "reference/Validate.html#creating-a-validation-plan-and-interrogating",
    "href": "reference/Validate.html#creating-a-validation-plan-and-interrogating",
    "title": "Validate",
    "section": "Creating a validation plan and interrogating",
    "text": "Creating a validation plan and interrogating\nLet’s walk through a data quality analysis of an extremely small table. It’s actually called \"small_table\" and it’s accessible through the load_dataset() function.\n\nimport pointblank as pb\n\n# Load the small_table dataset\nsmall_table = pb.load_dataset()\n\n# Preview the table\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nWe ought to think about what’s tolerable in terms of data quality so let’s designate proportional failure thresholds to the ‘warning’, ‘error’, and ‘critical’ states. This can be done by using the Thresholds class.\n\nthresholds = pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n\nNow, we use the Validate class and give it the thresholds object (which serves as a default for all validation steps but can be overridden). The static thresholds provided in thresholds= will make the reporting a bit more useful. We also need to provide a target table and we’ll use small_table for this.\n\nvalidation = (\n    pb.Validate(\n        data=small_table,\n        tbl_name=\"small_table\",\n        label=\"`Validate` example.\",\n        thresholds=thresholds\n    )\n)\n\nThen, as with any Validate object, we can add steps to the validation plan by using as many validation methods as we want. To conclude the process (and actually query the data table), we use the interrogate() method.\n\nvalidation = (\n    validation\n    .col_vals_gt(columns=\"d\", value=100)\n    .col_vals_le(columns=\"c\", value=5)\n    .col_vals_between(columns=\"c\", left=3, right=10, na_pass=True)\n    .col_vals_regex(columns=\"b\", pattern=r\"[0-9]-[a-z]{3}-[0-9]{3}\")\n    .col_exists(columns=[\"date\", \"date_time\"])\n    .interrogate()\n)\n\nThe validation object can be printed as a reporting table.\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    `Validate` example.Polarssmall_tableWARNING0.1ERROR0.25CRITICAL0.35\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    c\n    [3, 10]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    b\n    [0-9]-[a-z]{3}-[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n        \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n         col_exists()\n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:51:49 UTC&lt; 1 s2025-03-06 23:51:49 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe report could be further customized by using the get_tabular_report() method, which contains options for modifying the display of the table.\nFurthermore, post-interrogation methods such as get_step_report(), get_data_extracts(), and get_sundered_data() allow you to generate additional reporting or extract useful data for downstream analysis from a Validate object."
  },
  {
    "objectID": "reference/col.html",
    "href": "reference/col.html",
    "title": "col",
    "section": "",
    "text": "col(exprs)\nHelper function for referencing a column in the input table.\nMany of the validation methods (i.e., col_vals_*() methods) in Pointblank have a value= argument. These validations are comparisons between column values and a literal value, or, between column values and adjacent values in another column. The col() helper function is used to specify that it is a column being referenced, not a literal value.\nThe col() doesn’t check that the column exists in the input table. It acts to signal that the value being compared is a column value. During validation (i.e., when interrogate() is called), Pointblank will then check that the column exists in the input table."
  },
  {
    "objectID": "reference/col.html#parameters",
    "href": "reference/col.html#parameters",
    "title": "col",
    "section": "Parameters",
    "text": "Parameters\n\nexprs : str | ColumnSelector | ColumnSelectorNarwhals\n\nEither the name of a single column in the target table, provided as a string, or, an expression involving column selector functions (e.g., starts_with(\"a\"), ends_with(\"e\") \\| starts_with(\"a\"), etc.). Please read the documentation for further details on which input forms are valid depending on the context."
  },
  {
    "objectID": "reference/col.html#returns",
    "href": "reference/col.html#returns",
    "title": "col",
    "section": "Returns",
    "text": "Returns\n\n : Column\n\nA Column object representing the column."
  },
  {
    "objectID": "reference/col.html#usage-with-the-columns-argument",
    "href": "reference/col.html#usage-with-the-columns-argument",
    "title": "col",
    "section": "Usage with the columns= Argument",
    "text": "Usage with the columns= Argument\nThe col() function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nIf specifying a single column with certainty (you have the exact name), col() is not necessary since you can just pass the column name as a string (though it is still valid to use col(\"column_name\"), if preferred). However, if you want to select columns based on complex logic involving multiple column selector functions (e.g., columns that start with \"a\" but don’t end with \"e\"), you need to use col() to wrap expressions involving column selector functions and logical operators such as &, |, -, and ~.\nHere is an example of such usage with the col_vals_gt() validation method:\ncol_vals_gt(columns=col(starts_with(\"a\") & ~ends_with(\"e\")), value=10)\nIf using only a single column selector function, you can pass the function directly to the columns= argument of the validation method, or, you can use col() to wrap the function (either is valid though the first is more concise). Here is an example of that simpler usage:\ncol_vals_gt(columns=starts_with(\"a\"), value=10)"
  },
  {
    "objectID": "reference/col.html#usage-with-the-value-left-and-right-arguments",
    "href": "reference/col.html#usage-with-the-value-left-and-right-arguments",
    "title": "col",
    "section": "Usage with the value=, left=, and right= Arguments",
    "text": "Usage with the value=, left=, and right= Arguments\nThe col() function can be used in the value= argument of the following validation methods\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\n\nand in the left= and right= arguments (either or both) of these two validation methods\n\ncol_vals_between()\ncol_vals_outside()\n\nYou cannot use column selector functions such as starts_with() in either of the value=, left=, or right= arguments since there would be no guarantee that a single column will be resolved from the target table with this approach. The col() function is used to signal that the value being compared is a column value and not a literal value."
  },
  {
    "objectID": "reference/col.html#available-selectors",
    "href": "reference/col.html#available-selectors",
    "title": "col",
    "section": "Available Selectors",
    "text": "Available Selectors\nThere is a collection of selectors available in pointblank, allowing you to select columns based on attributes of column names and positions. The selectors are:\n\nstarts_with()\nends_with()\ncontains()\nmatches()\neverything()\nfirst_n()\nlast_n()\n\nAlternatively, we support selectors from the Narwhals library! Those selectors can additionally take advantage of the data types of the columns. The selectors are:\n\nboolean()\nby_dtype()\ncategorical()\nmatches()\nnumeric()\nstring()\n\nHave a look at the Narwhals API documentation on selectors for more information."
  },
  {
    "objectID": "reference/col.html#examples",
    "href": "reference/col.html#examples",
    "title": "col",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns a and b and we’d like to validate that the values in column a are greater than the values in column b. We can use the col() helper function to reference the comparison column when creating the validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 6, 5],\n        \"b\": [4, 2, 3, 3, 4, 3],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom results of the validation table it can be seen that values in a were greater than values in b for every row (or test unit). Using value=pb.col(\"b\") specified that the greater-than comparison is across columns, not with a fixed literal value.\nIf you want to select an arbitrary set of columns upon which to base a validation, you can use column selector functions (e.g., starts_with(), ends_with(), etc.) to specify columns in the columns= argument of a validation method. Let’s use the starts_with() column selector function to select columns that start with \"paid\" and validate that the values in those columns are greater than 10.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [16.32, 16.25, 15.75],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.starts_with(\"paid\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the starts_with() column selector function. This is not strictly necessary when using a single column selector function, so columns=pb.starts_with(\"paid\") would be equivalent usage here. However, the use of col() is required when using multiple column selector functions with logical operators. Here is an example of that more complex usage:\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.starts_with(\"paid\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the starts_with() and matches() column selector functions, combined with the & operator. This is necessary to specify the set of columns that start with \"paid\" and match the text \"2023\" or \"2024\".\nIf you’d like to take advantage of Narwhals selectors, that’s also possible. Here is an example of using the numeric() column selector function to select all numeric columns for validation, checking that their values are greater than 0.\n\nimport narwhals.selectors as ncs\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=pb.col(ncs.numeric()), value=0)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    hours_2022\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    hours_2023\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    hours_2024\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    paid_2022\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    paid_2023\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    paid_2024\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the numeric() column selector function from Narwhals. As with the other selectors, this is not strictly necessary when using a single column selector, so columns=ncs.numeric() would also be fine here.\nNarwhals selectors can also use operators to combine multiple selectors. Here is an example of using the numeric() and matches() selectors together to select all numeric columns that fit a specific pattern.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_status\": [\"ft\", \"ft\", \"pt\"],\n        \"2023_status\": [\"ft\", \"pt\", \"ft\"],\n        \"2024_status\": [\"ft\", \"pt\", \"ft\"],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.col(ncs.numeric() & ncs.matches(\"2023|2024\")), value=30)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2023_pay_total\n    30\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    2024_pay_total\n    30\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the numeric() and matches() column selector functions from Narwhals, combined with the & operator. This is necessary to specify the set of columns that are numeric and match the text \"2023\" or \"2024\"."
  },
  {
    "objectID": "reference/get_row_count.html",
    "href": "reference/get_row_count.html",
    "title": "get_row_count",
    "section": "",
    "text": "get_row_count(data)\nGet the number of rows in a table.\nThe get_row_count() function returns the number of rows in a table. The function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.)."
  },
  {
    "objectID": "reference/get_row_count.html#parameters",
    "href": "reference/get_row_count.html#parameters",
    "title": "get_row_count",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to get the row count, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/get_row_count.html#returns",
    "href": "reference/get_row_count.html#returns",
    "title": "get_row_count",
    "section": "Returns",
    "text": "Returns\n\n : int\n\nThe number of rows in the table."
  },
  {
    "objectID": "reference/get_row_count.html#supported-input-table-types",
    "href": "reference/get_row_count.html#supported-input-table-types",
    "title": "get_row_count",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using get_row_count() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/get_row_count.html#examples",
    "href": "reference/get_row_count.html#examples",
    "title": "get_row_count",
    "section": "Examples",
    "text": "Examples\nGetting the number of rows in a table is easily done by using the get_row_count() function. Here’s an example using the game_revenue dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\ngame_revenue_polars = pb.load_dataset(\"game_revenue\")\n\npb.get_row_count(game_revenue_polars)\n\n2000\n\n\nThis table is a Polars DataFrame, but the get_row_count() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\ngame_revenue_duckdb = pb.load_dataset(\"game_revenue\", tbl_type=\"duckdb\")\n\npb.get_row_count(game_revenue_duckdb)\n\n2000\n\n\nThe function always returns the number of rows in the table as an integer value, which is 2000 for the game_revenue dataset."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html",
    "href": "reference/Validate.col_vals_ge.html",
    "title": "Validate.col_vals_ge",
    "section": "",
    "text": "Validate.col_vals_ge(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data greater than or equal to a fixed value or data in another column?\nThe col_vals_ge() validation method checks whether column values in a table are greater than or equal to a specified value= (the exact comparison used in this function is col_val &gt;= value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#parameters",
    "href": "reference/Validate.col_vals_ge.html#parameters",
    "title": "Validate.col_vals_ge",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#returns",
    "href": "reference/Validate.col_vals_ge.html#returns",
    "title": "Validate.col_vals_ge",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#examples",
    "href": "reference/Validate.col_vals_ge.html#examples",
    "title": "Validate.col_vals_ge",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [5, 3, 1, 8, 2, 3],\n        \"c\": [2, 3, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    5\n    2\n  \n  \n    2\n    6\n    3\n    3\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    8\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    3\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all greater than or equal to the value of 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=\"a\", value=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_ge(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col()) to perform a column-column comparison. For the next example, we’ll use col_vals_ge() to check whether the values in column b are greater than values in column c.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=\"b\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_ge()\n        \n    b\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 0: b is 2 and c is 3.\nRow 4: b is 3 and c is 4."
  },
  {
    "objectID": "reference/Validate.n_failed.html",
    "href": "reference/Validate.n_failed.html",
    "title": "Validate.n_failed",
    "section": "",
    "text": "Validate.n_failed(i=None, scalar=False)\nProvides a dictionary of the number of test units that failed for each validation step.\nThe n_failed() method provides the number of test units that failed for each validation step. This is the number of test units that did not pass in the the validation step. It is always some integer value between 0 and the total number of test units.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of failing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the n_passed() method (i.e., n - n_passed)."
  },
  {
    "objectID": "reference/Validate.n_failed.html#parameters",
    "href": "reference/Validate.n_failed.html#parameters",
    "title": "Validate.n_failed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of failing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n_failed.html#returns",
    "href": "reference/Validate.n_failed.html#returns",
    "title": "Validate.n_failed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of failing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n_failed.html#examples",
    "href": "reference/Validate.n_failed.html#examples",
    "title": "Validate.n_failed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps and, as it turns out, all of them will have failing test units. After interrogation, the n_failed() method is used to determine the number of failing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12],\n        \"b\": [9, 8, 10, 5, 10],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.n_failed()\n\n{1: 1, 2: 2, 3: 1}\n\n\nThe returned dictionary shows that all validation steps had failing test units.\nIf we wanted to check the number of failing test units for a single validation step, we can provide the step number. Also, we could forego the dictionary and get a scalar value by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.n_failed(i=1)\n\n{1: 1}\n\n\nThe returned value of 1 is the number of failing test units for the first validation step."
  },
  {
    "objectID": "reference/get_column_count.html",
    "href": "reference/get_column_count.html",
    "title": "get_column_count",
    "section": "",
    "text": "get_column_count(data)\nGet the number of columns in a table.\nThe get_column_count() function returns the number of columns in a table. The function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.)."
  },
  {
    "objectID": "reference/get_column_count.html#parameters",
    "href": "reference/get_column_count.html#parameters",
    "title": "get_column_count",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to get the column count, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/get_column_count.html#returns",
    "href": "reference/get_column_count.html#returns",
    "title": "get_column_count",
    "section": "Returns",
    "text": "Returns\n\n : int\n\nThe number of columns in the table."
  },
  {
    "objectID": "reference/get_column_count.html#supported-input-table-types",
    "href": "reference/get_column_count.html#supported-input-table-types",
    "title": "get_column_count",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using get_column_count() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/get_column_count.html#examples",
    "href": "reference/get_column_count.html#examples",
    "title": "get_column_count",
    "section": "Examples",
    "text": "Examples\nTo get the number of columns in a table, we can use the get_column_count() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table_polars = pb.load_dataset(\"small_table\")\n\npb.get_column_count(small_table_polars)\n\n8\n\n\nThis table is a Polars DataFrame, but the get_column_count() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nsmall_table_duckdb = pb.load_dataset(\"small_table\", tbl_type=\"duckdb\")\n\npb.get_column_count(small_table_duckdb)\n\n8\n\n\nThe function always returns the number of columns in the table as an integer value, which is 8 for the small_table dataset."
  },
  {
    "objectID": "reference/Validate.critical.html",
    "href": "reference/Validate.critical.html",
    "title": "Validate.critical",
    "section": "",
    "text": "Validate.critical(i=None, scalar=False)\nGet the ‘critical’ level status for each validation step.\nThe ‘critical’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the notification level. Otherwise, the status is False.\nThe ascribed name of ‘critical’ is semantic and is thus simply a status indicator that could be used to trigger some action to be take. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the notification status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.critical.html#parameters",
    "href": "reference/Validate.critical.html#parameters",
    "title": "Validate.critical",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the notification status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.critical.html#returns",
    "href": "reference/Validate.critical.html#returns",
    "title": "Validate.critical",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the notification status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.critical.html#examples",
    "href": "reference/Validate.critical.html#examples",
    "title": "Validate.critical",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have many failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the critical() method is used to determine the ‘critical’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [2, 4, 4, 7, 2, 3, 8],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.critical()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘critical’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘critical’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘critical’ level.\nWe can also visually inspect the ‘critical’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:52:23PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    20.29\n    50.71\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-03-06 23:52:23 UTC&lt; 1 s2025-03-06 23:52:23 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there are filled gray, yellow, and red circles in the first step (far right side, in the W, E, and C columns) indicating that the ‘warning’, ‘error’, and ‘critical’ thresholds were met. The other steps have empty gray, yellow, and red circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘critical’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.critical(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had the ‘critical’ threshold met."
  },
  {
    "objectID": "reference/matches.html",
    "href": "reference/matches.html",
    "title": "matches",
    "section": "",
    "text": "matches(pattern, case_sensitive=False)\nSelect columns that match a specified regular expression pattern.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The matches() selector function can be used to select one or more columns matching a provided regular expression pattern. So if the set of table columns consists of\n[rev_01, rev_02, profit_01, profit_02, age]\nand you want to validate columns that have two digits at the end of the name, you can use columns=matches(r\"\\d{2}$\"). This will select the rev_01, rev_02, profit_01, and profit_02 columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using matches() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/matches.html#parameters",
    "href": "reference/matches.html#parameters",
    "title": "matches",
    "section": "Parameters",
    "text": "Parameters\n\npattern : str\n\nThe regular expression pattern that the column name should match.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/matches.html#returns",
    "href": "reference/matches.html#returns",
    "title": "matches",
    "section": "Returns",
    "text": "Returns\n\n : Matches\n\nA Matches object, which can be used to select columns that match the specified pattern."
  },
  {
    "objectID": "reference/matches.html#relevant-validation-methods-where-matches-can-be-used",
    "href": "reference/matches.html#relevant-validation-methods-where-matches-can-be-used",
    "title": "matches",
    "section": "Relevant Validation Methods where matches() can be Used",
    "text": "Relevant Validation Methods where matches() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe matches() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/matches.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/matches.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "matches",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe matches() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that have the text starting with five digits and end with \"_id\", you can use the matches() and ends_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(matches(r\"^\\d{5}\") & ends_with(\"_id\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/matches.html#examples",
    "href": "reference/matches.html#examples",
    "title": "matches",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, id_old, new_identifier, and pay_2021 and we’d like to validate that text values in columns having \"id\" or \"identifier\" in the name have a specific syntax. We can use the matches() column selector function to specify the columns that match the pattern.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"id_old\": [\"ID0021\", \"ID0032\", \"ID0043\"],\n        \"new_identifier\": [\"ID9054\", \"ID9065\", \"ID9076\"],\n        \"pay_2021\": [16.32, 16.25, 15.75],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=pb.matches(\"id|identifier\"), pattern=r\"ID\\d{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    id_old\n    ID\\d{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    new_identifier\n    ID\\d{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for id_old and one for new_identifier. The values in both columns all match the pattern \"ID\\d{4}\".\nWe can also use the matches() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that contain \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.contains(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2023_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    2024_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay_total and one for 2024_pay_total."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html",
    "href": "reference/Validate.col_vals_not_in_set.html",
    "title": "Validate.col_vals_not_in_set",
    "section": "",
    "text": "Validate.col_vals_not_in_set(\n    columns,\n    set,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values are not in a set of values.\nThe col_vals_not_in_set() validation method checks whether column values in a table are not part of a specified set= of values. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#parameters",
    "href": "reference/Validate.col_vals_not_in_set.html#parameters",
    "title": "Validate.col_vals_not_in_set",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nset : list[float | int]\n\nA list of values to compare against.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False"
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#returns",
    "href": "reference/Validate.col_vals_not_in_set.html#returns",
    "title": "Validate.col_vals_not_in_set",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#examples",
    "href": "reference/Validate.col_vals_not_in_set.html#examples",
    "title": "Validate.col_vals_not_in_set",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 8, 1, 9, 1, 7],\n        \"b\": [1, 8, 2, 6, 9, 1],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    7\n    1\n  \n  \n    2\n    8\n    8\n  \n  \n    3\n    1\n    2\n  \n  \n    4\n    9\n    6\n  \n  \n    5\n    1\n    9\n  \n  \n    6\n    7\n    1\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that none of the values in column a are in the set of [2, 3, 4, 5, 6]. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_in_set(columns=\"a\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_in_set()\n        \n    a\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_not_in_set(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_in_set(columns=\"b\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_in_set()\n        \n    b\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the column b values of 2 and 6, both of which are in the set of [2, 3, 4, 5, 6]."
  },
  {
    "objectID": "reference/Validate.all_passed.html",
    "href": "reference/Validate.all_passed.html",
    "title": "Validate.all_passed",
    "section": "",
    "text": "Validate.all_passed()\nDetermine if every validation step passed perfectly, with no failing test units.\nThe all_passed() method determines if every validation step passed perfectly, with no failing test units. This method is useful for quickly checking if the table passed all validation steps with flying colors. If there’s even a single failing test unit in any validation step, this method will return False.\nThis validation metric might be overly stringent for some validation plans where failing test units are generally expected (and the strategy is to monitor data quality over time). However, the value of all_passed() could be suitable for validation plans designed to ensure that every test unit passes perfectly (e.g., checks for column presence, null-checking tests, etc.)."
  },
  {
    "objectID": "reference/Validate.all_passed.html#returns",
    "href": "reference/Validate.all_passed.html#returns",
    "title": "Validate.all_passed",
    "section": "Returns",
    "text": "Returns\n\n : bool\n\nTrue if all validation steps had no failing test units, False otherwise."
  },
  {
    "objectID": "reference/Validate.all_passed.html#examples",
    "href": "reference/Validate.all_passed.html#examples",
    "title": "Validate.all_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the second step will have a failing test unit (the value 10 isn’t less than 9). After interrogation, the all_passed() method is used to determine if all validation steps passed perfectly.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 9, 5],\n        \"b\": [5, 6, 10, 3],\n        \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_vals_lt(columns=\"b\", value=9)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.all_passed()\n\nFalse\n\n\nThe returned value is False since the second validation step had a failing test unit. If it weren’t for that one failing test unit, the return value would have been True."
  },
  {
    "objectID": "reference/starts_with.html",
    "href": "reference/starts_with.html",
    "title": "starts_with",
    "section": "",
    "text": "starts_with(text, case_sensitive=False)\nSelect columns that start with specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The starts_with() selector function can be used to select one or more columns that start with some specified text. So if the set of table columns consists of\n[name_first, name_last, age, address]\nand you want to validate columns that start with \"name\", you can use columns=starts_with(\"name\"). This will select the name_first and name_last columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using starts_with() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/starts_with.html#parameters",
    "href": "reference/starts_with.html#parameters",
    "title": "starts_with",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should start with.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/starts_with.html#returns",
    "href": "reference/starts_with.html#returns",
    "title": "starts_with",
    "section": "Returns",
    "text": "Returns\n\n : StartsWith\n\nA StartsWith object, which can be used to select columns that start with the specified text."
  },
  {
    "objectID": "reference/starts_with.html#relevant-validation-methods-where-starts_with-can-be-used",
    "href": "reference/starts_with.html#relevant-validation-methods-where-starts_with-can-be-used",
    "title": "starts_with",
    "section": "Relevant Validation Methods where starts_with() can be Used",
    "text": "Relevant Validation Methods where starts_with() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe starts_with() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/starts_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/starts_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "starts_with",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe starts_with() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that start with \"a\" and end with \"e\", you can use the starts_with() and ends_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(starts_with(\"a\") & ends_with(\"e\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/starts_with.html#examples",
    "href": "reference/starts_with.html#examples",
    "title": "starts_with",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, paid_2021, paid_2022, and person_id and we’d like to validate that the values in columns that start with \"paid\" are greater than 10. We can use the starts_with() column selector function to specify the columns that start with \"paid\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [16.32, 16.25, 15.75],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.starts_with(\"paid\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for paid_2021 and one for paid_2022. The values in both columns were all greater than 10.\nWe can also use the starts_with() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that start with \"paid\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.starts_with(\"paid\") & pb.matches(\"23|24\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for paid_2023 and one for paid_2024."
  },
  {
    "objectID": "reference/Actions.html",
    "href": "reference/Actions.html",
    "title": "Actions",
    "section": "",
    "text": "Actions(self, warning=None, error=None, critical=None)\nDefinition of action values.\nActions complement threshold values by defining what action should be taken when a threshold level is reached. The action can be a string or a Callable. When a string is used, it is interpreted as a message to be displayed. When a Callable is used, it will be invoked at interrogation time if the threshold level is met or exceeded.\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. These levels correspond to different levels of severity when a threshold is reached. Those thresholds can be defined using the Thresholds class or various shorthand forms. Actions don’t have to be defined for all threshold levels; if an action is not defined for a level in exceedence, no action will be taken."
  },
  {
    "objectID": "reference/Actions.html#parameters",
    "href": "reference/Actions.html#parameters",
    "title": "Actions",
    "section": "Parameters",
    "text": "Parameters\n\nwarning : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘warning’ level. Using None means no action should be performed at the ‘warning’ level.\n\nerror : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘error’ level. Using None means no action should be performed at the ‘error’ level.\n\ncritical : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘critical’ level. Using None means no action should be performed at the ‘critical’ level."
  },
  {
    "objectID": "reference/Actions.html#returns",
    "href": "reference/Actions.html#returns",
    "title": "Actions",
    "section": "Returns",
    "text": "Returns\n\n : Actions\n\nAn Actions object. This can be used when using the Validate class (to set actions for meeting different threshold levels globally) or when defining validation steps like col_vals_gt() (so that actions are scoped to individual validation steps, overriding any globally set actions)."
  },
  {
    "objectID": "reference/Actions.html#types-of-actions",
    "href": "reference/Actions.html#types-of-actions",
    "title": "Actions",
    "section": "Types of Actions",
    "text": "Types of Actions\nActions can be defined in different ways:\n\nString: A message to be displayed when the threshold level is met or exceeded.\nCallable: A function that is called when the threshold level is met or exceeded.\nList of Strings/Callables: Multiple messages or functions to be called when the threshold level is met or exceeded.\n\nThe actions are executed at interrogation time when the threshold level assigned to the action is exceeded by the number or proportion of failing test units. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of interrogation. If providing a list of strings or callables, each item in the list will be executed in order. Such a list can contain a mix of strings and callables."
  },
  {
    "objectID": "reference/Actions.html#string-templating",
    "href": "reference/Actions.html#string-templating",
    "title": "Actions",
    "section": "String Templating",
    "text": "String Templating\nWhen using a string as an action, you can include placeholders for the following variables:\n\n{type}: The validation step type where the action is executed (e.g., ‘col_vals_gt’, ‘col_vals_lt’, etc.)\n{level}: The threshold level where the action is executed (‘warning’, ‘error’, or ‘critical’)\n{step} or {i}: The step number in the validation workflow where the action is executed\n{col} or {column}: The column name where the action is executed\n{val} or {value}: An associated value for the validation method (e.g., the value to compare against in a ‘col_vals_gt’ validation step)\n{time}: A datetime value for when the action was executed\n\nThe first two placeholders can also be used in uppercase (e.g., {TYPE} or {LEVEL}) and the corresponding values will be displayed in uppercase. The placeholders are replaced with the actual values during interrogation.\nFor example, the string \"{LEVEL}: '{type}' threshold exceeded for column {col}.\" will be displayed as \"WARNING: 'col_vals_gt' threshold exceeded for column a.\" when the ‘warning’ threshold is exceeded in a ‘col_vals_gt’ validation step involving column a."
  },
  {
    "objectID": "reference/Actions.html#examples",
    "href": "reference/Actions.html#examples",
    "title": "Actions",
    "section": "Examples",
    "text": "Examples\nLet’s define both threshold values and actions for a data validation workflow. We’ll set these thresholds and actions globally for all validation steps. In this specific example, the only actions we’ll define are for the ‘critical’ level:\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=\"Major data quality issue found in step {step}.\"),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation\n\nMajor data quality issue found in step 3.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:52:51DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nBecause we set the ‘critical’ action to display \"Major data quality issue found.\" in the console, this message will be displayed if the number of failing test units exceeds the ‘critical’ threshold (set to 15% of the total number of test units). In step 3 of the validation workflow, the ‘critical’ threshold is exceeded, so the message is displayed in the console.\nActions can be defined locally for individual validation steps, which will override any global actions set at the beginning of the validation workflow. Here’s a variation of the above example where we set global threshold values but assign an action only for an individual validation step:\n\ndef dq_issue():\n    from datetime import datetime\n\n    print(f\"Data quality issue found ({datetime.now()}).\")\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n        actions=pb.Actions(warning=dq_issue),\n    )\n    .interrogate()\n)\n\nvalidation\n\nData quality issue found (2025-03-06 23:52:51.333477).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:52:51DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the ‘warning’ action is set to call the dq_issue() function. This action is only executed when the ‘warning’ threshold is exceeded in the ‘session_duration’ column. Because all three thresholds are exceeded in step 3, the ‘warning’ action of executing the function occurs (resulting in a message being printed to the console). If actions were set for the other two threshold levels, they would also be executed."
  },
  {
    "objectID": "reference/preview.html",
    "href": "reference/preview.html",
    "title": "preview",
    "section": "",
    "text": "preview(\n    data,\n    columns_subset=None,\n    n_head=5,\n    n_tail=5,\n    limit=50,\n    show_row_numbers=True,\n    max_col_width=250,\n    incl_header=None,\n)\nDisplay a table preview that shows some rows from the top, some from the bottom.\nTo get a quick look at the data in a table, we can use the preview() function to display a preview of the table. The function shows a subset of the rows from the start and end of the table, with the number of rows from the start and end determined by the n_head= and n_tail= parameters (set to 5 by default). This function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.).\nThe view is optimized for readability, with column names and data types displayed in a compact format. The column widths are sized to fit the column names, dtypes, and column content up to a configurable maximum width of max_col_width= pixels. The table can be scrolled horizontally to view even very large datasets. Since the output is a Great Tables (GT) object, it can be further customized using the great_tables API."
  },
  {
    "objectID": "reference/preview.html#parameters",
    "href": "reference/preview.html#parameters",
    "title": "preview",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to preview, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ncolumns_subset : str | list[str] | Column | None = None\n\nThe columns to display in the table, by default None (all columns are shown). This can be a string, a list of strings, a Column object, or a ColumnSelector object. The latter two options allow for more flexible column selection using column selector functions. Errors are raised if the column names provided don’t match any columns in the table (when provided as a string or list of strings) or if column selector expressions don’t resolve to any columns.\n\nn_head : int = 5\n\nThe number of rows to show from the start of the table. Set to 5 by default.\n\nn_tail : int = 5\n\nThe number of rows to show from the end of the table. Set to 5 by default.\n\nlimit : int | None = 50\n\nThe limit value for the sum of n_head= and n_tail= (the total number of rows shown). If the sum of n_head= and n_tail= exceeds the limit, an error is raised.\n\nshow_row_numbers : bool = True\n\nShould row numbers be shown? The numbers shown reflect the row numbers of the head and tail in the full table.\n\nmax_col_width : int | None = 250\n\nThe maximum width of the columns in pixels. This is 250 (\"250px\") by default.\n\nincl_header : bool = None\n\nShould the table include a header with the table type and table dimensions? Set to True by default."
  },
  {
    "objectID": "reference/preview.html#returns",
    "href": "reference/preview.html#returns",
    "title": "preview",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the preview of the table."
  },
  {
    "objectID": "reference/preview.html#supported-input-table-types",
    "href": "reference/preview.html#supported-input-table-types",
    "title": "preview",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using preview() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/preview.html#examples",
    "href": "reference/preview.html#examples",
    "title": "preview",
    "section": "Examples",
    "text": "Examples\nIt’s easy to preview a table using the preview() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table_polars = pb.load_dataset(\"small_table\")\n\npb.preview(small_table_polars)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThis table is a Polars DataFrame, but the preview() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nsmall_table_duckdb = pb.load_dataset(\"small_table\", tbl_type=\"duckdb\")\n\npb.preview(small_table_duckdb)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows13Columns8\n  \n\n  \n  date_timetimestamp\n  datedate\n  aint64\n  bstring\n  cint64\n  dfloat64\n  eboolean\n  fstring\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    NULL\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    NULL\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThe blue dividing line marks the end of the first n_head= rows and the start of the last n_tail= rows.\nWe can adjust the number of rows shown from the start and end of the table by setting the n_head= and n_tail= parameters. Let’s enlarge each of these to 10:\n\npb.preview(small_table_polars, n_head=10, n_tail=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIn the above case, the entire dataset is shown since the sum of n_head= and n_tail= is greater than the number of rows in the table (which is 13).\nThe columns_subset= parameter can be used to show only specific columns in the table. You can provide a list of column names to make the selection. Let’s try that with the \"game_revenue\" dataset as a Pandas DataFrame:\n\ngame_revenue_pandas = pb.load_dataset(\"game_revenue\", tbl_type=\"pandas\")\n\npb.preview(game_revenue_pandas, columns_subset=[\"player_id\", \"item_name\", \"item_revenue\"])\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns3\n  \n\n  \n  player_idobject\n  item_nameobject\n  item_revenuefloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    offer2\n    8.99\n  \n  \n    2\n    ECPANOIXLZHF896\n    gems3\n    22.49\n  \n  \n    3\n    ECPANOIXLZHF896\n    gold7\n    107.99\n  \n  \n    4\n    ECPANOIXLZHF896\n    ad_20sec\n    0.76\n  \n  \n    5\n    ECPANOIXLZHF896\n    ad_5sec\n    0.03\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    ad_survey\n    1.332\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    ad_survey\n    1.35\n  \n  \n    1998\n    RMOSWHJGELCI675\n    ad_5sec\n    0.03\n  \n  \n    1999\n    RMOSWHJGELCI675\n    offer5\n    26.09\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    ad_5sec\n    0.12\n  \n\n\n\n\n\n\n        \n\n\nAlternatively, we can use column selector functions like starts_with() and matches()` to select columns based on text or patterns:\n\npb.preview(game_revenue_pandas, n_head=2, n_tail=2, columns_subset=pb.starts_with(\"session\"))\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns3\n  \n\n  \n  session_idobject\n  session_startdatetime64[ns, UTC]\n  session_durationfloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    16.3\n  \n  \n    2\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    16.3\n  \n  \n    1999\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    8.4\n  \n  \n    2000\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    18.5\n  \n\n\n\n\n\n\n        \n\n\nMultiple column selector functions can be combined within col() using operators like | and &:\n\npb.preview(\n  game_revenue_pandas,\n  n_head=2,\n  n_tail=2,\n  columns_subset=pb.col(pb.starts_with(\"item\") | pb.matches(\"player\"))\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns4\n  \n\n  \n  player_idobject\n  item_typeobject\n  item_nameobject\n  item_revenuefloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    iap\n    offer2\n    8.99\n  \n  \n    2\n    ECPANOIXLZHF896\n    iap\n    gems3\n    22.49\n  \n  \n    1999\n    RMOSWHJGELCI675\n    iap\n    offer5\n    26.09\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    ad\n    ad_5sec\n    0.12"
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html",
    "href": "reference/Validate.col_vals_expr.html",
    "title": "Validate.col_vals_expr",
    "section": "",
    "text": "Validate.col_vals_expr(\n    expr,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate column values using a custom expression.\nThe col_vals_expr() validation method checks whether column values in a table satisfy a custom expr= expression. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#parameters",
    "href": "reference/Validate.col_vals_expr.html#parameters",
    "title": "Validate.col_vals_expr",
    "section": "Parameters",
    "text": "Parameters\n\nexpr : any\n\nA column expression that will evaluate each row in the table, returning a boolean value per table row. If the target table is a Polars DataFrame, the expression should either be a Polars column expression or a Narwhals one. For a Pandas DataFrame, the expression should either be a lambda expression or a Narwhals column expression.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#returns",
    "href": "reference/Validate.col_vals_expr.html#returns",
    "title": "Validate.col_vals_expr",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#examples",
    "href": "reference/Validate.col_vals_expr.html#examples",
    "title": "Validate.col_vals_expr",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 1, 7, 8, 6],\n        \"b\": [0, 0, 0, 1, 1, 1],\n        \"c\": [0.5, 0.3, 0.8, 1.4, 1.9, 1.2],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cFloat64\n\n\n\n  \n    1\n    1\n    0\n    0.5\n  \n  \n    2\n    2\n    0\n    0.3\n  \n  \n    3\n    1\n    0\n    0.8\n  \n  \n    4\n    7\n    1\n    1.4\n  \n  \n    5\n    8\n    1\n    1.9\n  \n  \n    6\n    6\n    1\n    1.2\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the values in column a are all integers. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_expr(expr=pl.col(\"a\") % 1 == 0)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_expr()\n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_expr(). All test units passed, with no failing test units."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html",
    "href": "reference/Validate.col_vals_le.html",
    "title": "Validate.col_vals_le",
    "section": "",
    "text": "Validate.col_vals_le(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data less than or equal to a fixed value or data in another column?\nThe col_vals_le() validation method checks whether column values in a table are less than or equal to a specified value= (the exact comparison used in this function is col_val &lt;= value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#parameters",
    "href": "reference/Validate.col_vals_le.html#parameters",
    "title": "Validate.col_vals_le",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single numeric value or a single column name given in col(). The latter option allows for a column-column comparison.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#returns",
    "href": "reference/Validate.col_vals_le.html#returns",
    "title": "Validate.col_vals_le",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#examples",
    "href": "reference/Validate.col_vals_le.html#examples",
    "title": "Validate.col_vals_le",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [1, 3, 1, 5, 2, 5],\n        \"c\": [2, 1, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    3\n    1\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    5\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    5\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all less than or equal to the value of 9. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_le(columns=\"a\", value=9)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    a\n    9\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_le(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col()) to perform a column-column comparison. For the next example, we’ll use col_vals_le() to check whether the values in column c are less than values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_le(columns=\"c\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_le()\n        \n    c\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 0: c is 2 and b is 1.\nRow 4: c is 3 and b is 2."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html",
    "href": "reference/Validate.col_vals_not_null.html",
    "title": "Validate.col_vals_not_null",
    "section": "",
    "text": "Validate.col_vals_not_null(\n    columns,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether values in a column are not NULL.\nThe col_vals_not_null() validation method checks whether column values in a table are not NULL. This validation will operate over the number of test units that is equal to the number of rows in the table."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#parameters",
    "href": "reference/Validate.col_vals_not_null.html#parameters",
    "title": "Validate.col_vals_not_null",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nOptional failure threshold levels for the validation step(s), so that the interrogation can react accordingly when exceeding the set levels for different states (‘warning’, ‘error’, and ‘critical’). This can be created using the Thresholds class or more simply as (1) an integer or float denoting the absolute number or fraction of failing test units for the ‘warn’ level, (2) a tuple of 1-3 values, or (3) a dictionary of 1-3 entries.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | None = None\n\nAn optional brief description of the validation step. The templating elements \"{col}\" and \"{step}\" can be used to insert the column name and step number, respectively.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#returns",
    "href": "reference/Validate.col_vals_not_null.html#returns",
    "title": "Validate.col_vals_not_null",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#examples",
    "href": "reference/Validate.col_vals_not_null.html#examples",
    "title": "Validate.col_vals_not_null",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [4, 7, 2, 8],\n        \"b\": [5, None, 1, None],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    4\n    5\n  \n  \n    2\n    7\n    None\n  \n  \n    3\n    2\n    1\n  \n  \n    4\n    8\n    None\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that none of the values in column a are Null values. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_null(columns=\"a\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_not_null(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_null(columns=\"b\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the two Null values in column b."
  },
  {
    "objectID": "reference/first_n.html",
    "href": "reference/first_n.html",
    "title": "first_n",
    "section": "",
    "text": "first_n(n, offset=0)\nSelect the first n columns in the column list.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The first_n() selector function can be used to select n columns positioned at the start of the column list. So if the set of table columns consists of\n[rev_01, rev_02, profit_01, profit_02, age]\nand you want to validate the first two columns, you can use columns=first_n(2). This will select the rev_01 and rev_02 columns and a validation step will be created for each.\nThe offset= parameter can be used to skip a certain number of columns from the start of the column list. So if you want to select the third and fourth columns, you can use columns=first_n(2, offset=2)."
  },
  {
    "objectID": "reference/first_n.html#parameters",
    "href": "reference/first_n.html#parameters",
    "title": "first_n",
    "section": "Parameters",
    "text": "Parameters\n\nn : int\n\nThe number of columns to select from the start of the column list. Should be a positive integer value. If n is greater than the number of columns in the table, all columns will be selected.\n\noffset : int = 0\n\nThe offset from the start of the column list. The default is 0. If offset is greater than the number of columns in the table, no columns will be selected."
  },
  {
    "objectID": "reference/first_n.html#returns",
    "href": "reference/first_n.html#returns",
    "title": "first_n",
    "section": "Returns",
    "text": "Returns\n\n : FirstN\n\nA FirstN object, which can be used to select the first n columns."
  },
  {
    "objectID": "reference/first_n.html#relevant-validation-methods-where-first_n-can-be-used",
    "href": "reference/first_n.html#relevant-validation-methods-where-first_n-can-be-used",
    "title": "first_n",
    "section": "Relevant Validation Methods where first_n() can be Used",
    "text": "Relevant Validation Methods where first_n() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe first_n() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/first_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/first_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "first_n",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe first_n() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names starting with “rev” along with the first two columns, you can use the first_n() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(first_n(2) | starts_with(\"rev\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/first_n.html#examples",
    "href": "reference/first_n.html#examples",
    "title": "first_n",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns paid_2021, paid_2022, paid_2023, paid_2024, and name and we’d like to validate that the values in the first four columns are greater than 10. We can use the first_n() column selector function to specify that the first four columns in the table are the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.first_n(4), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps. The values in all those columns were all greater than 10.\nWe can also use the first_n() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select the first four columns but also omit those columns that end with \"2023\", we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.first_n(4) - pb.ends_with(\"2023\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get three validation steps, one for paid_2021, paid_2022, and paid_2024."
  },
  {
    "objectID": "reference/missing_vals_tbl.html",
    "href": "reference/missing_vals_tbl.html",
    "title": "missing_vals_tbl",
    "section": "",
    "text": "missing_vals_tbl(data)\nDisplay a table that shows the missing values in the input table.\nThe missing_vals_tbl() function generates a table that shows the missing values in the input table. The table is displayed using the Great Tables API, which allows for further customization of the table’s appearance if so desired."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#parameters",
    "href": "reference/missing_vals_tbl.html#parameters",
    "title": "missing_vals_tbl",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to display the missing values. This could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#returns",
    "href": "reference/missing_vals_tbl.html#returns",
    "title": "missing_vals_tbl",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the table of missing values in the input table."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#supported-input-table-types",
    "href": "reference/missing_vals_tbl.html#supported-input-table-types",
    "title": "missing_vals_tbl",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using missing_vals_tbl() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#the-missing-values-table",
    "href": "reference/missing_vals_tbl.html#the-missing-values-table",
    "title": "missing_vals_tbl",
    "section": "The Missing Values Table",
    "text": "The Missing Values Table\nThe missing values table shows the proportion of missing values in each column of the input table. The table is divided into sectors, with each sector representing a range of rows in the table. The proportion of missing values in each sector is calculated for each column. The table is displayed using the Great Tables API, which allows for further customization of the table’s appearance.\nTo ensure that the table can scale to tables with many columns, each row in the reporting table represents a column in the input table. There are 10 sectors shown in the table, where the first sector represents the first 10% of the rows, the second sector represents the next 10% of the rows, and so on. Any sectors that are light blue indicate that there are no missing values in that sector. If there are missing values, the proportion of missing values is shown by a gray color (light gray for low proportions, dark gray to black for very high proportions)."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#examples",
    "href": "reference/missing_vals_tbl.html#examples",
    "title": "missing_vals_tbl",
    "section": "Examples",
    "text": "Examples\nThe missing_vals_tbl() function is useful for quickly identifying columns with missing values in a table. Here’s an example using the nycflights dataset (loaded as a Polars DataFrame using the load_dataset() function):\n\nimport pointblank as pb\n\nnycflights = pb.load_dataset(\"nycflights\", tbl_type=\"polars\")\n\npb.missing_vals_tbl(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values   46,595 in total\n  \n  \n    PolarsRows336,776Columns18\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    year\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    month\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    day\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    carrier\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    flight\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    tailnum\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    origin\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dest\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    air_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    distance\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    hour\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    minute\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 3367733678 – 6735467355 – 101031101032 – 134708134709 – 168385168386 – 202062202063 – 235739235740 – 269416269417 – 303093303094 – 336776\n  \n\n\n\n\n\n\n        \n\n\nThe table shows the proportion of missing values in each column of the nycflights dataset. The table is divided into sectors, with each sector representing a range of rows in the table (with around 34,000 rows per sector). The proportion of missing values in each sector is calculated for each column. The various shades of gray indicate the proportion of missing values in each sector. Many columns have no missing values at all, and those sectors are colored light blue."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html",
    "href": "reference/Validate.get_data_extracts.html",
    "title": "Validate.get_data_extracts",
    "section": "",
    "text": "Validate.get_data_extracts(i=None, frame=False)\nGet the rows that failed for each validation step.\nAfter the interrogate() method has been called, the get_data_extracts() method can be used to extract the rows that failed in each row-based validation step (e.g., col_vals_gt(), etc.). The method returns a dictionary of tables containing the rows that failed in every row-based validation function. If frame=True and i= is a scalar, the value is conveniently returned as a table (forgoing the dictionary structure)."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#parameters",
    "href": "reference/Validate.get_data_extracts.html#parameters",
    "title": "Validate.get_data_extracts",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the failed rows are obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nframe : bool = False\n\nIf True and i= is a scalar, return the value as a DataFrame instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#returns",
    "href": "reference/Validate.get_data_extracts.html#returns",
    "title": "Validate.get_data_extracts",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, FrameT | None] | FrameT | None\n\nA dictionary of tables containing the rows that failed in every row-based validation step or a DataFrame."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#validation-methods-that-are-row-based",
    "href": "reference/Validate.get_data_extracts.html#validation-methods-that-are-row-based",
    "title": "Validate.get_data_extracts",
    "section": "Validation Methods that are Row-Based",
    "text": "Validation Methods that are Row-Based\nThe following validation methods are row-based and will have rows extracted when there are failing test units.\n\ncol_vals_gt()\ncol_vals_ge()\ncol_vals_lt()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\n\nAn extracted row means that a test unit failed for that row in the validation step. The extracted rows are a subset of the original table and are useful for further analysis or for understanding the nature of the failing test units."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#examples",
    "href": "reference/Validate.get_data_extracts.html#examples",
    "title": "Validate.get_data_extracts",
    "section": "Examples",
    "text": "Examples\nLet’s perform a series of validation steps on a Polars DataFrame. We’ll use the col_vals_gt() in the first step, col_vals_lt() in the second step, and col_vals_ge() in the third step. The interrogate() method executes the validation; then, we can extract the rows that failed for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 3, 6, 1],\n        \"b\": [1, 2, 1, 5, 2, 6],\n        \"c\": [3, 7, 2, 6, 3, 1],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=4)\n    .col_vals_lt(columns=\"c\", value=5)\n    .col_vals_ge(columns=\"b\", value=1)\n    .interrogate()\n)\n\nvalidation.get_data_extracts()\n\n{1: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n │ 6         ┆ 1   ┆ 6   ┆ 1   │\n └───────────┴─────┴─────┴─────┘,\n 2: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 2         ┆ 6   ┆ 2   ┆ 7   │\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n └───────────┴─────┴─────┴─────┘,\n 3: shape: (0, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n └───────────┴─────┴─────┴─────┘}\n\n\nThe get_data_extracts() method returns a dictionary of tables, where each table contains a subset of rows from the table. These are the rows that failed for each validation step.\nIn the first step, thecol_vals_gt() method was used to check if the values in column a were greater than 4. The extracted table shows the rows where this condition was not met; look at the a column: all values are less than 4.\nIn the second step, the col_vals_lt() method was used to check if the values in column c were less than 5. In the extracted two-row table, we see that the values in column c are greater than 5.\nThe third step (col_vals_ge()) checked if the values in column b were greater than or equal to 1. There were no failing test units, so the extracted table is empty (i.e., has columns but no rows).\nThe i= argument can be used to narrow down the extraction to one or more steps. For example, to extract the rows that failed in the first step only:\n\nvalidation.get_data_extracts(i=1)\n\n{1: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n │ 6         ┆ 1   ┆ 6   ┆ 1   │\n └───────────┴─────┴─────┴─────┘}\n\n\nNote that the first validation step is indexed at 1 (not 0). This 1-based indexing is in place here to match the step numbers reported in the validation table. What we get back is still a dictionary, but it only contains one table (the one for the first step).\nIf you want to get the extracted table as a DataFrame, set frame=True and provide a scalar value for i. For example, to get the extracted table for the second step as a DataFrame:\n\npb.preview(validation.get_data_extracts(i=2, frame=True))\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    2\n    6\n    2\n    7\n  \n  \n    4\n    3\n    5\n    6\n  \n\n\n\n\n\n\n        \n\n\nThe extracted table is now a DataFrame, which can serve as a more convenient format for further analysis or visualization. We further used the preview() function to show the DataFrame in an HTML view."
  },
  {
    "objectID": "user-guide/index.html",
    "href": "user-guide/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "The Pointblank library is all about assessing the state of data quality for a table. You provide the validation rules and the library will dutifully interrogate the data and provide useful reporting. We can use different types of tables like Polars and Pandas DataFrames, Parquet files, or various database tables. Let’s walk through what data validation looks like in Pointblank.",
    "crumbs": [
      "User Guide",
      "Getting Started"
    ]
  },
  {
    "objectID": "user-guide/index.html#a-simple-validation-table",
    "href": "user-guide/index.html#a-simple-validation-table",
    "title": "Getting Started",
    "section": "A Simple Validation Table",
    "text": "A Simple Validation Table\nThis is a validation report table that is produced from a validation of a Polars DataFrame:\n\n\nCode\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"), label=\"Example Validation\")\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n        \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_between()\n        \n    d\n    [0, 5000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_in_set()\n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    b\n    ^[0-9]-[a-z]{3}-[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nEach row in this reporting table constitutes a single validation step. Roughly, the left-hand side outlines the validation rules and the right-hand side provides the results of each validation step. While simple in principle, there’s a lot of useful information packed into this validation table.\nHere’s a diagram that describes a few of the important parts of the validation table:\n\nThere are three things that should be noted here:\n\nvalidation steps: each step is a separate test on the table, focused on a certain aspect of the table\nvalidation rules: the validation type is provided here along with key constraints\nvalidation results: interrogation results are provided here, with a breakdown of test units (total, passing, and failing), threshold flags, and more\n\nThe intent is to provide the key information in one place, and have it be interpretable by data stakeholders.",
    "crumbs": [
      "User Guide",
      "Getting Started"
    ]
  },
  {
    "objectID": "user-guide/index.html#example-code-step-by-step",
    "href": "user-guide/index.html#example-code-step-by-step",
    "title": "Getting Started",
    "section": "Example Code, Step-by-Step",
    "text": "Example Code, Step-by-Step\nHere’s the code that performs the validation on the Polars table.\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\n\nvalidation\nNote these three key pieces in the code:\n\nthe Validate(data=) argument takes a DataFrame or database table that you want to validate\nthe methods starting with col_vals_ specify validation steps that run on specific columns\nthe interrogate() method executes the validation plan on the table\n\nThis common pattern is used in a validation workflow, where Validate and interrogate() bookend a validation plan generated through calling validation methods. And that’s data validation with Pointblank in a nutshell! In the next section we’ll go a bit further by understanding how we can measure data quality with test units and failure thresholds.",
    "crumbs": [
      "User Guide",
      "Getting Started"
    ]
  },
  {
    "objectID": "user-guide/index.html#understanding-test-units",
    "href": "user-guide/index.html#understanding-test-units",
    "title": "Getting Started",
    "section": "Understanding Test Units",
    "text": "Understanding Test Units\nEach validation step will execute a type of validation test on the target table. For example, a col_vals_lt() validation step can test that each value in a column is less than a specified number. The key finding that’s reported as a result of this test is the number of test units that pass or fail.\nTest units are dependent on the test being run. The collection of col_vals_* validation methods will test each and every value in a particular column, so each value will be a test unit (and the number of test units is the number of rows in the target table). Some validation methods like col_exists() or row_count_match() have only a single test unit since they aren’t testing individual values but rather if the overall test passes or fails.",
    "crumbs": [
      "User Guide",
      "Getting Started"
    ]
  },
  {
    "objectID": "user-guide/index.html#using-threshold-levels",
    "href": "user-guide/index.html#using-threshold-levels",
    "title": "Getting Started",
    "section": "Using Threshold Levels",
    "text": "Using Threshold Levels\nKnowing about the numbers of test units across validation methods matters because you have the option to set thresholds (that can signal ‘warning’, ‘error’, and ‘critical’ flags) based on either the relative proportion or absolute number of failing test units.\nHere’s a simple example that uses a single validation step along with thresholds set in the thresholds= argument of the validation method.\n\nvalidation_2 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_lt(columns=\"a\", value=7, thresholds=(2, 4))\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:53:35Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    a\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe code uses thresholds=(2, 4) to set a ‘warning’ threshold of 2 and an error threshold of 4. If you look at the validation report table, we can see:\n\nthe FAIL column shows that 2 tests units have failed\nthe W column (short for ‘warning’) shows a filled gray circle indicating those failing test units reached that threshold value\nthe E column (short for ‘error’) shows an open yellow circle indicating that the number of failing test units is below that threshold\n\nThe one final threshold level, C (for ‘critical’), wasn’t set so it appears on the validation table as a long dash.\nSetting thresholds is important since you might want some sort of signal for the discovery of errors in your data. How you set the particular threshold levels is highly dependent on your tolerance for data failures. The idea of thresholds and associated actions is central to how Pointblank works, so, the next two sections in the User Guide will deal with (1) a more in-depth treatment of thresholds, and (2) how to set actions for threshold exceedences.",
    "crumbs": [
      "User Guide",
      "Getting Started"
    ]
  },
  {
    "objectID": "user-guide/extracts.html",
    "href": "user-guide/extracts.html",
    "title": "Data Extracts",
    "section": "",
    "text": "Data extracts consist of target table rows containing at least one cell that was found to be a failing test unit. Many of the validation methods check values down a column according to some rule (e.g., values are not null/None, values are greater than zero, etc.). So if any of those test units (which are really cells) failed during a validation step, the row is marked as failing for the purposes of data extract collection. This article will:\nData extracts can be useful after interrogation since they reveal which rows resulted in failures during interrogation. It is hoped that having quick access to entire rows of data with failing elements can be useful in uncovering the root causes of data quality issues.",
    "crumbs": [
      "User Guide",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#the-validation-methods-that-work-with-data-extracts",
    "href": "user-guide/extracts.html#the-validation-methods-that-work-with-data-extracts",
    "title": "Data Extracts",
    "section": "The Validation Methods that Work with Data Extracts",
    "text": "The Validation Methods that Work with Data Extracts\nThe following validation methods are row-based and will have rows extracted when there are failing test units:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\n\nAn extracted row means that a test unit failed for that row in the validation step. The extracted rows are a subset of the original table and are useful for further analysis or understanding the nature of the failing test units.",
    "crumbs": [
      "User Guide",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#data-extracts-as-csv-data-in-the-validation-report",
    "href": "user-guide/extracts.html#data-extracts-as-csv-data-in-the-validation-report",
    "title": "Data Extracts",
    "section": "Data Extracts as CSV Data in the Validation Report",
    "text": "Data Extracts as CSV Data in the Validation Report\nData extracts are embedded within validation report tables. Let’s look at an example, using the small_table dataset, where data extracts are collected in a single validation step due to failing test units:\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_lt(columns=\"d\", value=3000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_lt()\n        \n    d\n    3000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe single validation step checks whether values in d are less than 3000. Within that column values range from 108.34 to 9999.99, so it makes sense that we can see 4 failing test units in the FAIL column.\nIf you look at the far right of the validation report you’ll find there’s a CSV button. Pressing it initiates the download of a CSV, and that CSV contains the data extract for this validation step. The CSV button only appears when:\n\nthere is a non-zero number of failing test units\nthe validation step is based on the use of a row-based validation method (the methods outlined above)\n\nAccess to CSV data for the row-based errors is useful when the validation report is shared with other data quality stakeholders, since it is easily accessible and doesn’t require futher use of Pointblank.",
    "crumbs": [
      "User Guide",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#using-the-get_data_extracts-method-to-collect",
    "href": "user-guide/extracts.html#using-the-get_data_extracts-method-to-collect",
    "title": "Data Extracts",
    "section": "Using the get_data_extracts() Method to Collect",
    "text": "Using the get_data_extracts() Method to Collect\nAside from the low-tech CSV buttons in validation report tables, we can more directly pull out the data extracts from the validation object created above. We do that with the get_data_extracts() method, supplying the step number (1) to the i= parameter:\n\nextract_1 = validation.get_data_extracts(i=1, frame=True)\n\nextract_1\n\n\nshape: (4, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr12016-01-04 11:00:002016-01-042\"1-bcd-345\"33423.29true\"high\"22016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"42016-01-06 17:23:002016-01-062\"5-jdo-903\"null3892.4false\"mid\"62016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"\n\n\nThe extracted table is of the same type (a Polars DataFrame) as the target table. We used load_dataset() with the tbl_type=\"polars\" option to fetch the dataset in that form.\nNotice that the frame=True option was used above. What this does is return the table itself as normally the return type is a dictionary. This only works if what’s provided to i= is a scalar integer (which is the case here).\nAlso notice that within the DataFrame returned, we get all the columns of the original dataset (i.e., not just the column being checked in the validation step) plus an additional column: _row_num_. That column provides the 1-indexed row numbers from the original dataset. The combination of rows in their entirety plus row numbers is to provide more context on where data failures occurred.",
    "crumbs": [
      "User Guide",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#viewing-data-extracts-with-preview",
    "href": "user-guide/extracts.html#viewing-data-extracts-with-preview",
    "title": "Data Extracts",
    "section": "Viewing Data Extracts with preview()",
    "text": "Viewing Data Extracts with preview()\nTo get a consistent HTML representation of any data extract (regardless of the table type), we can use the preview() function:\n\npb.preview(data=extract_1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows4Columns9\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n\n\n\n\n\n\n        \n\n\nThe view is optimized for readability, with column names and data types displayed in a compact format. Notice that the _row_num_ column is now part of the table stub and doesn’t steal focus from the table’s original columns.\nThe preview() function is designed to provide the head and tail (5 rows each) of the table so very large extracts won’t overflow the display.",
    "crumbs": [
      "User Guide",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/actions.html",
    "href": "user-guide/actions.html",
    "title": "Actions",
    "section": "",
    "text": "Actions are meant to be combined with thresholds and they allow you easily write text to the console or execute custom functions. As an example, when testing a column for values that should always be greater than 2 you might want some text emitted to the console when any failing test units are found. To do that, you need to pair a threshold level with an associated action (and that action could take the form of a console message).",
    "crumbs": [
      "User Guide",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#how-actions-work",
    "href": "user-guide/actions.html#how-actions-work",
    "title": "Actions",
    "section": "How Actions Work",
    "text": "How Actions Work\nLet’s look at an example on how this works in practice. The following validation plan contains a single step (using col_vals_gt()) where the thresholds= and actions= parameters are set:\n\nimport pointblank as pb\n\nvalidation_1 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_gt(\n        columns=\"c\",\n        value=2,\n        thresholds=pb.Thresholds(warning=1, error=5),\n        actions=pb.Actions(warning=\"WARNING: failing test found.\")\n    )\n    .interrogate()\n)\n\nvalidation_1\n\nWARNING: failing test found.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:53:47Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    c\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    100.77\n    30.23\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe code uses thresholds=pb.Thresholds(warning=1, error=5) to set a ‘warning’ threshold of 1 and an ‘error’ threshold of 5 failing test units. The results part of the validation table shows that:\n\nThe FAIL column shows that 3 tests units have failed\nThe W column (short for ‘warning’) shows a filled gray circle indicating it’s reached its threshold level\nThe E (‘error’) column shows an open yellow circle indicating it’s below the threshold level\n\nMore importantly, the text \"WARNING: failing test found.\" has been emitted. Here it appears above the validation table and that’s because the action is executed eagerly during interrogation (before the report has even been generated).\nSo, an action is executed for a particular condition (e.g., ‘warning’) within a validation step if these three things are true:\n\nthere is a threshold set for that condition (either globally, or as part of that step)\nthere is an associated action set for the condition (again, either set globally or within the step)\nduring interrogation, the threshold value for the condition was exceeded by the number or proportion of failing test units\n\nThere is a lot of flexibility for setting both thresholds and actions and everything here is considered optional. Put another way, you can set various thresholds and various actions as needed and the interrogation phase will determine whether all the requirements are met for executing an action.",
    "crumbs": [
      "User Guide",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#ways-to-express-actions-text-or-custom-functions",
    "href": "user-guide/actions.html#ways-to-express-actions-text-or-custom-functions",
    "title": "Actions",
    "section": "Ways to Express Actions: Text or Custom Functions",
    "text": "Ways to Express Actions: Text or Custom Functions\nThere are a few options in how to define the actions:\n\nString: A message to be displayed in the console.\nCallable: A function to be called (which could do virtually anything).\nList of Strings/Callables: For execution of multiple messages or functions.\n\nThe actions are executed at interrogation time when the threshold level assigned to the action is exceeded by the number or proportion of failing test units. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of interrogation. If providing a list of strings or callables, each item in the list will be executed in order. Such a list can contain a mix of strings and callables.\nHere’s an example where we use a custom function as part of an action:\n\ndef dq_issue():\n    from datetime import datetime\n\n    print(f\"Data quality issue found ({datetime.now()}).\")\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n        actions=pb.Actions(warning=dq_issue),\n    )\n    .interrogate()\n)\n\nvalidation\n\nData quality issue found (2025-03-06 23:53:48.009395).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-03-06|23:53:47DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n         col_vals_regex()\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the ‘warning’ action is set to call the user’s dq_issue() function. This action is only executed when the ‘warning’ threshold is exceeded in step 3. Because all three thresholds are exceeded in that step, the ‘warning’ action of executing the function occurs (resulting in a message being printed to the console).\nThis is an example where actions can be defined locally for an individual validation step. The global threshold setting applied to all three validation steps but the step-level action only applied to step 3. You are free to mix and match both threshold and action settings at the global level (i.e., set in the Validate call) or at the step level. The key thing to be aware of is that step-level settings of thresholds and actions take precedence.",
    "crumbs": [
      "User Guide",
      "Actions"
    ]
  },
  {
    "objectID": "user-guide/columns.html",
    "href": "user-guide/columns.html",
    "title": "Defining Columns",
    "section": "",
    "text": "Most of the validation methods included in Pointblank perform column-level checks. As such, they provide the common argument columns=. The pluralization in the name indicates that multiple columns can be provided. And it goes further than that, as column selectors can be used in columns= to resolve columns.\nWhy do this? It can often be the case that you’d want to perform a validation check of a certain common type (e.g., checking that numerical values are all positive) across a number of columns. Rather than define the same rules across multiple invocations of the same validation method (one for each column), we can simply map the validation rules across those columns.\nWe’ll run through several examples below, and all of them will use the game_revenue dataset. Here’s a preview of it:\nPolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\nLet’s start with the simpler case of providing a list of columns to columns= before getting into the resolving columns with column selector functions.",
    "crumbs": [
      "User Guide",
      "Defining Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#using-a-list-of-column-names",
    "href": "user-guide/columns.html#using-a-list-of-column-names",
    "title": "Defining Columns",
    "section": "Using a List of Column Names",
    "text": "Using a List of Column Names\nThe columns= parameter (in every validation method that has that argument) can accept a list of column names. In the game_revenue dataset, there are two columns that contain numerical data: item_revenue and session_duration. Suppose we expect that data in both columns should be greater than 0, providing a list of those two columns will create two validation steps from a single invocation of col_vals_gt():\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(columns=[\"item_revenue\", \"session_duration\"], value=0)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation report table indeed shows that two validation steps were created! Further to this, the interrogation results show that all values in the two columns are greater than 0.\nIt’s important to note that all validation parameters are shared across all generated steps. So if thresholds= were to be set, those threshold values would be cloned and used in each and every step generated (one step per column provided):\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(\n        columns=[\"item_revenue\", \"session_duration\"],\n        value=0,\n        thresholds=(0.1, 0.2, 0.3)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis is all to say that if you wanted the same validation rules but different threshold settings, you would have to define two individual validation steps with only the thresholds= values differing.",
    "crumbs": [
      "User Guide",
      "Defining Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#using-pointblanks-column-selectors",
    "href": "user-guide/columns.html#using-pointblanks-column-selectors",
    "title": "Defining Columns",
    "section": "Using Pointblank’s Column Selectors",
    "text": "Using Pointblank’s Column Selectors\nPointblank includes a few column selector functions for use in columns=. If you’re new to selectors what they do is resolve column names typically based on:\n\ntext patterns\ncolumn position\ncolumn type\n\nTwo common ones, starts_with() and ends_with(), resolve columns based on starting and ending text in column names.\nThe game_revenue dataset has three columns starting with the text ‘item’: item_type, item_name, and item_revenue. Let’s look at an example where we can succinctly express a validation plan checking that these columns contain no missing values:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.starts_with(\"item\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nAs can be seen, three validation steps were created from the use of columns=pb.starts_with(\"item\") because those three columns were found in the table.\nThe complete list of column selectors includes:\n\nstarts_with()\nends_with()\ncontains()\nmatches()\neverything()\nfirst_n()\nlast_n()\n\nand in the next section, we’ll learn how they could actually be combined in interesting ways for more advanced column selection tasks.",
    "crumbs": [
      "User Guide",
      "Defining Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#combining-column-selectors",
    "href": "user-guide/columns.html#combining-column-selectors",
    "title": "Defining Columns",
    "section": "Combining Column Selectors",
    "text": "Combining Column Selectors\nColumn selector functions can be combined for extra column selection power. We need to use two things to make this possible: (1) the col() helper function, and (2) one or more logical operators for composing column selectors. Those operators are:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nTaken together, if you wanted to select all columns except the first four, you would use this col() expression:\n\ncol_selection = pb.col(pb.everything() - pb.first_n(4))\n\nThe logic here is that everything() selects every column and the - pb.first_n(4) part then removes the first four columns from that selection.\nLet’s try using the col_selection object in a validation of the game_revenue table, checking multiple columns for missing values:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=col_selection, thresholds=(1, 0.05, 0.1))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    session_duration\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    acquisition\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    7\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    country\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis column selection worked to select every column except for the first four, resulting in seven separate validation steps.",
    "crumbs": [
      "User Guide",
      "Defining Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#even-more-selectors-narwhals-selectors",
    "href": "user-guide/columns.html#even-more-selectors-narwhals-selectors",
    "title": "Defining Columns",
    "section": "Even More Selectors: Narwhals Selectors",
    "text": "Even More Selectors: Narwhals Selectors\nWhile Pointblank offers a good selection of selectors, we built in support for the column selectors available in the Narwhals library. If you don’t know Narwhals yet, it’s a lightweight compatibility layer between dataframe libraries. The library is so good that we use it under the hood in Pointblank so that you can easily validate Pandas, Polars, PyArrow, cuDF, and Modin tables. The narwhals.selectors module contains the following column selector functions:\n\nmatches()\nby_dtype()\nboolean()\ncategorical()\ndatetime()\nnumeric()\nstring()\n\nWe support the use of these selectors for any input DataFrame that is supported by Narwhals (i.e., is in the aforementioned list). Here’s an example using the numeric() selector to select all numeric columns in game_revenue:\n\nimport narwhals.selectors as ncs\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(columns=ncs.numeric(), value=0)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n         col_vals_gt()\n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nNarwhals selectors can also use operators to combine multiple selectors. Here is an example of using the string() and matches() selectors together to select all string columns matching the text ‘item_’:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.col(ncs.string() & ncs.matches(\"item_\")))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe combination of Narwhals column selectors as used in the above serves to constrain the set of columns resolved. And the same set of logical operators (&, |, -, ~) can be used to compose Narwhals selectors within a col() expression.",
    "crumbs": [
      "User Guide",
      "Defining Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#caveats-for-using-column-selectors",
    "href": "user-guide/columns.html#caveats-for-using-column-selectors",
    "title": "Defining Columns",
    "section": "Caveats for Using Column Selectors",
    "text": "Caveats for Using Column Selectors\nProvided there is systematic column naming already in place, using column selectors like starts_with() can be very convenient. This is especially true as column counts become larger.\nA slight disadvantage to this approach is some uncertainty on whether those columns being checked actually exist. You might resolve fewer columns than anticipated or no columns at all due to errors in using the column selectors or through misunderstanding in the columns’ naming conventions.\nShould the use of a column selector yield no columns the interrogation process itself won’t fail, however, the validation report table will strongly signal that there was an evaluation issue:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.starts_with(\"items\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n        \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n         col_vals_not_null()\n        \n    StartsWith(text='items', case_sensitive=False)\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    💥\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nAside from the validation step being tinted in red, the EVAL column will display an explosion (and there won’t be any results). In practice, you would either correct the string supplied to starts_with() or take a different approach.\nGiven the slight bit of uncertainty you get when using column selectors (rather than the explicit use of column names), it’s good to also include validation steps that check for the existence of key column names with col_exists() (and checking the table schema itself with col_schema_match() is also worthwhile here).",
    "crumbs": [
      "User Guide",
      "Defining Columns"
    ]
  }
]