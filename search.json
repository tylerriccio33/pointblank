[
  {
    "objectID": "user-guide/preprocessing.html",
    "href": "user-guide/preprocessing.html",
    "title": "Transforming the Target Table",
    "section": "",
    "text": "While the validation methods available can do a lot for you, there’s likewise a lot of things you can’t easily do with them. What if you wanted to validate:\nThese are more complicated types of validations, yet checks of this type are very commonplace. We don’t need to have a very large library of validation methods to tackle each an every case; the number of combinations indeed seems exceedingly large. Instead, let’s transform the table we are validating through a preprocessing step and expose the key values. In conjunction with that sort of table transformation, we then can use the existing library of validation methods.\nCentral to this approach is the idea of composability. Pointblank makes it easy to safely transform the target table for a given validation via the pre= argument. Any computed columns are available for the (short) lifetime of the validation step during interrogation. This sort of composibility means: (1) we can validate on different forms of the initial dataset (e.g., validating on aggregate forms, validating on calculated columns, etc.), (2) there’s no need to start an entirely new validation process for each transformed version of the data (i.e., one tabular report could be produced instead of several).\nNow, through a series of examples, let’s look at the process of performing the validations mentioned above. We’ll use the small_table dataset for all of the examples. Here it is in its entirety:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Transforming the Target Table"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#the-basics-of-preprocessing-the-target-table",
    "href": "user-guide/preprocessing.html#the-basics-of-preprocessing-the-target-table",
    "title": "Transforming the Target Table",
    "section": "The Basics of Preprocessing the Target Table",
    "text": "The Basics of Preprocessing the Target Table\nIn getting to grips with the basics, we’ll try to validate that string lengths in the b column are less than 10 characters. We can’t directly use the col_vals_lt() validation method with that column because it is meant to be used with a column of numeric values. Let’s just give that method what it needs and create a column with string lengths! The target table is a Polars DataFrame so we’ll provide a lambda function that uses the Polars API to add in that numeric column:\n\nimport polars as pl\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"String lengths\"\n    )\n    .col_vals_lt(\n        columns=\"string_lengths\",  # the generated column through `pre=`\n        value=10,                  # the string length value to be less than\n        pre=lambda df: df.with_columns(string_lengths=pl.col(\"b\").str.len_chars())\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    String lengthsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    string_lengths\n    10\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation was successfully constructed and we can see from the validation report table that all strings in b had lengths less than 10 characters. Also note that the icon under the TBL column is no longer a rightward-facing arrow, but one that is indicative of a transformation taking place.\nNow look at the code itself. Notice that we’re not directly testing the b column. Instead the test is of the string_lengths column generated by the lambda provided to pre=. We used Polars to do the transformation work here (via its with_columns() method) and that’s the piece that generates numerical values of string lengths in the computed column.\nThat transformation occurs only during interrogation and only for that validation step. Any prior or subsequent steps would normally use the as-provided small_table. Having the possibility for data transformation being isolated at the step level means that you don’t have to generate separate validation plans for each form of the data, you’re free to fluidly transform the target table as necessary for perform validations on different representations of the data.\nAside from using a lambda, you can pass in a custom function. Just make sure not to evaluate it at the pre= parameter (everything is stored lazily until interrogation time). Here’s an example of that approach:\n\ndef add_string_lengths(df):\n    return df.with_columns(string_lengths=pl.col(\"b\").str.len_chars())\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"String lengths\"\n    )\n    .col_vals_lt(columns=pb.last_n(1), value=10, pre=add_string_lengths)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    String lengthsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    string_lengths\n    10\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe column-generating logic was placed in the add_string_lengths() function, which is then passed to pre=. We also know that the column generated will be the final one in the column series, so the last_n() column selector obviates the need to provide \"string_lengths\" here (you’ll still find the target column name echoed on the validation report table).",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Transforming the Target Table"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#using-narwhals-to-preprocess-many-types-of-dataframes",
    "href": "user-guide/preprocessing.html#using-narwhals-to-preprocess-many-types-of-dataframes",
    "title": "Transforming the Target Table",
    "section": "Using Narwhals to Preprocess Many Types of DataFrames",
    "text": "Using Narwhals to Preprocess Many Types of DataFrames\nIn this previous example we used a Polars table (the load_dataset() returns a Polars DataFrame by default). You might have a situation where where you perform data validation variously on Pandas and Polars DataFrames. This is where Narwhals becomes handy.\nLet’s obtain small_table as a Pandas DataFrame. We’ll construct a validation step to verify that the median of column c is greater than the median in column a.\n\nimport narwhals as nw\n\n(\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Median comparison\",\n    )\n    .col_vals_gt(\n        columns=\"c\",\n        value=pb.col(\"a\"),\n        pre=lambda df: nw.from_native(df).select(nw.median(\"c\"), nw.median(\"a\"))\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Median comparisonPandassmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    a\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThere’s a bit to unpack here so let’s look at at the lambda function first. Narwhals can translate a Pandas DataFrame to a Narwhals DataFrame with its from_native() function. After that initiating step, you’re free to use the Narwhals API (which is modeled on a subset of the Polars API) to do the necessary data transformation. In this case, we are getting the medians of the c and a columns and ending up with a one-row, two-column table.\nThe goal is to check that the median value of c is greater than the corresponding median of column a, so we set up columns= and value= parameters in that way within the col_vals_gt() validation method call.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Transforming the Target Table"
    ]
  },
  {
    "objectID": "user-guide/preprocessing.html#swapping-in-a-totally-different-dataframe",
    "href": "user-guide/preprocessing.html#swapping-in-a-totally-different-dataframe",
    "title": "Transforming the Target Table",
    "section": "Swapping in a Totally Different DataFrame",
    "text": "Swapping in a Totally Different DataFrame\nLet’s now try to prepare the final validation scenario, checking that there are at least three instances of every categorical value in column f (which contains string values in the set of \"low\", \"mid\", and \"high\"). This time, we’ll prepare the transformed table (transformed by Polars expressions) outside of the Pointblank code.\n\ndata_original = pb.load_dataset(\"small_table\")\ndata_transformed = data_original.group_by(\"f\").len(name=\"n\")\n\ndata_transformed\n\n\nshape: (3, 2)fnstru32\"mid\"2\"low\"5\"high\"6\n\n\nThen, we’ll plug in the data_transformed DataFrame with a lambda expression in pre=:\n\ndata_original = pb.load_dataset(\"small_table\")\ndata_transformed = data_original.group_by(\"f\").len(name=\"n\")\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"Category counts\",\n    )\n    .col_vals_ge(columns=\"n\", value=3, pre=lambda x: data_transformed)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Category countsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    n\n    3\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    20.67\n    10.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWe can see from the validation report table that there are three test units. This corresponds to a row for each of the categorical value counts. From the report, we find that two of the three test units are passing test units (turns out there are only two instances of \"mid\" in column f).",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Transforming the Target Table"
    ]
  },
  {
    "objectID": "user-guide/columns.html",
    "href": "user-guide/columns.html",
    "title": "Applying Rules Across Columns",
    "section": "",
    "text": "Most of the validation methods included in Pointblank perform column-level checks. As such, they provide the common argument columns=. The pluralization in the name indicates that multiple columns can be provided. And it goes further than that, as column selectors can be used in columns= to resolve columns.\nWhy do this? It can often be the case that you’d want to perform a validation check of a certain common type (e.g., checking that numerical values are all positive) across a number of columns. Rather than define the same rules across multiple invocations of the same validation method (one for each column), we can simply map the validation rules across those columns.\nWe’ll run through several examples below, and all of them will use the game_revenue dataset. Here’s a preview of it:\nPolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\nLet’s start with the simpler case of providing a list of columns to columns= before getting into the resolving columns with column selector functions.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Applying Rules Across Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#using-a-list-of-column-names",
    "href": "user-guide/columns.html#using-a-list-of-column-names",
    "title": "Applying Rules Across Columns",
    "section": "Using a List of Column Names",
    "text": "Using a List of Column Names\nThe columns= parameter (in every validation method that has that argument) can accept a list of column names. In the game_revenue dataset, there are two columns that contain numerical data: item_revenue and session_duration. Suppose we expect that data in both columns should be greater than 0, providing a list of those two columns will create two validation steps from a single invocation of col_vals_gt():\n\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(columns=[\"item_revenue\", \"session_duration\"], value=0)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation report table indeed shows that two validation steps were created! Further to this, the interrogation results show that all values in the two columns are greater than 0.\nIt’s important to note that all validation parameters are shared across all generated steps. So if thresholds= were to be set, those threshold values would be cloned and used in each and every step generated (one step per column provided):\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(\n        columns=[\"item_revenue\", \"session_duration\"],\n        value=0,\n        thresholds=(0.1, 0.2, 0.3)\n    )\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis is all to say that if you wanted the same validation rules but different threshold settings, you would have to define two individual validation steps with only the thresholds= values differing.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Applying Rules Across Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#using-pointblanks-column-selectors",
    "href": "user-guide/columns.html#using-pointblanks-column-selectors",
    "title": "Applying Rules Across Columns",
    "section": "Using Pointblank’s Column Selectors",
    "text": "Using Pointblank’s Column Selectors\nPointblank includes a few column selector functions for use in columns=. If you’re new to selectors what they do is resolve column names typically based on:\n\ntext patterns\ncolumn position\ncolumn type\n\nTwo common ones, starts_with() and ends_with(), resolve columns based on starting and ending text in column names.\nThe game_revenue dataset has three columns starting with the text ‘item’: item_type, item_name, and item_revenue. Let’s look at an example where we can succinctly express a validation plan checking that these columns contain no missing values:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.starts_with(\"item\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nAs can be seen, three validation steps were created from the use of columns=pb.starts_with(\"item\") because those three columns were found in the table.\nThe complete list of column selectors includes:\n\nstarts_with()\nends_with()\ncontains()\nmatches()\neverything()\nfirst_n()\nlast_n()\n\nand in the next section, we’ll learn how they could actually be combined in interesting ways for more advanced column selection tasks.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Applying Rules Across Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#combining-column-selectors",
    "href": "user-guide/columns.html#combining-column-selectors",
    "title": "Applying Rules Across Columns",
    "section": "Combining Column Selectors",
    "text": "Combining Column Selectors\nColumn selector functions can be combined for extra column selection power. We need to use two things to make this possible: (1) the col() helper function, and (2) one or more logical operators for composing column selectors. Those operators are:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nTaken together, if you wanted to select all columns except the first four, you would use this col() expression:\n\ncol_selection = pb.col(pb.everything() - pb.first_n(4))\n\nThe logic here is that everything() selects every column and the - pb.first_n(4) part then removes the first four columns from that selection.\nLet’s try using the col_selection object in a validation of the game_revenue table, checking multiple columns for missing values:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=col_selection, thresholds=(1, 0.05, 0.1))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    session_duration\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    acquisition\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    country\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis column selection worked to select every column except for the first four, resulting in seven separate validation steps.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Applying Rules Across Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#even-more-selectors-narwhals-selectors",
    "href": "user-guide/columns.html#even-more-selectors-narwhals-selectors",
    "title": "Applying Rules Across Columns",
    "section": "Even More Selectors: Narwhals Selectors",
    "text": "Even More Selectors: Narwhals Selectors\nWhile Pointblank offers a good selection of selectors, we built in support for the column selectors available in the Narwhals library. If you don’t know Narwhals yet, it’s a lightweight compatibility layer between dataframe libraries. The library is so good that we use it under the hood in Pointblank so that you can easily validate Pandas, Polars, PyArrow, cuDF, and Modin tables. The narwhals.selectors module contains the following column selector functions:\n\nmatches()\nby_dtype()\nboolean()\ncategorical()\ndatetime()\nnumeric()\nstring()\n\nWe support the use of these selectors for any input DataFrame that is supported by Narwhals (i.e., is in the aforementioned list). Here’s an example using the numeric() selector to select all numeric columns in game_revenue:\n\nimport narwhals.selectors as ncs\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_gt(columns=ncs.numeric(), value=0)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nNarwhals selectors can also use operators to combine multiple selectors. Here is an example of using the string() and matches() selectors together to select all string columns matching the text ‘item_’:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.col(ncs.string() & ncs.matches(\"item_\")))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe combination of Narwhals column selectors as used in the above serves to constrain the set of columns resolved. And the same set of logical operators (&, |, -, ~) can be used to compose Narwhals selectors within a col() expression.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Applying Rules Across Columns"
    ]
  },
  {
    "objectID": "user-guide/columns.html#caveats-for-using-column-selectors",
    "href": "user-guide/columns.html#caveats-for-using-column-selectors",
    "title": "Applying Rules Across Columns",
    "section": "Caveats for Using Column Selectors",
    "text": "Caveats for Using Column Selectors\nProvided there is systematic column naming already in place, using column selectors like starts_with() can be very convenient. This is especially true as column counts become larger.\nA slight disadvantage to this approach is some uncertainty on whether those columns being checked actually exist. You might resolve fewer columns than anticipated or no columns at all due to errors in using the column selectors or through misunderstanding in the columns’ naming conventions.\nShould the use of a column selector yield no columns the interrogation process itself won’t fail, however, the validation report table will strongly signal that there was an evaluation issue:\n\n(\n    pb.Validate(data=pb.load_dataset(\"game_revenue\"))\n    .col_vals_not_null(columns=pb.starts_with(\"items\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    StartsWith(text='items', case_sensitive=False)\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    💥\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nAside from the validation step being tinted in red, the EVAL column will display an explosion (and there won’t be any results). In practice, you would either correct the string supplied to starts_with() or take a different approach.\nGiven the slight bit of uncertainty you get when using column selectors (rather than the explicit use of column names), it’s good to also include validation steps that check for the existence of key column names with col_exists() (and checking the table schema itself with col_schema_match() is also worthwhile here).",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Applying Rules Across Columns"
    ]
  },
  {
    "objectID": "user-guide/actions.html",
    "href": "user-guide/actions.html",
    "title": "Triggering Actions",
    "section": "",
    "text": "Actions are meant to be combined with thresholds and they allow you easily write text to the console or execute custom functions. As an example, when testing a column for values that should always be greater than 2 you might want some text emitted to the console when any failing test units are found. To do that, you need to pair a threshold level with an associated action (and that action could take the form of a console message).",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Triggering Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#how-actions-work",
    "href": "user-guide/actions.html#how-actions-work",
    "title": "Triggering Actions",
    "section": "How Actions Work",
    "text": "How Actions Work\nLet’s look at an example on how this works in practice. The following validation plan contains a single step (using col_vals_gt()) where the thresholds= and actions= parameters are set using Thresholds and Actions calls:\n\nimport pointblank as pb\n\nvalidation_1 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_gt(\n        columns=\"c\",\n        value=2,\n        thresholds=pb.Thresholds(warning=1, error=5),\n        actions=pb.Actions(warning=\"WARNING: failing test found.\")\n    )\n    .interrogate()\n)\n\nvalidation_1\n\nWARNING: failing test found.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:01:18Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    100.77\n    30.23\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe code uses thresholds=pb.Thresholds(warning=1, error=5) to set a ‘warning’ threshold of 1 and an ‘error’ threshold of 5 failing test units. The results part of the validation table shows that:\n\nThe FAIL column shows that 3 tests units have failed\nThe W column (short for ‘warning’) shows a filled gray circle indicating it’s reached its threshold level\nThe E (‘error’) column shows an open yellow circle indicating it’s below the threshold level\n\nMore importantly, the text \"WARNING: failing test found.\" has been emitted. Here it appears above the validation table and that’s because the action is executed eagerly during interrogation (before the report has even been generated).\nSo, an action is executed for a particular condition (e.g., ‘warning’) within a validation step if these three things are true:\n\nthere is a threshold set for that condition (either globally, or as part of that step)\nthere is an associated action set for the condition (again, either set globally or within the step)\nduring interrogation, the threshold value for the condition was exceeded by the number or proportion of failing test units\n\nThere is a lot of flexibility for setting both thresholds and actions and everything here is considered optional. Put another way, you can set various thresholds and various actions as needed and the interrogation phase will determine whether all the requirements are met for executing an action.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Triggering Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#expressing-actions-with-a-string",
    "href": "user-guide/actions.html#expressing-actions-with-a-string",
    "title": "Triggering Actions",
    "section": "Expressing Actions with a String",
    "text": "Expressing Actions with a String\nThere are a few options in how to define the actions:\n\nString: a message to be displayed in the console\nCallable: a function to be called\nList of Strings/Callables: for execution of multiple messages or functions\n\nThe actions are executed at interrogation time when the threshold level assigned to the action is exceeded by the number or proportion of failing test units. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of interrogation. If providing a list of strings or callables, each item in the list will be executed in order. Such a list can contain a mix of strings and callables.\nDisplaying console messages may be a simple approach, but it is effective. And the strings don’t have to be static, there are templating features that can be useful for constructing strings for a variety of situations. The following placeholders are available for use:\n\n{type}: The validation step type where the action is executed (e.g., ‘col_vals_gt’, etc.)\n{level}: The threshold level where the action is executed (‘warning’, ‘error’, or ‘critical’)\n{step} or {i}: The step number in the validation workflow where the action is executed\n{col} or {column}: The column name where the action is executed\n{val} or {value}: An associated value for the validation method\n{time}: A datetime value for when the action was executed\n\nHere’s an example where we prepare a console message with a number of value placeholders (action_str) and use it globally at Actions(critical=):\n\naction_str = \"[{LEVEL}: {TYPE}]: Step {step} has failed validation. ({time})\"\n\nvalidation_2 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=action_str),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.10)\n    .col_vals_ge(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation_2\n\n[CRITICAL: COL_VALS_GT]: Step 2 has failed validation. (2025-04-23 01:01:18.962109+00:00)\n[CRITICAL: COL_VALS_GE]: Step 3 has failed validation. (2025-04-23 01:01:18.985277+00:00)\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:01:18DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    14400.72\n    5600.28\n    ●\n    ●\n    ●\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16860.84\n    3140.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nWhat we get here are two messages in the console, corresponding to critical failures in steps 2 and 3. The placeholders were replaced with the correct text for the context. Note that some of the resulting text is capitalized (e.g., \"CRITICAL\", \"COL_VALS_GT\", etc.) and this is because we capitalized the placeholder text itself. Have a look at the documentation article of Actions for more details on this.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Triggering Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#making-actions-with-callables",
    "href": "user-guide/actions.html#making-actions-with-callables",
    "title": "Triggering Actions",
    "section": "Making Actions with Callables",
    "text": "Making Actions with Callables\nAside from strings, any callable can be used as an action value. Here’s an example where we use a custom function as part of an action:\n\ndef duration_issue():\n    from datetime import datetime\n\n    print(f\"Data quality issue found ({datetime.now()}).\")\n\n\nvalidation_3 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n        actions=pb.Actions(warning=duration_issue),\n    )\n    .interrogate()\n)\n\nvalidation_3\n\nData quality issue found (2025-04-23 01:01:19.332953).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:01:19DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the ‘warning’ action is set to call the user’s dq_issue() function. This action is only executed when the ‘warning’ threshold is exceeded in step 3. Because all three thresholds are exceeded in that step, the ‘warning’ action of executing the function occurs (resulting in a message being printed to the console).\nThis is an example where actions can be defined locally for an individual validation step. The global threshold setting applied to all three validation steps but the step-level action only applied to step 3. You are free to mix and match both threshold and action settings at the global level (i.e., set in the Validate call) or at the step level. The key thing to be aware of is that step-level settings of thresholds and actions take precedence.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Triggering Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#using-get_action_metadata-to-access-metadata-when-building-an-action-callable",
    "href": "user-guide/actions.html#using-get_action_metadata-to-access-metadata-when-building-an-action-callable",
    "title": "Triggering Actions",
    "section": "Using get_action_metadata() to Access Metadata When Building an Action Callable",
    "text": "Using get_action_metadata() to Access Metadata When Building an Action Callable\nTo access information about the validation step where an action was triggered, we can call get_action_metadata() in the body of a function to be used within Actions. The dictionary that’s returned by that function allows us to make more generalized actions that could react accordingly to different failure states.\nIn the following example, we’ll make a function called print_problem() that prints information to the console about the failure state for a validation step. In this case, the action will be applied to any threshold level being exceeded (by using Actions(default=print_problem)). And only the most severe level exceeded per step will execute print_problem() since Actions(highest_only=True) by default.\n\ndef print_problem():\n    m = pb.get_action_metadata()\n    print(f\"{m['level']} ({m['level_num']}) for Step {m['step']}: {m['failure_text']}\")\n\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(default=print_problem),\n        brief=True,\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n    )\n    .interrogate()\n)\n\nvalidation\n\nerror (40) for Step 2: Exceedance of failed test units where values in `item_revenue` should have been &gt; `0.05`.\ncritical (50) for Step 3: Exceedance of failed test units where values in `session_duration` should have been &gt; `15`.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:01:19DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        Expect that values in player_id should match the regular expression: [A-Z]{12}\\d{3}.\n\n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in item_revenue should be &gt; 0.05.\n\n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Expect that values in session_duration should be &gt; 15.\n\n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nWe end up seeing two messages printed for failures in Steps 2 and 3. And though those steps had more than one threshold exceeded, only the most severe level in each yielded a console message.\nAlse note that we set the action in Validate(actions=) so that the action would apply to all validation steps where thresholds are exceeded. This obviated the need to set actions= at every validation step (though you can do this as a local override, even setting actions=None to disable globally set actions).\nThe metadata dictionary contains the following fields for a given validation step:\n\nstep: The step number.\ncolumn: The column name.\nvalue: The value being compared (only available in certain validation steps).\ntype: The assertion type (e.g., \"col_vals_gt\", etc.).\ntime: The time the validation step was executed (in ISO format).\nlevel: The severity level (\"warning\", \"error\", or \"critical\").\nlevel_num: The severity level as a numeric value (30, 40, or 50).\nautobrief: A localized and brief statement of the expectation for the step.\nfailure_text: Localized text that explains how the validation step failed.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Triggering Actions"
    ]
  },
  {
    "objectID": "user-guide/actions.html#final-actions-with-finalactions",
    "href": "user-guide/actions.html#final-actions-with-finalactions",
    "title": "Triggering Actions",
    "section": "Final Actions with FinalActions",
    "text": "Final Actions with FinalActions\nWhen you need to execute actions after all validation steps are complete, Pointblank provides the FinalActions class. Unlike Actions which triggers on a per-step basis during the validation process, FinalActions executes after the entire validation is complete, giving you a way to respond to the overall validation results.\nHere’s how to use FinalActions:\n\nimport pointblank as pb\n\ndef send_alert():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        print(f\"ALERT: Critical validation failures found in `{summary['tbl_name']}`\")\n\nvalidation_with_final = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        tbl_name=\"game_revenue\",\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        final_actions=pb.FinalActions(\n            \"Validation complete.\",  # a string message\n            send_alert               # a callable function\n        )\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.10)\n    .interrogate()\n)\n\nvalidation_with_final\n\nValidation complete.\nALERT: Critical validation failures found in `game_revenue`\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:01:19DuckDBgame_revenueWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    14400.72\n    5600.28\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this example:\n\nWe define the function send_alert() that checks the validation summary for critical failures\nWe provide a simple string message \"Validation complete.\" that will print to the console\nBoth actions will execute in order after all validation steps have completed\n\nBecause the ‘critical’ threshold was exceeded in Step 2, we see the printed alert of send_alert() after the simple string message.\n\nCreating Final Actions\nFinalActions accepts any number of actions as positional arguments. Each argument can be:\n\nString: A message to be displayed in the console\nCallable: A function to be called with no arguments\nList of Strings/Callables: Multiple actions to execute in sequence\n\nAll actions will be executed in the order they are provided after all validation steps have completed.\n\n\nUsing get_validation_summary() in Final Actions\nWhen creating a callable function to use with FinalActions, you can access information about the overall validation results using the get_validation_summary() function. This gives you a dictionary with comprehensive information about the validation:\ndef comprehensive_report():\n    summary = pb.get_validation_summary()\n    print(f\"Validation Report for {summary['tbl_name']}:\")\n    print(f\"- Steps: {summary['n_steps']}\")\n    print(f\"- Passing steps: {summary['n_passing_steps']}\")\n    print(f\"- Failing steps: {summary['n_failing_steps']}\")\n\n    # Take additional actions based on results\n    if summary[\"n_failing_steps\"] &gt; 0:\n        # Create and execute a Slack notification\n        notify = pb.send_slack_notification(\n            webhook_url=\"https://hooks.slack.com/services/your/webhook/url\",\n            summary_msg=\"\"\"\n            🚨 *Validation Failure Alert*\n            • Table: {tbl_name}\n            • Failed Steps: {n_failing_steps} of {n_steps}\n            • Highest Severity: {highest_severity}\n            • Time: {time}\n            \"\"\",\n        )\n        notify()  # Execute the notification function\n\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        tbl_name=\"game_revenue\",\n        final_actions=pb.FinalActions(comprehensive_report),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:01:19DuckDBgame_revenue\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nHere we used the send_slack_notification() function, which is available in Pointblank as a pre-built action. It can be used by itself in final_actions= but here it’s integrated into the user’s comprehensive_report() function.\n\n\nCombining the Two Types of Actions\nYou can use both Actions and FinalActions together for comprehensive validation control:\n\ndef log_step_failure():\n    m = pb.get_action_metadata()\n    print(f\"Step {m['step']} failed with {m['level']}\")\n\n\ndef generate_summary():\n    summary = pb.get_validation_summary()\n    # Sum up total failed test units across all steps\n    total_failed = sum(summary[\"dict_n_failed\"].values())\n    # Sum up total test units across all steps\n    total_units = sum(summary[\"dict_n\"].values())\n    print(f\"Validation complete: {total_failed} failures out of {total_units} tests\")\n\n\nvalidation_combined = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10),\n        actions=pb.Actions(default=log_step_failure),\n        final_actions=pb.FinalActions(generate_summary),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .interrogate()\n)\n\nStep 2 failed with error\nValidation complete: 299 failures out of 4000 tests\n\n\nThis approach allows you to:\n\nLog individual step failures during the validation process using Actions\nGenerate a comprehensive report after all validation steps are complete using FinalActions\n\nUsing both action types gives you fine-grained control over when and how notifications and other actions are triggered in your validation workflow.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Triggering Actions"
    ]
  },
  {
    "objectID": "user-guide/missing-vals-tbl.html",
    "href": "user-guide/missing-vals-tbl.html",
    "title": "Checking Missingness in a Table",
    "section": "",
    "text": "Sometimes values just aren’t there: they’re missing. This can either be expected or another thing to worry about. Either way, we can dig a little deeper if need be and use the missing_vals_tbl() function to generate a summary table that can elucidate how many values are missing, and roughly where.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Checking Missingness in a Table"
    ]
  },
  {
    "objectID": "user-guide/missing-vals-tbl.html#using-and-understanding-missing_vals_tbl",
    "href": "user-guide/missing-vals-tbl.html#using-and-understanding-missing_vals_tbl",
    "title": "Checking Missingness in a Table",
    "section": "Using and Understanding missing_vals_tbl()",
    "text": "Using and Understanding missing_vals_tbl()\nThe missing values table is arranged a lot like the column summary table (generated via the col_summary_tbl() function) in that columns of the input table are arranged as rows in the reporting table. Let’s use missing_vals_tbl() on the nycflights dataset, which has a lot of missing values:\n\nimport pointblank as pb\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"polars\")\npb.missing_vals_tbl(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values   46,595 in total\n  \n  \n    PolarsRows336,776Columns18\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    year\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    month\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    day\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    carrier\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    flight\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    tailnum\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    origin\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dest\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    air_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    distance\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    hour\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    minute\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 3367733678 – 6735467355 – 101031101032 – 134708134709 – 168385168386 – 202062202063 – 235739235740 – 269416269417 – 303093303094 – 336776\n  \n\n\n\n\n\n\n        \n\n\nThere are 18 columns in nycflights and they’re arranged down the missing values table as rows. To the right we see column headers indicating 10 columns that are row sectors. Row sectors are groups of rows and each sector contains a tenth of the total rows in the table. The leftmost sectors are the rows at the top of the table whereas the sectors on the right are closer to the bottom. If you’d like to know which rows make up each row sector, there are details on this in the table footer area (click the ROW SECTORS text or the disclosure triangle).\nNow that we know about row sectors, we need to understand the visuals here. A light blue cell indicates there are no (0) missing values within a given row sector of a column. For nycflights we can see that several columns have no missing values at all (i.e., the light blue color makes up the entire row in the missing values table).\nWhen there are missing values in a column’s row sector, you’ll be met with a grayscale color. The proportion of missing values corresponds to the color ramp from light gray to solid black. Interestingly, most of the columns that have missing values appear to be related to each other in terms of the extent of missing values (i.e., the appearance in the reporting table looks roughly the same, indicating a sort of systematic missingness). These columns are dep_time, dep_delay, arr_time, arr_delay, and air_time.\nThe odd column out with regard to the distribution of missing values is tailnum. By scanning the row and observing that the grayscale color values are all a little different we see that the degree of missingness of more variable and not related to the other columns containing missing values.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Checking Missingness in a Table"
    ]
  },
  {
    "objectID": "user-guide/missing-vals-tbl.html#missing-value-tables-from-the-other-datasets",
    "href": "user-guide/missing-vals-tbl.html#missing-value-tables-from-the-other-datasets",
    "title": "Checking Missingness in a Table",
    "section": "Missing Value Tables from the Other Datasets",
    "text": "Missing Value Tables from the Other Datasets\nThe small_table dataset has only 13 rows to it. Let’s use that as a Pandas DataFrame with missing_vals_tbl():\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\npb.missing_vals_tbl(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values   2 in total\n  \n  \n    PandasRows13Columns8\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    date_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    date\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    a\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    b\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    c\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    d\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    e\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    f\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 12 – 23 – 34 – 45 – 56 – 67 – 78 – 89 – 910 – 13\n  \n\n\n\n\n\n\n        \n\n\nIt appears that only column c has missing values. And since the table is very small in terms of row count, most of the row sectors contain only a single row.\nThe game_revenue dataset has no missing values. And this can be easily proven by using missing_vals_tbl() with it:\n\ngame_revenue = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\")\npb.missing_vals_tbl(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values ✓\n  \n  \n    DuckDBRows2,000Columns11\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    player_id\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    session_id\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    session_start\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    item_type\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    item_name\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    item_revenue\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    session_duration\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    start_day\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    acquisition\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    country\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 200201 – 400401 – 600601 – 800801 – 10001001 – 12001201 – 14001401 – 16001601 – 18001801 – 2000\n  \n\n\n\n\n\n\n        \n\n\nWe see nothing but light blue in this report! The header also indicates that there are no missing values by displaying a large green check mark (the other report tables provided a count of total missing values across all columns).",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Checking Missingness in a Table"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html",
    "href": "user-guide/thresholds.html",
    "title": "Setting Failure Thresholds",
    "section": "",
    "text": "Thresholds enable you to signal failure at different severity levels. They also allow for the triggering of custom actions, a topic which is covered in the next section. For instance you might be testing a column for null/missing values. When doing so, you’d want to know when there are at least 10% missing values in the column. Alternatively, it could be the case that even a single missing value is critical to your work. Threshold settings in Pointblank give you the flexibility to devise data-failure signaling to whatever tolerances are important to you.\nLet’s start with the basics though. Here’s an example of a simple validation where threshold values are set in the col_vals_not_null() validation step (this type of validation expects that there are no null/missing values in a particular column):\nimport pointblank as pb\n\nvalidation_1 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_not_null(columns=\"c\", thresholds=(1, 0.2))\n    .interrogate()\n)\n\nvalidation_1\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:01:05Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    —\n    CSV\nThe code uses thresholds=(1, 0.2) to set a ‘warning’ threshold of 1 and an ‘error’ threshold of 0.2 (which is 20%) failing test units. You might notice the following in the validation table:\nThe one final threshold, C (‘critical’), wasn’t set so appears on the validation table as a dash.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Setting Failure Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#two-types-of-threshold-values-proportional-and-absolute",
    "href": "user-guide/thresholds.html#two-types-of-threshold-values-proportional-and-absolute",
    "title": "Setting Failure Thresholds",
    "section": "Two Types of Threshold Values: Proportional and Absolute",
    "text": "Two Types of Threshold Values: Proportional and Absolute\nThreshold values can be specified in two ways:\n\nproportional: a decimal value like 0.1 is taken to mean 10% of all test units failed\nabsolute: a whole number represents a fixed number of test units failed\n\nThreshold values act as cutoffs and are inclusive. So, any value of failing test units greater than or equal to the threshold value will result in exceeding the threshold. So if a threshold is defined with a value of 5, then 5 failing test units will result in an exceedance.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Setting Failure Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#using-the-validationthresholds-argument",
    "href": "user-guide/thresholds.html#using-the-validationthresholds-argument",
    "title": "Setting Failure Thresholds",
    "section": "Using the Validation(thresholds=) Argument",
    "text": "Using the Validation(thresholds=) Argument\nWe can also define thresholds globally (within the Validate call). This means that every validation step will re-use the same set of threshold values.\nimport pointblank as pb\n\nvalidation_2 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        thresholds=(1, 0.1)\n    )\n    .col_vals_not_null(columns=\"a\")\n    .col_vals_gt(columns=\"b\", value=2)\n    .interrogate()\n)\n\nvalidation_2\nIn this, both the col_vals_not_null() and col_vals_gt() steps will use the thresholds= value set in the Validate call. Now, if you want to override these global threshold values for a given validation step, you can always use the thresholds= argument when calling a validation method (this argument is present within every validation method).",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Setting Failure Thresholds"
    ]
  },
  {
    "objectID": "user-guide/thresholds.html#ways-to-define-thresholds",
    "href": "user-guide/thresholds.html#ways-to-define-thresholds",
    "title": "Setting Failure Thresholds",
    "section": "Ways to Define Thresholds",
    "text": "Ways to Define Thresholds\nThere are more than a few ways to set threshold levels. We provide this flexibility because it’s often useful to have simple shorthand methods for such a common task.\n\nUsing a Tuple or a Single Value\nThe fastest way to define a threshold is to use a tuple with positional entries for the ‘warning’, ‘error’, and ‘critical’ levels.\nthresholds_tuple = (1, 0.1, 0.25) # (warning, error, critical)\nNote that a shorter tuple is also allowed:\n\n(1, ): ‘warning’ state at 1 failing test unit\n(1, 0.1): ‘warning’ state at 1 failing test unit, error state at 10% failing test units\n\nYou can even use a scalar value (float between 0 and 1 or an integer). That single value represents the threshold value for the ‘warning’ level:\nthresholds_single = 1\nWhile using a tuple or a scalar can be very succinct, a problem that arises is that the ordering of values always begins at the ‘warning’ level. This means you cannot define a threshold level for just the ‘error’ level, for example. This is fine for many cases, however, there are other ways to express thresholds without these constraints.\n\n\nUsing the Thresholds Class\nUsing the Thresholds class lets you define the threshold levels using the warning=, error=, and critical= arguments. And unlike the method of setting thresholds with a tuple, any of the threshold levels can be left unset. Here’s an example where you might want to set the ‘error’ and ‘critical’ levels (leaving the ‘warning’ level unset):\nthresholds_class = pb.Thresholds(error=0.3, critical=0.5)\n\n\nUsing a Dictionary to Set Thresholds\nA specially-crafted dictionary is acceptable as input to any thresholds= argument. You need to ensure that the keys are named using either \"warning\", \"error\", or \"critical\". Any combination of keys is fine, but be careful to use only the aforementioned names (otherwise, you’ll receive a ValueError). Here’s an example that sets the ‘warning’ and ‘critical’ levels:\nthresholds_dict = {\"warning\": 1, \"critical\": 0.1}",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Setting Failure Thresholds"
    ]
  },
  {
    "objectID": "user-guide/index.html",
    "href": "user-guide/index.html",
    "title": "Introduction",
    "section": "",
    "text": "The Pointblank library is all about assessing the state of data quality for a table. You provide the validation rules and the library will dutifully interrogate the data and provide useful reporting. We can use different types of tables like Polars and Pandas DataFrames, Parquet files, or various database tables. Let’s walk through what data validation looks like in Pointblank.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#a-simple-validation-table",
    "href": "user-guide/index.html#a-simple-validation-table",
    "title": "Introduction",
    "section": "A Simple Validation Table",
    "text": "A Simple Validation Table\nThis is a validation report table that is produced from a validation of a Polars DataFrame:\n\n\nCode\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"), label=\"Example Validation\")\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [0, 5000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^[0-9]-[a-z]{3}-[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nEach row in this reporting table constitutes a single validation step. Roughly, the left-hand side outlines the validation rules and the right-hand side provides the results of each validation step. While simple in principle, there’s a lot of useful information packed into this validation table.\nHere’s a diagram that describes a few of the important parts of the validation table:\n\nThere are three things that should be noted here:\n\nvalidation steps: each step is a separate test on the table, focused on a certain aspect of the table\nvalidation rules: the validation type is provided here along with key constraints\nvalidation results: interrogation results are provided here, with a breakdown of test units (total, passing, and failing), threshold flags, and more\n\nThe intent is to provide the key information in one place, and have it be interpretable by data stakeholders.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#example-code-step-by-step",
    "href": "user-guide/index.html#example-code-step-by-step",
    "title": "Introduction",
    "section": "Example Code, Step-by-Step",
    "text": "Example Code, Step-by-Step\nHere’s the code that performs the validation on the Polars table.\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\n\nvalidation\nNote these three key pieces in the code:\n\nthe Validate(data=) argument takes a DataFrame or database table that you want to validate\nthe methods starting with col_vals_ specify validation steps that run on specific columns\nthe interrogate() method executes the validation plan on the table\n\nThis common pattern is used in a validation workflow, where Validate and interrogate() bookend a validation plan generated through calling validation methods. And that’s data validation with Pointblank in a nutshell! In the next section we’ll go a bit further by understanding how we can measure data quality with test units and failure thresholds.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#understanding-test-units",
    "href": "user-guide/index.html#understanding-test-units",
    "title": "Introduction",
    "section": "Understanding Test Units",
    "text": "Understanding Test Units\nEach validation step will execute a type of validation test on the target table. For example, a col_vals_lt() validation step can test that each value in a column is less than a specified number. The key finding that’s reported as a result of this test is the number of test units that pass or fail.\nTest units are dependent on the test being run. The collection of col_vals_* validation methods will test each and every value in a particular column, so each value will be a test unit (and the number of test units is the number of rows in the target table). Some validation methods like col_exists() or row_count_match() have only a single test unit since they aren’t testing individual values but rather if the overall test passes or fails.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "user-guide/index.html#using-threshold-levels",
    "href": "user-guide/index.html#using-threshold-levels",
    "title": "Introduction",
    "section": "Using Threshold Levels",
    "text": "Using Threshold Levels\nKnowing about the numbers of test units across validation methods matters because you have the option to set thresholds (that can signal ‘warning’, ‘error’, and ‘critical’ flags) based on either the relative proportion or absolute number of failing test units.\nHere’s a simple example that uses a single validation step along with thresholds set in the thresholds= argument of the validation method.\n\nvalidation_2 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_lt(columns=\"a\", value=7, thresholds=(2, 4))\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:00:59Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe code uses thresholds=(2, 4) to set a ‘warning’ threshold of 2 and an error threshold of 4. If you look at the validation report table, we can see:\n\nthe FAIL column shows that 2 tests units have failed\nthe W column (short for ‘warning’) shows a filled gray circle indicating those failing test units reached that threshold value\nthe E column (short for ‘error’) shows an open yellow circle indicating that the number of failing test units is below that threshold\n\nThe one final threshold level, C (for ‘critical’), wasn’t set so it appears on the validation table as a long dash.\nSetting thresholds is important since you might want some sort of signal for the discovery of errors in your data. How you set the particular threshold levels is highly dependent on your tolerance for data failures. The idea of thresholds and associated actions is central to how Pointblank works, so, the next two sections in the User Guide will deal with (1) a more in-depth treatment of thresholds, and (2) how to set actions for threshold exceedances.",
    "crumbs": [
      "User Guide",
      "Getting Started",
      "Introduction"
    ]
  },
  {
    "objectID": "reference/expr_col.html",
    "href": "reference/expr_col.html",
    "title": "expr_col",
    "section": "",
    "text": "expr_col(column_name)\nCreate a column expression for use in conjointly() validation.\nThis function returns a ColumnExpression object that supports operations like &gt;, &lt;, +, etc. for use in conjointly() validation expressions."
  },
  {
    "objectID": "reference/expr_col.html#parameters",
    "href": "reference/expr_col.html#parameters",
    "title": "expr_col",
    "section": "Parameters",
    "text": "Parameters\n\ncolumn_name : str\n\nThe name of the column to reference."
  },
  {
    "objectID": "reference/expr_col.html#returns",
    "href": "reference/expr_col.html#returns",
    "title": "expr_col",
    "section": "Returns",
    "text": "Returns\n\n : ColumnExpression\n\nA column expression that can be used in comparisons and operations."
  },
  {
    "objectID": "reference/expr_col.html#examples",
    "href": "reference/expr_col.html#examples",
    "title": "expr_col",
    "section": "Examples",
    "text": "Examples\nLet’s say we have a table with three columns: a, b, and c. We want to validate that:\n\nThe values in column a are greater than 2.\nThe values in column b are less than 7.\nThe sum of columns a and b is less than the values in column c.\n\nWe can use the expr_col() function to create a column expression for each of these conditions.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 7, 1, 3, 9, 4],\n        \"b\": [6, 3, 0, 5, 8, 2],\n        \"c\": [10, 4, 8, 9, 10, 5],\n    }\n)\n\n# Using expr_col() to create backend-agnostic validation expressions\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pb.expr_col(\"a\") &gt; 2,\n        lambda df: pb.expr_col(\"b\") &lt; 7,\n        lambda df: pb.expr_col(\"a\") + pb.expr_col(\"b\") &lt; pb.expr_col(\"c\")\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    10.17\n    50.83\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe above code creates a validation object that checks the specified conditions using the expr_col() function. The resulting validation table will show whether each condition was satisfied for each row in the table."
  },
  {
    "objectID": "reference/expr_col.html#see-also",
    "href": "reference/expr_col.html#see-also",
    "title": "expr_col",
    "section": "See Also",
    "text": "See Also\nThe conjointly() validation method, which is where this function should be used."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html",
    "href": "reference/Validate.get_sundered_data.html",
    "title": "Validate.get_sundered_data",
    "section": "",
    "text": "Validate.get_sundered_data(type='pass')\nGet the data that passed or failed the validation steps.\nValidation of the data is one thing but, sometimes, you want to use the best part of the input dataset for something else. The get_sundered_data() method works with a Validate object that has been interrogated (i.e., the interrogate() method was used). We can get either the ‘pass’ data piece (rows with no failing test units across all row-based validation functions), or, the ‘fail’ data piece (rows with at least one failing test unit across the same series of validations)."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#details",
    "href": "reference/Validate.get_sundered_data.html#details",
    "title": "Validate.get_sundered_data",
    "section": "Details",
    "text": "Details\nThere are some caveats to sundering. The validation steps considered for this splitting will only involve steps where:\n\nof certain check types, where test units are cells checked row-by-row (e.g., the col_vals_*() methods)\nactive= is not set to False\npre= has not been given an expression for modifying the input table\n\nSo long as these conditions are met, the data will be split into two constituent tables: one with the rows that passed all validation steps and another with the rows that failed at least one validation step."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#parameters",
    "href": "reference/Validate.get_sundered_data.html#parameters",
    "title": "Validate.get_sundered_data",
    "section": "Parameters",
    "text": "Parameters\n\ntype :  = 'pass'\n\nThe type of data to return. Options are \"pass\" or \"fail\", where the former returns a table only containing rows where test units always passed validation steps, and the latter returns a table only containing rows had test units that failed in at least one validation step."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#returns",
    "href": "reference/Validate.get_sundered_data.html#returns",
    "title": "Validate.get_sundered_data",
    "section": "Returns",
    "text": "Returns\n\n : FrameT\n\nA table containing the data that passed or failed the validation steps."
  },
  {
    "objectID": "reference/Validate.get_sundered_data.html#examples",
    "href": "reference/Validate.get_sundered_data.html#examples",
    "title": "Validate.get_sundered_data",
    "section": "Examples",
    "text": "Examples\nLet’s create a Validate object with three validation steps and then interrogate the data.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 6, 9, 7, 3, 2],\n        \"b\": [9, 8, 10, 5, 10, 6],\n        \"c\": [\"c\", \"d\", \"a\", \"b\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:00:45Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-04-23 01:00:45 UTC&lt; 1 s2025-04-23 01:00:45 UTC\n  \n\n\n\n\n\n\n        \n\n\nFrom the validation table, we can see that the first and second steps each had 4 passing test units. A failing test unit will mark the entire row as failing in the context of the get_sundered_data() method. We can use this method to get the rows of data that passed the during interrogation.\n\npb.preview(validation.get_sundered_data())\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cString\n\n\n\n  \n    1\n    9\n    10\n    a\n  \n  \n    2\n    7\n    5\n    b\n  \n\n\n\n\n\n\n        \n\n\nThe returned DataFrame contains the rows that passed all validation steps (we passed this object to preview() to show it in an HTML view). From the six-row input DataFrame, the first two rows and the last two rows had test units that failed validation. Thus the middle two rows are the only ones that passed all validation steps and that’s what we see in the returned DataFrame."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html",
    "href": "reference/Validate.col_vals_between.html",
    "title": "Validate.col_vals_between",
    "section": "",
    "text": "Validate.col_vals_between(\n    columns,\n    left,\n    right,\n    inclusive=(True, True),\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo column data lie between two specified values or data in other columns?\nThe col_vals_between() validation method checks whether column values in a table fall within a range. The range is specified with three arguments: left=, right=, and inclusive=. The left= and right= values specify the lower and upper bounds. These bounds can be specified as literal values or as column names provided within col(). The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#parameters",
    "href": "reference/Validate.col_vals_between.html#parameters",
    "title": "Validate.col_vals_between",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nleft : float | int | Column\n\nThe lower bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\nright : float | int | Column\n\nThe upper bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\ninclusive : tuple[bool, bool] = (True, True)\n\nA tuple of two boolean values indicating whether the comparison should be inclusive. The position of the boolean values correspond to the left= and right= values, respectively. By default, both values are True.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#returns",
    "href": "reference/Validate.col_vals_between.html#returns",
    "title": "Validate.col_vals_between",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#what-can-be-used-in-left-and-right",
    "href": "reference/Validate.col_vals_between.html#what-can-be-used-in-left-and-right",
    "title": "Validate.col_vals_between",
    "section": "What Can Be Used in left= and right=?",
    "text": "What Can Be Used in left= and right=?\nThe left= and right= arguments both allow for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column in the target table\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value within left= and right=. There is flexibility in how you provide the date or datetime values for the bounds; they can be:\n\nstring-based dates or datetimes (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\ndate or datetime objects using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in either left= or right= (or both), it must be specified within col(). This facilitates column-to-column comparisons and, crucially, the columns being compared to either/both of the bounds must be of the same type as the column data (e.g., all numeric, all dates, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#preprocessing",
    "href": "reference/Validate.col_vals_between.html#preprocessing",
    "title": "Validate.col_vals_between",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and left=col(...)/right=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#thresholds",
    "href": "reference/Validate.col_vals_between.html#thresholds",
    "title": "Validate.col_vals_between",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_between.html#examples",
    "href": "reference/Validate.col_vals_between.html#examples",
    "title": "Validate.col_vals_between",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [2, 3, 2, 4, 3, 4],\n        \"b\": [5, 6, 1, 6, 8, 5],\n        \"c\": [9, 8, 8, 7, 7, 8],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    2\n    5\n    9\n  \n  \n    2\n    3\n    6\n    8\n  \n  \n    3\n    2\n    1\n    8\n  \n  \n    4\n    4\n    6\n    7\n  \n  \n    5\n    3\n    8\n    7\n  \n  \n    6\n    4\n    5\n    8\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all between the fixed boundary values of 1 and 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_between(columns=\"a\", left=1, right=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    a\n    [1, 5]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_between(). All test units passed, and there are no failing test units.\nAside from checking a column against two literal values representing the lower and upper bounds, we can also provide column names to the left= and/or right= arguments (by using the helper function col(). In this way, we can perform three additional comparison types:\n\nleft=column, right=column\nleft=literal, right=column\nleft=column, right=literal\n\nFor the next example, we’ll use col_vals_between() to check whether the values in column b are between than corresponding values in columns a (lower bound) and c (upper bound).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_between(columns=\"b\", left=pb.col(\"a\"), right=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    b\n    [a, c]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 2: b is 1 but the bounds are 2 (a) and 8 (c).\nRow 4: b is 8 but the bounds are 3 (a) and 7 (c)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html",
    "href": "reference/Validate.rows_distinct.html",
    "title": "Validate.rows_distinct",
    "section": "",
    "text": "Validate.rows_distinct(\n    columns_subset=None,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether rows in the table are distinct.\nThe rows_distinct() method checks whether rows in the table are distinct. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#parameters",
    "href": "reference/Validate.rows_distinct.html#parameters",
    "title": "Validate.rows_distinct",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns_subset : str | list[str] | None = None\n\nA single column or a list of columns to use as a subset for the distinct comparison. If None, then all columns in the table will be used for the comparison. If multiple columns are supplied, the distinct comparison will be made over the combination of values in those columns.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#returns",
    "href": "reference/Validate.rows_distinct.html#returns",
    "title": "Validate.rows_distinct",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#preprocessing",
    "href": "reference/Validate.rows_distinct.html#preprocessing",
    "title": "Validate.rows_distinct",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns_subset= that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#thresholds",
    "href": "reference/Validate.rows_distinct.html#thresholds",
    "title": "Validate.rows_distinct",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.rows_distinct.html#examples",
    "href": "reference/Validate.rows_distinct.html#examples",
    "title": "Validate.rows_distinct",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three string columns (col_1, col_2, and col_3). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"col_1\": [\"a\", \"b\", \"c\", \"d\"],\n        \"col_2\": [\"a\", \"a\", \"c\", \"d\"],\n        \"col_3\": [\"a\", \"a\", \"d\", \"e\"],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  col_1String\n  col_2String\n  col_3String\n\n\n\n  \n    1\n    a\n    a\n    a\n  \n  \n    2\n    b\n    a\n    a\n  \n  \n    3\n    c\n    c\n    d\n  \n  \n    4\n    d\n    d\n    e\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the rows in the table are distinct with rows_distinct(). We’ll determine if this validation had any failing test units (there are four test units, one for each row). A failing test units means that a given row is not distinct from every other row.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom this validation table we see that there are no failing test units. All rows in the table are distinct from one another.\nWe can also use a subset of columns to determine distinctness. Let’s specify the subset using columns col_2 and col_3 for the next validation.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .rows_distinct(columns_subset=[\"col_2\", \"col_3\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    col_2, col_3\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The first and second rows are duplicated when considering only the values in columns col_2 and col_3. There’s only one set of duplicates but there are two failing test units since each row is compared to all others."
  },
  {
    "objectID": "reference/DraftValidation.html",
    "href": "reference/DraftValidation.html",
    "title": "DraftValidation",
    "section": "",
    "text": "DraftValidation(self, data, model, api_key=None)\nDraft a validation plan for a given table using an LLM.\nBy using a large language model (LLM) to draft a validation plan, you can quickly generate a starting point for validating a table. This can be useful when you have a new table and you want to get a sense of how to validate it (and adjustments could always be made later). The DraftValidation class uses the chatlas package to draft a validation plan for a given table using an LLM from either the \"anthropic\", \"openai\", \"ollama\" or \"bedrock\" provider. You can install all requirements for the class through an optional ‘generate’ install of Pointblank via pip install pointblank[generate].\n\n\n\n\n\n\nWarning\n\n\n\nThe DraftValidation class is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\ndata : FrameT | Any\n\nThe data to be used for drafting a validation plan.\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\napi_key : str | None = None\n\nThe API key to be used for the model.\n\n\n\n\n\n\n : str\n\nThe drafted validation plan.\n\n\n\n\n\nThe model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names.\n\n\n\nProviding a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way.\n\n\n\nThe data sent to the model provider is a JSON summary of the table. This data summary is generated internally by DraftValidation using the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information to draft a validation plan. As such, even very large tables can be used with the DraftValidation class since the contents of the table are not sent to the model provider.\nThe Amazon Bedrock is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally, though only a few models are capable enough to perform the task of drafting a validation plan.\n\n\n\nLet’s look at how the DraftValidation class can be used to draft a validation plan for a table. The table to be used is \"nycflights\", which is available here via the load_dataset() function. The model to be used is \"anthropic:claude-3-5-sonnet-latest\" (which performs very well compared to other LLMs). The example assumes that the API key is stored in an .env file as ANTHROPIC_API_KEY.\nimport pointblank as pb\n\n# Load the \"nycflights\" dataset as a DuckDB table\ndata = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\n# Draft a validation plan for the \"nycflights\" table\npb.DraftValidation(data=nycflights, model=\"anthropic:claude-3-5-sonnet-latest\")\nThe output will be a drafted validation plan for the \"nycflights\" table and this will appear in the console.\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"year\", \"int64\"),\n    (\"month\", \"int64\"),\n    (\"day\", \"int64\"),\n    (\"dep_time\", \"int64\"),\n    (\"sched_dep_time\", \"int64\"),\n    (\"dep_delay\", \"int64\"),\n    (\"arr_time\", \"int64\"),\n    (\"sched_arr_time\", \"int64\"),\n    (\"arr_delay\", \"int64\"),\n    (\"carrier\", \"string\"),\n    (\"flight\", \"int64\"),\n    (\"tailnum\", \"string\"),\n    (\"origin\", \"string\"),\n    (\"dest\", \"string\"),\n    (\"air_time\", \"int64\"),\n    (\"distance\", \"int64\"),\n    (\"hour\", \"int64\"),\n    (\"minute\", \"int64\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"year\", \"month\", \"day\", \"sched_dep_time\", \"carrier\", \"flight\",\n        \"origin\", \"dest\", \"distance\", \"hour\", \"minute\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12)\n    .col_vals_between(columns=\"day\", left=1, right=31)\n    .col_vals_between(columns=\"sched_dep_time\", left=106, right=2359)\n    .col_vals_between(columns=\"dep_delay\", left=-43, right=1301, na_pass=True)\n    .col_vals_between(columns=\"air_time\", left=20, right=695, na_pass=True)\n    .col_vals_between(columns=\"distance\", left=17, right=4983)\n    .col_vals_between(columns=\"hour\", left=1, right=23)\n    .col_vals_between(columns=\"minute\", left=0, right=59)\n    .col_vals_in_set(columns=\"origin\", set=[\"EWR\", \"LGA\", \"JFK\"])\n    .col_count_match(count=18)\n    .row_count_match(count=336776)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\nThe drafted validation plan can be copied and pasted into a Python script or notebook for further use. In other words, the generated plan can be adjusted as needed to suit the specific requirements of the table being validated.\nNote that the output does not know how the data was obtained, so it uses the placeholder your_data in the data= argument of the Validate class. When adapted for use, this should be replaced with the actual data variable."
  },
  {
    "objectID": "reference/DraftValidation.html#parameters",
    "href": "reference/DraftValidation.html#parameters",
    "title": "DraftValidation",
    "section": "",
    "text": "data : FrameT | Any\n\nThe data to be used for drafting a validation plan.\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\napi_key : str | None = None\n\nThe API key to be used for the model."
  },
  {
    "objectID": "reference/DraftValidation.html#returns",
    "href": "reference/DraftValidation.html#returns",
    "title": "DraftValidation",
    "section": "",
    "text": ": str\n\nThe drafted validation plan."
  },
  {
    "objectID": "reference/DraftValidation.html#constructing-the-model-argument",
    "href": "reference/DraftValidation.html#constructing-the-model-argument",
    "title": "DraftValidation",
    "section": "",
    "text": "The model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names."
  },
  {
    "objectID": "reference/DraftValidation.html#notes-on-authentication",
    "href": "reference/DraftValidation.html#notes-on-authentication",
    "title": "DraftValidation",
    "section": "",
    "text": "Providing a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way."
  },
  {
    "objectID": "reference/DraftValidation.html#notes-on-data-sent-to-the-model-provider",
    "href": "reference/DraftValidation.html#notes-on-data-sent-to-the-model-provider",
    "title": "DraftValidation",
    "section": "",
    "text": "The data sent to the model provider is a JSON summary of the table. This data summary is generated internally by DraftValidation using the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information to draft a validation plan. As such, even very large tables can be used with the DraftValidation class since the contents of the table are not sent to the model provider.\nThe Amazon Bedrock is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally, though only a few models are capable enough to perform the task of drafting a validation plan."
  },
  {
    "objectID": "reference/DraftValidation.html#examples",
    "href": "reference/DraftValidation.html#examples",
    "title": "DraftValidation",
    "section": "",
    "text": "Let’s look at how the DraftValidation class can be used to draft a validation plan for a table. The table to be used is \"nycflights\", which is available here via the load_dataset() function. The model to be used is \"anthropic:claude-3-5-sonnet-latest\" (which performs very well compared to other LLMs). The example assumes that the API key is stored in an .env file as ANTHROPIC_API_KEY.\nimport pointblank as pb\n\n# Load the \"nycflights\" dataset as a DuckDB table\ndata = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\n# Draft a validation plan for the \"nycflights\" table\npb.DraftValidation(data=nycflights, model=\"anthropic:claude-3-5-sonnet-latest\")\nThe output will be a drafted validation plan for the \"nycflights\" table and this will appear in the console.\n```python\nimport pointblank as pb\n\n# Define schema based on column names and dtypes\nschema = pb.Schema(columns=[\n    (\"year\", \"int64\"),\n    (\"month\", \"int64\"),\n    (\"day\", \"int64\"),\n    (\"dep_time\", \"int64\"),\n    (\"sched_dep_time\", \"int64\"),\n    (\"dep_delay\", \"int64\"),\n    (\"arr_time\", \"int64\"),\n    (\"sched_arr_time\", \"int64\"),\n    (\"arr_delay\", \"int64\"),\n    (\"carrier\", \"string\"),\n    (\"flight\", \"int64\"),\n    (\"tailnum\", \"string\"),\n    (\"origin\", \"string\"),\n    (\"dest\", \"string\"),\n    (\"air_time\", \"int64\"),\n    (\"distance\", \"int64\"),\n    (\"hour\", \"int64\"),\n    (\"minute\", \"int64\")\n])\n\n# The validation plan\nvalidation = (\n    pb.Validate(\n        data=your_data,  # Replace your_data with the actual data variable\n        label=\"Draft Validation\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n    )\n    .col_schema_match(schema=schema)\n    .col_vals_not_null(columns=[\n        \"year\", \"month\", \"day\", \"sched_dep_time\", \"carrier\", \"flight\",\n        \"origin\", \"dest\", \"distance\", \"hour\", \"minute\"\n    ])\n    .col_vals_between(columns=\"month\", left=1, right=12)\n    .col_vals_between(columns=\"day\", left=1, right=31)\n    .col_vals_between(columns=\"sched_dep_time\", left=106, right=2359)\n    .col_vals_between(columns=\"dep_delay\", left=-43, right=1301, na_pass=True)\n    .col_vals_between(columns=\"air_time\", left=20, right=695, na_pass=True)\n    .col_vals_between(columns=\"distance\", left=17, right=4983)\n    .col_vals_between(columns=\"hour\", left=1, right=23)\n    .col_vals_between(columns=\"minute\", left=0, right=59)\n    .col_vals_in_set(columns=\"origin\", set=[\"EWR\", \"LGA\", \"JFK\"])\n    .col_count_match(count=18)\n    .row_count_match(count=336776)\n    .rows_distinct()\n    .interrogate()\n)\n\nvalidation\n```\nThe drafted validation plan can be copied and pasted into a Python script or notebook for further use. In other words, the generated plan can be adjusted as needed to suit the specific requirements of the table being validated.\nNote that the output does not know how the data was obtained, so it uses the placeholder your_data in the data= argument of the Validate class. When adapted for use, this should be replaced with the actual data variable."
  },
  {
    "objectID": "reference/send_slack_notification.html",
    "href": "reference/send_slack_notification.html",
    "title": "send_slack_notification",
    "section": "",
    "text": "send_slack_notification(\n    webhook_url=None,\n    step_msg=None,\n    summary_msg=None,\n    debug=False,\n)\nCreate a Slack notification function using a webhook URL.\nThis function can be used in two ways:\n\nWith Actions to notify about individual validation step failures\nWith FinalActions to provide a summary notification after all validation steps have undergone interrogation\n\nThe function creates a callable that sends notifications through a Slack webhook. Message formatting can be customized using templates for both individual steps and summary reports.\n\n\n\nwebhook_url : str | None = None\n\nThe Slack webhook URL. If None (and debug=True), a dry run is performed (see the Offline Testing section below for information on this).\n\nstep_msg : str | None = None\n\nTemplate string for step notifications. Some of the available variables include: \"{step}\", \"{column}\", \"{value}\", \"{type}\", \"{time}\", \"{level}\", etc. See the Available Template Variables for Step Notifications section below for more details. If not provided, a default step message template will be used.\n\nsummary_msg : str | None = None\n\nTemplate string for summary notifications. Some of the available variables are: \"{n_steps}\", \"{n_passing_steps}\", \"{n_failing_steps}\", \"{all_passed}\", \"{highest_severity}\", etc. See the Available Template Variables for Summary Notifications section below for more details. If not provided, a default summary message template will be used.\n\ndebug : bool = False\n\nPrint debug information if True. This includes the message content and the response from Slack. This is useful for testing and debugging the notification function. If webhook_url is None, the function will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly.\n\n\n\n\n\n\n : Callable\n\nA function that sends notifications to Slack.\n\n\n\n\n\nWhen creating a custom template for validation step alerts (step_msg=), the following templating strings can be used:\n\n\"{step}\": The step number.\n\"{column}\": The column name.\n\"{value}\": The value being compared (only available in certain validation steps).\n\"{type}\": The assertion type (e.g., \"col_vals_gt\", etc.).\n\"{level}\": The severity level (\"warning\", \"error\", or \"critical\").\n\"{level_num}\": The severity level as a numeric value (30, 40, or 50).\n\"{autobrief}\": A localized and brief statement of the expectation for the step.\n\"{failure_text}\": Localized text that explains how the validation step failed.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to construct a step_msg= template:\nstep_msg = '''🚨 *Validation Step Alert*\n• Step Number: {step}\n• Column: {column}\n• Test Type: {type}\n• Value Tested: {value}\n• Severity: {level} (level {level_num})\n• Brief: {autobrief}\n• Details: {failure_text}\n• Time: {time}'''\nThis template will be filled with the relevant information when a validation step fails. The placeholders will be replaced with actual values when the Slack notification is sent.\n\n\n\nWhen creating a custom template for a validation summary (summary_msg=), the following templating strings can be used:\n\n\"{n_steps}\": The total number of validation steps.\n\"{n_passing_steps}\": The number of validation steps where all test units passed.\n\"{n_failing_steps}\": The number of validation steps that had some failing test units.\n\"{n_warning_steps}\": The number of steps that exceeded a ‘warning’ threshold.\n\"{n_error_steps}\": The number of steps that exceeded an ‘error’ threshold.\n\"{n_critical_steps}\": The number of steps that exceeded a ‘critical’ threshold.\n\"{all_passed}\": Whether or not every validation step had no failing test units.\n\"{highest_severity}\": The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\n\"{tbl_row_count}\": The number of rows in the target table.\n\"{tbl_column_count}\": The number of columns in the target table.\n\"{tbl_name}\": The name of the target table.\n\"{validation_duration}\": The duration of the validation in seconds.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to put together a summary_msg= template:\nsummary_msg = '''📊 *Validation Summary Report*\n*Overview*\n• Status: {highest_severity}\n• All Passed: {all_passed}\n• Total Steps: {n_steps}\n\n*Step Results*\n• Passing Steps: {n_passing_steps}\n• Failing Steps: {n_failing_steps}\n• Warning Level: {n_warning_steps}\n• Error Level: {n_error_steps}\n• Critical Level: {n_critical_steps}\n\n*Table Info*\n• Table Name: {tbl_name}\n• Row Count: {tbl_row_count}\n• Column Count: {tbl_column_count}\n\n*Timing*\n• Duration: {validation_duration}s\n• Completed: {time}'''\nThis template will be filled with the relevant information when the validation summary is generated. The placeholders will be replaced with actual values when the Slack notification is sent.\n\n\n\nIf you want to test the function without sending actual notifications, you can leave the webhook_url= as None and set debug=True. This will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly. Furthermore, the function could be run globally (i.e., outside of the context of a validation plan) to show the message templates with all possible variables. Here’s an example of how to do this:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None,  # Leave as None for dry run\n    debug=True,  # Enable debug mode to print message previews\n)\n# Call the function to see the message previews\nnotify_slack()\nThis will print the step and summary message previews to the console, allowing you to see how the templates will look when filled with actual data. You can then adjust your templates as needed before using them in a real validation plan.\nWhen step_msg= and summary_msg= are not provided, the function will use default templates. However, you can customize the templates to include additional information or change the format to better suit your needs. Iterating on the templates can help you create more informative and visually appealing messages. Here’s an example of that:\nimport pointblank as pb\n\n# Create a Slack notification function with custom templates\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None, # Leave as None for dry run\n    step_msg='''*Data Validation Alert*\n    • Type: {type}\n    • Level: {level}\n    • Step: {step}\n    • Column: {column}\n    • Time: {time}''',\n    summary_msg='''*Data Validation Summary*\n    • Highest Severity: {highest_severity}\n    • Total Steps: {n_steps}\n    • Failed Steps: {n_failing_steps}\n    • Time: {time}''',\n    debug=True,  # Enable debug mode to print message previews\n)\nThese templates will be used with sample data when the function is called. The combination of webhook_url=None and debug=True allows you to test your custom templates without having to send actual notifications to Slack.\n\n\n\nWhen using an action with one or more validation steps, you typically provide callables that fire when a matched threshold of failed test units is exceeded. The callable can be a function or a lambda. The send_slack_notification() function creates a callable that sends a Slack notification when the validation step fails. Here is how it can be set up to work for multiple validation steps by using of Actions:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation\nBy placing the notify_slack function in the Validate(actions=Actions(critical=)) argument, you can ensure that the notification is sent whenever the ‘critical’ threshold is reached (as set here, when 15% or more of the test units fail). The notification will include information about the validation step that triggered the alert.\nWhen using a FinalActions object, the notification will be sent after all validation steps have been completed. This is useful for providing a summary of the validation process. Here is an example of how to set up a summary notification:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this case, the same notify_slack function is used, but it is placed in Validate(final_actions=FinalActions()). This results in the summary notification being sent after all validation steps are completed, regardless of whether any steps failed or not.\nThis simplicity is possible because the send_slack_notification() function creates a callable that can be used in both contexts. The function will automatically determine whether to send a step notification or a summary notification based on the context in which it is called.\nWe can customize the message templates for both step and summary notifications. In that way, it’s possible to create a more informative and visually appealing message. For example, we can use Markdown formatting to make the message more readable and visually appealing. Here is an example of how to customize the templates:\nimport pointblank as pb\n# Create a Slack notification function\n\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\",\n    step_msg='''\n    🚨 *Validation Step Alert*\n    • Step Number: {step}\n    • Column: {column}\n    • Test Type: {type}\n    • Value Tested: {value}\n    • Severity: {level} (level {level_num})\n    • Brief: {autobrief}\n    • Details: {failure_text}\n    • Time: {time}''',\n    summary_msg='''\n    📊 *Validation Summary Report*\n    *Overview*\n    • Status: {highest_severity}\n    • All Passed: {all_passed}\n    • Total Steps: {n_steps}\n\n    *Step Results*\n    • Passing Steps: {n_passing_steps}\n    • Failing Steps: {n_failing_steps}\n    • Warning Level: {n_warning_steps}\n    • Error Level: {n_error_steps}\n    • Critical Level: {n_critical_steps}\n\n    *Table Info*\n    • Table Name: {tbl_name}\n    • Row Count: {tbl_row_count}\n    • Column Count: {tbl_column_count}\n\n    *Timing*\n    • Duration: {validation_duration}s\n    • Completed: {time}''',\n)\n\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(default=notify_slack),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this example, we have customized the templates for both step and summary notifications. The step notification includes details about the validation step, including the step number, column name, test type, value tested, severity level, brief description, and time of the notification. The summary notification includes an overview of the validation process, including the status, number of steps, passing and failing steps, table information, and timing details."
  },
  {
    "objectID": "reference/send_slack_notification.html#parameters",
    "href": "reference/send_slack_notification.html#parameters",
    "title": "send_slack_notification",
    "section": "",
    "text": "webhook_url : str | None = None\n\nThe Slack webhook URL. If None (and debug=True), a dry run is performed (see the Offline Testing section below for information on this).\n\nstep_msg : str | None = None\n\nTemplate string for step notifications. Some of the available variables include: \"{step}\", \"{column}\", \"{value}\", \"{type}\", \"{time}\", \"{level}\", etc. See the Available Template Variables for Step Notifications section below for more details. If not provided, a default step message template will be used.\n\nsummary_msg : str | None = None\n\nTemplate string for summary notifications. Some of the available variables are: \"{n_steps}\", \"{n_passing_steps}\", \"{n_failing_steps}\", \"{all_passed}\", \"{highest_severity}\", etc. See the Available Template Variables for Summary Notifications section below for more details. If not provided, a default summary message template will be used.\n\ndebug : bool = False\n\nPrint debug information if True. This includes the message content and the response from Slack. This is useful for testing and debugging the notification function. If webhook_url is None, the function will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly."
  },
  {
    "objectID": "reference/send_slack_notification.html#returns",
    "href": "reference/send_slack_notification.html#returns",
    "title": "send_slack_notification",
    "section": "",
    "text": ": Callable\n\nA function that sends notifications to Slack."
  },
  {
    "objectID": "reference/send_slack_notification.html#available-template-variables-for-step-notifications",
    "href": "reference/send_slack_notification.html#available-template-variables-for-step-notifications",
    "title": "send_slack_notification",
    "section": "",
    "text": "When creating a custom template for validation step alerts (step_msg=), the following templating strings can be used:\n\n\"{step}\": The step number.\n\"{column}\": The column name.\n\"{value}\": The value being compared (only available in certain validation steps).\n\"{type}\": The assertion type (e.g., \"col_vals_gt\", etc.).\n\"{level}\": The severity level (\"warning\", \"error\", or \"critical\").\n\"{level_num}\": The severity level as a numeric value (30, 40, or 50).\n\"{autobrief}\": A localized and brief statement of the expectation for the step.\n\"{failure_text}\": Localized text that explains how the validation step failed.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to construct a step_msg= template:\nstep_msg = '''🚨 *Validation Step Alert*\n• Step Number: {step}\n• Column: {column}\n• Test Type: {type}\n• Value Tested: {value}\n• Severity: {level} (level {level_num})\n• Brief: {autobrief}\n• Details: {failure_text}\n• Time: {time}'''\nThis template will be filled with the relevant information when a validation step fails. The placeholders will be replaced with actual values when the Slack notification is sent."
  },
  {
    "objectID": "reference/send_slack_notification.html#available-template-variables-for-summary-notifications",
    "href": "reference/send_slack_notification.html#available-template-variables-for-summary-notifications",
    "title": "send_slack_notification",
    "section": "",
    "text": "When creating a custom template for a validation summary (summary_msg=), the following templating strings can be used:\n\n\"{n_steps}\": The total number of validation steps.\n\"{n_passing_steps}\": The number of validation steps where all test units passed.\n\"{n_failing_steps}\": The number of validation steps that had some failing test units.\n\"{n_warning_steps}\": The number of steps that exceeded a ‘warning’ threshold.\n\"{n_error_steps}\": The number of steps that exceeded an ‘error’ threshold.\n\"{n_critical_steps}\": The number of steps that exceeded a ‘critical’ threshold.\n\"{all_passed}\": Whether or not every validation step had no failing test units.\n\"{highest_severity}\": The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\n\"{tbl_row_count}\": The number of rows in the target table.\n\"{tbl_column_count}\": The number of columns in the target table.\n\"{tbl_name}\": The name of the target table.\n\"{validation_duration}\": The duration of the validation in seconds.\n\"{time}\": The time of the notification.\n\nHere’s an example of how to put together a summary_msg= template:\nsummary_msg = '''📊 *Validation Summary Report*\n*Overview*\n• Status: {highest_severity}\n• All Passed: {all_passed}\n• Total Steps: {n_steps}\n\n*Step Results*\n• Passing Steps: {n_passing_steps}\n• Failing Steps: {n_failing_steps}\n• Warning Level: {n_warning_steps}\n• Error Level: {n_error_steps}\n• Critical Level: {n_critical_steps}\n\n*Table Info*\n• Table Name: {tbl_name}\n• Row Count: {tbl_row_count}\n• Column Count: {tbl_column_count}\n\n*Timing*\n• Duration: {validation_duration}s\n• Completed: {time}'''\nThis template will be filled with the relevant information when the validation summary is generated. The placeholders will be replaced with actual values when the Slack notification is sent."
  },
  {
    "objectID": "reference/send_slack_notification.html#offline-testing",
    "href": "reference/send_slack_notification.html#offline-testing",
    "title": "send_slack_notification",
    "section": "",
    "text": "If you want to test the function without sending actual notifications, you can leave the webhook_url= as None and set debug=True. This will print the message to the console instead of sending it to Slack. This is useful for debugging and ensuring that your templates are formatted correctly. Furthermore, the function could be run globally (i.e., outside of the context of a validation plan) to show the message templates with all possible variables. Here’s an example of how to do this:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None,  # Leave as None for dry run\n    debug=True,  # Enable debug mode to print message previews\n)\n# Call the function to see the message previews\nnotify_slack()\nThis will print the step and summary message previews to the console, allowing you to see how the templates will look when filled with actual data. You can then adjust your templates as needed before using them in a real validation plan.\nWhen step_msg= and summary_msg= are not provided, the function will use default templates. However, you can customize the templates to include additional information or change the format to better suit your needs. Iterating on the templates can help you create more informative and visually appealing messages. Here’s an example of that:\nimport pointblank as pb\n\n# Create a Slack notification function with custom templates\nnotify_slack = pb.send_slack_notification(\n    webhook_url=None, # Leave as None for dry run\n    step_msg='''*Data Validation Alert*\n    • Type: {type}\n    • Level: {level}\n    • Step: {step}\n    • Column: {column}\n    • Time: {time}''',\n    summary_msg='''*Data Validation Summary*\n    • Highest Severity: {highest_severity}\n    • Total Steps: {n_steps}\n    • Failed Steps: {n_failing_steps}\n    • Time: {time}''',\n    debug=True,  # Enable debug mode to print message previews\n)\nThese templates will be used with sample data when the function is called. The combination of webhook_url=None and debug=True allows you to test your custom templates without having to send actual notifications to Slack."
  },
  {
    "objectID": "reference/send_slack_notification.html#examples",
    "href": "reference/send_slack_notification.html#examples",
    "title": "send_slack_notification",
    "section": "",
    "text": "When using an action with one or more validation steps, you typically provide callables that fire when a matched threshold of failed test units is exceeded. The callable can be a function or a lambda. The send_slack_notification() function creates a callable that sends a Slack notification when the validation step fails. Here is how it can be set up to work for multiple validation steps by using of Actions:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation\nBy placing the notify_slack function in the Validate(actions=Actions(critical=)) argument, you can ensure that the notification is sent whenever the ‘critical’ threshold is reached (as set here, when 15% or more of the test units fail). The notification will include information about the validation step that triggered the alert.\nWhen using a FinalActions object, the notification will be sent after all validation steps have been completed. This is useful for providing a summary of the validation process. Here is an example of how to set up a summary notification:\nimport pointblank as pb\n\n# Create a Slack notification function\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n)\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this case, the same notify_slack function is used, but it is placed in Validate(final_actions=FinalActions()). This results in the summary notification being sent after all validation steps are completed, regardless of whether any steps failed or not.\nThis simplicity is possible because the send_slack_notification() function creates a callable that can be used in both contexts. The function will automatically determine whether to send a step notification or a summary notification based on the context in which it is called.\nWe can customize the message templates for both step and summary notifications. In that way, it’s possible to create a more informative and visually appealing message. For example, we can use Markdown formatting to make the message more readable and visually appealing. Here is an example of how to customize the templates:\nimport pointblank as pb\n# Create a Slack notification function\n\nnotify_slack = pb.send_slack_notification(\n    webhook_url=\"https://hooks.slack.com/services/your/webhook/url\",\n    step_msg='''\n    🚨 *Validation Step Alert*\n    • Step Number: {step}\n    • Column: {column}\n    • Test Type: {type}\n    • Value Tested: {value}\n    • Severity: {level} (level {level_num})\n    • Brief: {autobrief}\n    • Details: {failure_text}\n    • Time: {time}''',\n    summary_msg='''\n    📊 *Validation Summary Report*\n    *Overview*\n    • Status: {highest_severity}\n    • All Passed: {all_passed}\n    • Total Steps: {n_steps}\n\n    *Step Results*\n    • Passing Steps: {n_passing_steps}\n    • Failing Steps: {n_failing_steps}\n    • Warning Level: {n_warning_steps}\n    • Error Level: {n_error_steps}\n    • Critical Level: {n_critical_steps}\n\n    *Table Info*\n    • Table Name: {tbl_name}\n    • Row Count: {tbl_row_count}\n    • Column Count: {tbl_column_count}\n\n    *Timing*\n    • Duration: {validation_duration}s\n    • Completed: {time}''',\n)\n\n# Create a validation plan\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(default=notify_slack),\n        final_actions=pb.FinalActions(notify_slack),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\nIn this example, we have customized the templates for both step and summary notifications. The step notification includes details about the validation step, including the step number, column name, test type, value tested, severity level, brief description, and time of the notification. The summary notification includes an overview of the validation process, including the status, number of steps, passing and failing steps, table information, and timing details."
  },
  {
    "objectID": "reference/contains.html",
    "href": "reference/contains.html",
    "title": "contains",
    "section": "",
    "text": "contains(text, case_sensitive=False)\nSelect columns that contain specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The contains() selector function can be used to select one or more columns that contain some specified text. So if the set of table columns consists of\n[profit, conv_first, conv_last, highest_conv, age]\nand you want to validate columns that have \"conv\" in the name, you can use columns=contains(\"conv\"). This will select the conv_first, conv_last, and highest_conv columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using contains() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/contains.html#parameters",
    "href": "reference/contains.html#parameters",
    "title": "contains",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should contain.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/contains.html#returns",
    "href": "reference/contains.html#returns",
    "title": "contains",
    "section": "Returns",
    "text": "Returns\n\n : Contains\n\nA Contains object, which can be used to select columns that contain the specified text."
  },
  {
    "objectID": "reference/contains.html#relevant-validation-methods-where-contains-can-be-used",
    "href": "reference/contains.html#relevant-validation-methods-where-contains-can-be-used",
    "title": "contains",
    "section": "Relevant Validation Methods where contains() can be Used",
    "text": "Relevant Validation Methods where contains() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe contains() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/contains.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/contains.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "contains",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe contains() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that have the text \"_n\" and start with \"item\", you can use the contains() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(contains(\"_n\") & starts_with(\"item\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/contains.html#examples",
    "href": "reference/contains.html#examples",
    "title": "contains",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, 2021_pay_total, 2022_pay_total, and person_id and we’d like to validate that the values in columns having \"pay\" in the name are greater than 10. We can use the contains() column selector function to specify the column names that contain \"pay\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2021_pay_total\": [16.32, 16.25, 15.75],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.contains(\"pay\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2021_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2022_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2021_pay_total and one for 2022_pay_total. The values in both columns were all greater than 10.\nWe can also use the contains() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that contain \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.contains(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2023_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2024_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay_total and one for 2024_pay_total."
  },
  {
    "objectID": "reference/config.html",
    "href": "reference/config.html",
    "title": "config",
    "section": "",
    "text": "config(\n    report_incl_header=True,\n    report_incl_footer=True,\n    preview_incl_header=True,\n)\nConfiguration settings for the Pointblank library.\n\n\n\nreport_incl_header : bool = True\n\nThis controls whether the header should be present in the validation table report. The header contains the table name, label information, and might contain global failure threshold levels (if set).\n\nreport_incl_footer : bool = True\n\nShould the footer of the validation table report be displayed? The footer contains the starting and ending times of the interrogation.\n\npreview_incl_header : bool = True\n\nWhether the header should be present in any preview table (generated via the preview() function).\n\n\n\n\n\n\n : PointblankConfig\n\nA PointblankConfig object with the specified configuration settings."
  },
  {
    "objectID": "reference/config.html#parameters",
    "href": "reference/config.html#parameters",
    "title": "config",
    "section": "",
    "text": "report_incl_header : bool = True\n\nThis controls whether the header should be present in the validation table report. The header contains the table name, label information, and might contain global failure threshold levels (if set).\n\nreport_incl_footer : bool = True\n\nShould the footer of the validation table report be displayed? The footer contains the starting and ending times of the interrogation.\n\npreview_incl_header : bool = True\n\nWhether the header should be present in any preview table (generated via the preview() function)."
  },
  {
    "objectID": "reference/config.html#returns",
    "href": "reference/config.html#returns",
    "title": "config",
    "section": "",
    "text": ": PointblankConfig\n\nA PointblankConfig object with the specified configuration settings."
  },
  {
    "objectID": "reference/preview.html",
    "href": "reference/preview.html",
    "title": "preview",
    "section": "",
    "text": "preview(\n    data,\n    columns_subset=None,\n    n_head=5,\n    n_tail=5,\n    limit=50,\n    show_row_numbers=True,\n    max_col_width=250,\n    min_tbl_width=500,\n    incl_header=None,\n)\nDisplay a table preview that shows some rows from the top, some from the bottom.\nTo get a quick look at the data in a table, we can use the preview() function to display a preview of the table. The function shows a subset of the rows from the start and end of the table, with the number of rows from the start and end determined by the n_head= and n_tail= parameters (set to 5 by default). This function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.).\nThe view is optimized for readability, with column names and data types displayed in a compact format. The column widths are sized to fit the column names, dtypes, and column content up to a configurable maximum width of max_col_width= pixels. The table can be scrolled horizontally to view even very large datasets. Since the output is a Great Tables (GT) object, it can be further customized using the great_tables API."
  },
  {
    "objectID": "reference/preview.html#parameters",
    "href": "reference/preview.html#parameters",
    "title": "preview",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to preview, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ncolumns_subset : str | list[str] | Column | None = None\n\nThe columns to display in the table, by default None (all columns are shown). This can be a string, a list of strings, a Column object, or a ColumnSelector object. The latter two options allow for more flexible column selection using column selector functions. Errors are raised if the column names provided don’t match any columns in the table (when provided as a string or list of strings) or if column selector expressions don’t resolve to any columns.\n\nn_head : int = 5\n\nThe number of rows to show from the start of the table. Set to 5 by default.\n\nn_tail : int = 5\n\nThe number of rows to show from the end of the table. Set to 5 by default.\n\nlimit : int = 50\n\nThe limit value for the sum of n_head= and n_tail= (the total number of rows shown). If the sum of n_head= and n_tail= exceeds the limit, an error is raised. The default value is 50.\n\nshow_row_numbers : bool = True\n\nShould row numbers be shown? The numbers shown reflect the row numbers of the head and tail in the input data= table. By default, this is set to True.\n\nmax_col_width : int = 250\n\nThe maximum width of the columns (in pixels) before the text is truncated. The default value is 250 (\"250px\").\n\nmin_tbl_width : int = 500\n\nThe minimum width of the table in pixels. If the sum of the column widths is less than this value, the all columns are sized up to reach this minimum width value. The default value is 500 (\"500px\").\n\nincl_header : bool = None\n\nShould the table include a header with the table type and table dimensions? Set to True by default."
  },
  {
    "objectID": "reference/preview.html#returns",
    "href": "reference/preview.html#returns",
    "title": "preview",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the preview of the table."
  },
  {
    "objectID": "reference/preview.html#supported-input-table-types",
    "href": "reference/preview.html#supported-input-table-types",
    "title": "preview",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using preview() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/preview.html#examples",
    "href": "reference/preview.html#examples",
    "title": "preview",
    "section": "Examples",
    "text": "Examples\nIt’s easy to preview a table using the preview() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table_polars = pb.load_dataset(\"small_table\")\n\npb.preview(small_table_polars)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThis table is a Polars DataFrame, but the preview() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nsmall_table_duckdb = pb.load_dataset(\"small_table\", tbl_type=\"duckdb\")\n\npb.preview(small_table_duckdb)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows13Columns8\n  \n\n  \n  date_timetimestamp\n  datedate\n  aint64\n  bstring\n  cint64\n  dfloat64\n  eboolean\n  fstring\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    NULL\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    NULL\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThe blue dividing line marks the end of the first n_head= rows and the start of the last n_tail= rows.\nWe can adjust the number of rows shown from the start and end of the table by setting the n_head= and n_tail= parameters. Let’s enlarge each of these to 10:\n\npb.preview(small_table_polars, n_head=10, n_tail=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIn the above case, the entire dataset is shown since the sum of n_head= and n_tail= is greater than the number of rows in the table (which is 13).\nThe columns_subset= parameter can be used to show only specific columns in the table. You can provide a list of column names to make the selection. Let’s try that with the \"game_revenue\" dataset as a Pandas DataFrame:\n\ngame_revenue_pandas = pb.load_dataset(\"game_revenue\", tbl_type=\"pandas\")\n\npb.preview(game_revenue_pandas, columns_subset=[\"player_id\", \"item_name\", \"item_revenue\"])\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  player_idobject\n  item_nameobject\n  item_revenuefloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    offer2\n    8.99\n  \n  \n    2\n    ECPANOIXLZHF896\n    gems3\n    22.49\n  \n  \n    3\n    ECPANOIXLZHF896\n    gold7\n    107.99\n  \n  \n    4\n    ECPANOIXLZHF896\n    ad_20sec\n    0.76\n  \n  \n    5\n    ECPANOIXLZHF896\n    ad_5sec\n    0.03\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    ad_survey\n    1.332\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    ad_survey\n    1.35\n  \n  \n    1998\n    RMOSWHJGELCI675\n    ad_5sec\n    0.03\n  \n  \n    1999\n    RMOSWHJGELCI675\n    offer5\n    26.09\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    ad_5sec\n    0.12\n  \n\n\n\n\n\n\n        \n\n\nAlternatively, we can use column selector functions like starts_with() and matches()` to select columns based on text or patterns:\n\npb.preview(game_revenue_pandas, n_head=2, n_tail=2, columns_subset=pb.starts_with(\"session\"))\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  session_idobject\n  session_startdatetime64[ns, UTC]\n  session_durationfloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    16.3\n  \n  \n    2\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    16.3\n  \n  \n    1999\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    8.4\n  \n  \n    2000\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    18.5\n  \n\n\n\n\n\n\n        \n\n\nMultiple column selector functions can be combined within col() using operators like | and &:\n\npb.preview(\n  game_revenue_pandas,\n  n_head=2,\n  n_tail=2,\n  columns_subset=pb.col(pb.starts_with(\"item\") | pb.matches(\"player\"))\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  player_idobject\n  item_typeobject\n  item_nameobject\n  item_revenuefloat64\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    iap\n    offer2\n    8.99\n  \n  \n    2\n    ECPANOIXLZHF896\n    iap\n    gems3\n    22.49\n  \n  \n    1999\n    RMOSWHJGELCI675\n    iap\n    offer5\n    26.09\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    ad\n    ad_5sec\n    0.12"
  },
  {
    "objectID": "reference/Actions.html",
    "href": "reference/Actions.html",
    "title": "Actions",
    "section": "",
    "text": "Actions(\n    self,\n    warning=None,\n    error=None,\n    critical=None,\n    default=None,\n    highest_only=True,\n)\nDefinition of action values.\nActions complement threshold values by defining what action should be taken when a threshold level is reached. The action can be a string or a Callable. When a string is used, it is interpreted as a message to be displayed. When a Callable is used, it will be invoked at interrogation time if the threshold level is met or exceeded.\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. These levels correspond to different levels of severity when a threshold is reached. Those thresholds can be defined using the Thresholds class or various shorthand forms. Actions don’t have to be defined for all threshold levels; if an action is not defined for a level in exceedance, no action will be taken. Likewise, there is no negative consequence (other than a no-op) for defining actions for thresholds that don’t exist (e.g., setting an action for the ‘critical’ level when no corresponding ‘critical’ threshold has been set)."
  },
  {
    "objectID": "reference/Actions.html#parameters",
    "href": "reference/Actions.html#parameters",
    "title": "Actions",
    "section": "Parameters",
    "text": "Parameters\n\nwarning : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘warning’ level. Using None means no action should be performed at the ‘warning’ level.\n\nerror : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘error’ level. Using None means no action should be performed at the ‘error’ level.\n\ncritical : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for the ‘critical’ level. Using None means no action should be performed at the ‘critical’ level.\n\ndefault : str | Callable | list[str | Callable] | None = None\n\nA string, Callable, or list of Callable/string values for all threshold levels. This parameter can be used to set the same action for all threshold levels. If an action is defined for a specific threshold level, it will override the action set for all levels.\n\nhighest_only : bool = True\n\nA boolean value that, when set to True (the default), results in executing only the action for the highest threshold level that is exceeded. Useful when you want to ensure that only the most severe action is taken when multiple threshold levels are exceeded."
  },
  {
    "objectID": "reference/Actions.html#returns",
    "href": "reference/Actions.html#returns",
    "title": "Actions",
    "section": "Returns",
    "text": "Returns\n\n : Actions\n\nAn Actions object. This can be used when using the Validate class (to set actions for meeting different threshold levels globally) or when defining validation steps like col_vals_gt() (so that actions are scoped to individual validation steps, overriding any globally set actions)."
  },
  {
    "objectID": "reference/Actions.html#types-of-actions",
    "href": "reference/Actions.html#types-of-actions",
    "title": "Actions",
    "section": "Types of Actions",
    "text": "Types of Actions\nActions can be defined in different ways:\n\nString: A message to be displayed when the threshold level is met or exceeded.\nCallable: A function that is called when the threshold level is met or exceeded.\nList of Strings/Callables: Multiple messages or functions to be called when the threshold level is met or exceeded.\n\nThe actions are executed at interrogation time when the threshold level assigned to the action is exceeded by the number or proportion of failing test units. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of interrogation. If providing a list of strings or callables, each item in the list will be executed in order. Such a list can contain a mix of strings and callables."
  },
  {
    "objectID": "reference/Actions.html#string-templating",
    "href": "reference/Actions.html#string-templating",
    "title": "Actions",
    "section": "String Templating",
    "text": "String Templating\nWhen using a string as an action, you can include placeholders for the following variables:\n\n{type}: The validation step type where the action is executed (e.g., ‘col_vals_gt’, ‘col_vals_lt’, etc.)\n{level}: The threshold level where the action is executed (‘warning’, ‘error’, or ‘critical’)\n{step} or {i}: The step number in the validation workflow where the action is executed\n{col} or {column}: The column name where the action is executed\n{val} or {value}: An associated value for the validation method (e.g., the value to compare against in a ‘col_vals_gt’ validation step)\n{time}: A datetime value for when the action was executed\n\nThe first two placeholders can also be used in uppercase (e.g., {TYPE} or {LEVEL}) and the corresponding values will be displayed in uppercase. The placeholders are replaced with the actual values during interrogation.\nFor example, the string \"{LEVEL}: '{type}' threshold exceeded for column {col}.\" will be displayed as \"WARNING: 'col_vals_gt' threshold exceeded for column a.\" when the ‘warning’ threshold is exceeded in a ‘col_vals_gt’ validation step involving column a."
  },
  {
    "objectID": "reference/Actions.html#crafting-callables-with-get_action_metadata",
    "href": "reference/Actions.html#crafting-callables-with-get_action_metadata",
    "title": "Actions",
    "section": "Crafting Callables with get_action_metadata()",
    "text": "Crafting Callables with get_action_metadata()\nWhen creating a callable function to be used as an action, you can use the get_action_metadata() function to retrieve metadata about the step where the action is executed. This metadata contains information about the validation step, including the step type, level, step number, column name, and associated value. You can use this information to craft your action message or to take specific actions based on the metadata provided."
  },
  {
    "objectID": "reference/Actions.html#examples",
    "href": "reference/Actions.html#examples",
    "title": "Actions",
    "section": "Examples",
    "text": "Examples\nLet’s define both threshold values and actions for a data validation workflow. We’ll set these thresholds and actions globally for all validation steps. In this specific example, the only actions we’ll define are for the ‘critical’ level:\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(critical=\"Major data quality issue found in step {step}.\"),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(columns=\"session_duration\", value=15)\n    .interrogate()\n)\n\nvalidation\n\nMajor data quality issue found in step 3.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:00:05DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nBecause we set the ‘critical’ action to display \"Major data quality issue found.\" in the console, this message will be displayed if the number of failing test units exceeds the ‘critical’ threshold (set to 15% of the total number of test units). In step 3 of the validation workflow, the ‘critical’ threshold is exceeded, so the message is displayed in the console.\nActions can be defined locally for individual validation steps, which will override any global actions set at the beginning of the validation workflow. Here’s a variation of the above example where we set global threshold values but assign an action only for an individual validation step:\n\ndef dq_issue():\n    from datetime import datetime\n\n    print(f\"Data quality issue found ({datetime.now()}).\")\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n        actions=pb.Actions(warning=dq_issue),\n    )\n    .interrogate()\n)\n\nvalidation\n\nData quality issue found (2025-04-23 01:00:05.822440).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:00:05DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the ‘warning’ action is set to call the dq_issue() function. This action is only executed when the ‘warning’ threshold is exceeded in the ‘session_duration’ column. Because all three thresholds are exceeded in step 3, the ‘warning’ action of executing the function occurs (resulting in a message being printed to the console). If actions were set for the other two threshold levels, they would also be executed."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html",
    "href": "reference/Validate.get_tabular_report.html",
    "title": "Validate.get_tabular_report",
    "section": "",
    "text": "Validate.get_tabular_report(\n    title=':default:',\n    incl_header=None,\n    incl_footer=None,\n)\nValidation report as a GT table.\nThe get_tabular_report() method returns a GT table object that represents the validation report. This validation table provides a summary of the validation results, including the validation steps, the number of test units, the number of failing test units, and the fraction of failing test units. The table also includes status indicators for the ‘warning’, ‘error’, and ‘critical’ levels.\nYou could simply display the validation table without the use of the get_tabular_report() method. However, the method provides a way to customize the title of the report. In the future this method may provide additional options for customizing the report."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#parameters",
    "href": "reference/Validate.get_tabular_report.html#parameters",
    "title": "Validate.get_tabular_report",
    "section": "Parameters",
    "text": "Parameters\n\ntitle : str | None = ':default:'\n\nOptions for customizing the title of the report. The default is the \":default:\" value which produces a generic title. Another option is \":tbl_name:\", and that presents the name of the table as the title for the report. If no title is wanted, then \":none:\" can be used. Aside from keyword options, text can be provided for the title. This will be interpreted as Markdown text and transformed internally to HTML."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#returns",
    "href": "reference/Validate.get_tabular_report.html#returns",
    "title": "Validate.get_tabular_report",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT table object that represents the validation report."
  },
  {
    "objectID": "reference/Validate.get_tabular_report.html#examples",
    "href": "reference/Validate.get_tabular_report.html#examples",
    "title": "Validate.get_tabular_report",
    "section": "Examples",
    "text": "Examples\nLet’s create a Validate object with a few validation steps and then interrogate the data table to see how it performs against the validation plan. We can then generate a tabular report to get a summary of the results.\n\nimport pointblank as pb\nimport polars as pl\n\n# Create a Polars DataFrame\ntbl_pl = pl.DataFrame({\"x\": [1, 2, 3, 4], \"y\": [4, 5, 6, 7]})\n\n# Validate data using Polars DataFrame\nvalidation = (\n    pb.Validate(data=tbl_pl, tbl_name=\"tbl_xy\", thresholds=(2, 3, 4))\n    .col_vals_gt(columns=\"x\", value=1)\n    .col_vals_lt(columns=\"x\", value=3)\n    .col_vals_le(columns=\"y\", value=7)\n    .interrogate()\n)\n\n# Look at the validation table\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|01:00:00Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 01:00:00 UTC&lt; 1 s2025-04-23 01:00:00 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe validation table is displayed with a default title (‘Validation Report’). We can use the get_tabular_report() method to customize the title of the report. For example, we can set the title to the name of the table by using the title=\":tbl_name:\" option. This will use the string provided in the tbl_name= argument of the Validate object.\n\nvalidation.get_tabular_report(title=\":tbl_name:\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    tbl_xy\n  \n  \n    2025-04-23|01:00:00Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 01:00:00 UTC&lt; 1 s2025-04-23 01:00:00 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe title of the report is now set to the name of the table, which is ‘tbl_xy’. This can be useful if you have multiple tables and want to keep track of which table the validation report is for.\nAlternatively, you can provide your own title for the report.\n\nvalidation.get_tabular_report(title=\"Report for Table XY\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Table XY\n\n  \n  \n    2025-04-23|01:00:00Polarstbl_xyWARNING2ERROR3CRITICAL4\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    x\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    30.75\n    10.25\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    x\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    y\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 01:00:00 UTC&lt; 1 s2025-04-23 01:00:00 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe title of the report is now set to ‘Report for Table XY’. This can be useful if you want to provide a more descriptive title for the report."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html",
    "href": "reference/Validate.col_vals_in_set.html",
    "title": "Validate.col_vals_in_set",
    "section": "",
    "text": "Validate.col_vals_in_set(\n    columns,\n    set,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values are in a set of values.\nThe col_vals_in_set() validation method checks whether column values in a table are part of a specified set= of values. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#parameters",
    "href": "reference/Validate.col_vals_in_set.html#parameters",
    "title": "Validate.col_vals_in_set",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nset : Collection[Any]\n\nA list of values to compare against.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#returns",
    "href": "reference/Validate.col_vals_in_set.html#returns",
    "title": "Validate.col_vals_in_set",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#preprocessing",
    "href": "reference/Validate.col_vals_in_set.html#preprocessing",
    "title": "Validate.col_vals_in_set",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#thresholds",
    "href": "reference/Validate.col_vals_in_set.html#thresholds",
    "title": "Validate.col_vals_in_set",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_in_set.html#examples",
    "href": "reference/Validate.col_vals_in_set.html#examples",
    "title": "Validate.col_vals_in_set",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 2, 4, 6, 2, 5],\n        \"b\": [5, 8, 2, 6, 5, 1],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    2\n    8\n  \n  \n    3\n    4\n    2\n  \n  \n    4\n    6\n    6\n  \n  \n    5\n    2\n    5\n  \n  \n    6\n    5\n    1\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all in the set of [2, 3, 4, 5, 6]. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_in_set(columns=\"a\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    a\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_in_set(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_in_set(columns=\"b\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    b\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the column b values of 8 and 1, which are not in the set of [2, 3, 4, 5, 6]."
  },
  {
    "objectID": "reference/ends_with.html",
    "href": "reference/ends_with.html",
    "title": "ends_with",
    "section": "",
    "text": "ends_with(text, case_sensitive=False)\nSelect columns that end with specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The ends_with() selector function can be used to select one or more columns that end with some specified text. So if the set of table columns consists of\n[first_name, last_name, age, address]\nand you want to validate columns that end with \"name\", you can use columns=ends_with(\"name\"). This will select the first_name and last_name columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using ends_with() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/ends_with.html#parameters",
    "href": "reference/ends_with.html#parameters",
    "title": "ends_with",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should end with.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/ends_with.html#returns",
    "href": "reference/ends_with.html#returns",
    "title": "ends_with",
    "section": "Returns",
    "text": "Returns\n\n : EndsWith\n\nAn EndsWith object, which can be used to select columns that end with the specified text."
  },
  {
    "objectID": "reference/ends_with.html#relevant-validation-methods-where-ends_with-can-be-used",
    "href": "reference/ends_with.html#relevant-validation-methods-where-ends_with-can-be-used",
    "title": "ends_with",
    "section": "Relevant Validation Methods where ends_with() can be Used",
    "text": "Relevant Validation Methods where ends_with() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe ends_with() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/ends_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/ends_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "ends_with",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe ends_with() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that end with \"e\" and start with \"a\", you can use the ends_with() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(ends_with(\"e\") & starts_with(\"a\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/ends_with.html#examples",
    "href": "reference/ends_with.html#examples",
    "title": "ends_with",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, 2021_pay, 2022_pay, and person_id and we’d like to validate that the values in columns that end with \"pay\" are greater than 10. We can use the ends_with() column selector function to specify the columns that end with \"pay\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2021_pay\": [16.32, 16.25, 15.75],\n        \"2022_pay\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.ends_with(\"pay\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2021_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2022_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2021_pay and one for 2022_pay. The values in both columns were all greater than 10.\nWe can also use the ends_with() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that end with \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay\": [18.62, 16.95, 18.25],\n        \"2023_pay\": [19.29, 17.75, 18.35],\n        \"2024_pay\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.ends_with(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2023_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2024_pay\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay and one for 2024_pay."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html",
    "href": "reference/Validate.col_vals_not_in_set.html",
    "title": "Validate.col_vals_not_in_set",
    "section": "",
    "text": "Validate.col_vals_not_in_set(\n    columns,\n    set,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values are not in a set of values.\nThe col_vals_not_in_set() validation method checks whether column values in a table are not part of a specified set= of values. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#parameters",
    "href": "reference/Validate.col_vals_not_in_set.html#parameters",
    "title": "Validate.col_vals_not_in_set",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nset : list[float | int]\n\nA list of values to compare against.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#returns",
    "href": "reference/Validate.col_vals_not_in_set.html#returns",
    "title": "Validate.col_vals_not_in_set",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#preprocessing",
    "href": "reference/Validate.col_vals_not_in_set.html#preprocessing",
    "title": "Validate.col_vals_not_in_set",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#thresholds",
    "href": "reference/Validate.col_vals_not_in_set.html#thresholds",
    "title": "Validate.col_vals_not_in_set",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_in_set.html#examples",
    "href": "reference/Validate.col_vals_not_in_set.html#examples",
    "title": "Validate.col_vals_not_in_set",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 8, 1, 9, 1, 7],\n        \"b\": [1, 8, 2, 6, 9, 1],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    7\n    1\n  \n  \n    2\n    8\n    8\n  \n  \n    3\n    1\n    2\n  \n  \n    4\n    9\n    6\n  \n  \n    5\n    1\n    9\n  \n  \n    6\n    7\n    1\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that none of the values in column a are in the set of [2, 3, 4, 5, 6]. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_in_set(columns=\"a\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    a\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_not_in_set(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_in_set(columns=\"b\", set=[2, 3, 4, 5, 6])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    b\n    2, 3, 4, 5, 6\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the column b values of 2 and 6, both of which are in the set of [2, 3, 4, 5, 6]."
  },
  {
    "objectID": "reference/Validate.f_failed.html",
    "href": "reference/Validate.f_failed.html",
    "title": "Validate.f_failed",
    "section": "",
    "text": "Validate.f_failed(i=None, scalar=False)\nProvides a dictionary of the fraction of test units that failed for each validation step.\nA measure of the fraction of test units that failed is provided by the f_failed attribute. This is the fraction of test units that failed the validation step over the total number of test units. Given this is a fractional value, it will always be in the range of 0 to 1.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThis method provides a dictionary of the fraction of failing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the f_passed() method (i.e., 1 - f_passed())."
  },
  {
    "objectID": "reference/Validate.f_failed.html#parameters",
    "href": "reference/Validate.f_failed.html#parameters",
    "title": "Validate.f_failed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the fraction of failing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.f_failed.html#returns",
    "href": "reference/Validate.f_failed.html#returns",
    "title": "Validate.f_failed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, float] | float\n\nA dictionary of the fraction of failing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.f_failed.html#examples",
    "href": "reference/Validate.f_failed.html#examples",
    "title": "Validate.f_failed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, all having some failing test units. After interrogation, the f_failed() method is used to determine the fraction of failing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\", \"d\", \"c\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.f_failed()\n\n{1: 0.2857142857142857, 2: 0.42857142857142855, 3: 0.42857142857142855}\n\n\nThe returned dictionary shows the fraction of failing test units for each validation step. The values are all greater than 0 since there were failing test units in each step.\nIf we wanted to check the fraction of failing test units for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.f_failed(i=1)\n\n{1: 0.2857142857142857}\n\n\nThe returned value is the proportion of failing test units for the first validation step (2 failing test units out of 7 total test units)."
  },
  {
    "objectID": "reference/Validate.n.html",
    "href": "reference/Validate.n.html",
    "title": "Validate.n",
    "section": "",
    "text": "Validate.n(i=None, scalar=False)\nProvides a dictionary of the number of test units for each validation step.\nThe n() method provides the number of test units for each validation step. This is the total number of test units that were evaluated in the validation step. It is always an integer value.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. The total number of test units for a validation step is the sum of the number of passing and failing test units (i.e., n = n_passed + n_failed)."
  },
  {
    "objectID": "reference/Validate.n.html#parameters",
    "href": "reference/Validate.n.html#parameters",
    "title": "Validate.n",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n.html#returns",
    "href": "reference/Validate.n.html#returns",
    "title": "Validate.n",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n.html#examples",
    "href": "reference/Validate.n.html#examples",
    "title": "Validate.n",
    "section": "Examples",
    "text": "Examples\nDifferent types of validation steps can have different numbers of test units. In the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the number of test units for each step will be a little bit different.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 9, 5],\n        \"b\": [5, 6, 10, 3],\n        \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_exists(columns=\"b\")\n    .col_vals_lt(columns=\"b\", value=9, pre=lambda df: df.filter(pl.col(\"a\") &gt; 1))\n    .interrogate()\n)\n\nThe first validation step checks that all values in column a are greater than 0. Let’s use the n() method to determine the number of test units this validation step.\n\nvalidation.n(i=1, scalar=True)\n\n4\n\n\nThe returned value of 4 is the number of test units for the first validation step. This value is the same as the number of rows in the table.\nThe second validation step checks for the existence of column b. Using the n() method we can get the number of test units for this the second step.\n\nvalidation.n(i=2, scalar=True)\n\n1\n\n\nThere’s a single test unit here because the validation step is checking for the presence of a single column.\nThe third validation step checks that all values in column b are less than 9 after filtering the table to only include rows where the value in column a is greater than 1. Because the table is filtered, the number of test units will be less than the total number of rows in the input table. Let’s prove this by using the n() method.\n\nvalidation.n(i=3, scalar=True)\n\n3\n\n\nThe returned value of 3 is the number of test units for the third validation step. When using the pre= argument, the input table can be mutated before performing the validation. The n() method is a good way to determine whether the mutation performed as expected.\nIn all of these examples, the scalar=True argument was used to return the value as a scalar integer value. If scalar=False, the method will return a dictionary with an entry for the validation step number (from the i= argument) and the number of test units. Futhermore, leaving out the i= argument altogether will return a dictionary with filled with the number of test units for each validation step. Here’s what that looks like:\n\nvalidation.n()\n\n{1: 4, 2: 1, 3: 3}"
  },
  {
    "objectID": "reference/Thresholds.html",
    "href": "reference/Thresholds.html",
    "title": "Thresholds",
    "section": "",
    "text": "Thresholds(self, warning=None, error=None, critical=None)\nDefinition of threshold values.\nThresholds are used to set limits on the number of failing test units at different levels. The levels are ‘warning’, ‘error’, and ‘critical’. These levels correspond to different levels of severity when a threshold is reached. The threshold values can be set as absolute counts or as fractions of the total number of test units. When a threshold is reached, an action can be taken (e.g., displaying a message or calling a function) if there is an associated action defined for that level (defined through the Actions class)."
  },
  {
    "objectID": "reference/Thresholds.html#parameters",
    "href": "reference/Thresholds.html#parameters",
    "title": "Thresholds",
    "section": "Parameters",
    "text": "Parameters\n\nwarning : int | float | bool | None = None\n\nThe threshold for the ‘warning’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1.\n\nerror : int | float | bool | None = None\n\nThe threshold for the ‘error’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1.\n\ncritical : int | float | bool | None = None\n\nThe threshold for the ‘critical’ level. This can be an absolute count or a fraction of the total. Using True will set this threshold value to 1."
  },
  {
    "objectID": "reference/Thresholds.html#returns",
    "href": "reference/Thresholds.html#returns",
    "title": "Thresholds",
    "section": "Returns",
    "text": "Returns\n\n : Thresholds\n\nA Thresholds object. This can be used when using the Validate class (to set thresholds globally) or when defining validation steps like col_vals_gt() (so that threshold values are scoped to individual validation steps, overriding any global thresholds)."
  },
  {
    "objectID": "reference/Thresholds.html#examples",
    "href": "reference/Thresholds.html#examples",
    "title": "Thresholds",
    "section": "Examples",
    "text": "Examples\nIn a data validation workflow, you can set thresholds for the number of failing test units at different levels. For example, you can set a threshold for the ‘warning’ level when the number of failing test units exceeds 10% of the total number of test units:\n\nthresholds_1 = pb.Thresholds(warning=0.1)\n\nYou can also set thresholds for the ‘error’ and ‘critical’ levels:\n\nthresholds_2 = pb.Thresholds(warning=0.1, error=0.2, critical=0.05)\n\nThresholds can also be set as absolute counts. Here’s an example where the ‘warning’ level is set to 5 failing test units:\n\nthresholds_3 = pb.Thresholds(warning=5)\n\nThe thresholds object can be used to set global thresholds for all validation steps. Or, you can set thresholds for individual validation steps, which will override the global thresholds. Here’s a data validation workflow example where we set global thresholds and then override with different thresholds at the col_vals_gt() step:\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        label=\"Example Validation\",\n        thresholds=pb.Thresholds(warning=0.1, error=0.2, critical=0.3)\n    )\n    .col_vals_not_null(columns=[\"c\", \"d\"])\n    .col_vals_gt(columns=\"a\", value=3, thresholds=pb.Thresholds(warning=5))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolarsWARNING0.1ERROR0.2CRITICAL0.3\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    d\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #AAAAAA\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    ●\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nAs can be seen, the last step (col_vals_gt()) has its own thresholds, which override the global thresholds set at the beginning of the validation workflow (in the Validate class)."
  },
  {
    "objectID": "reference/Validate.f_passed.html",
    "href": "reference/Validate.f_passed.html",
    "title": "Validate.f_passed",
    "section": "",
    "text": "Validate.f_passed(i=None, scalar=False)\nProvides a dictionary of the fraction of test units that passed for each validation step.\nA measure of the fraction of test units that passed is provided by the f_passed attribute. This is the fraction of test units that passed the validation step over the total number of test units. Given this is a fractional value, it will always be in the range of 0 to 1.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThis method provides a dictionary of the fraction of passing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the f_failed() method (i.e., 1 - f_failed())."
  },
  {
    "objectID": "reference/Validate.f_passed.html#parameters",
    "href": "reference/Validate.f_passed.html#parameters",
    "title": "Validate.f_passed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the fraction of passing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.f_passed.html#returns",
    "href": "reference/Validate.f_passed.html#returns",
    "title": "Validate.f_passed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, float] | float\n\nA dictionary of the fraction of passing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.f_passed.html#examples",
    "href": "reference/Validate.f_passed.html#examples",
    "title": "Validate.f_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, all having some failing test units. After interrogation, the f_passed() method is used to determine the fraction of passing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\", \"d\", \"c\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.f_passed()\n\n{1: 0.7142857142857143, 2: 0.5714285714285714, 3: 0.5714285714285714}\n\n\nThe returned dictionary shows the fraction of passing test units for each validation step. The values are all less than 1 since there were failing test units in each step.\nIf we wanted to check the fraction of passing test units for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.f_passed(i=1)\n\n{1: 0.7142857142857143}\n\n\nThe returned value is the proportion of passing test units for the first validation step (5 passing test units out of 7 total test units)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html",
    "href": "reference/Validate.col_vals_ne.html",
    "title": "Validate.col_vals_ne",
    "section": "",
    "text": "Validate.col_vals_ne(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data not equal to a fixed value or data in another column?\nThe col_vals_ne() validation method checks whether column values in a table are not equal to a specified value= (the exact comparison used in this function is col_val != value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#parameters",
    "href": "reference/Validate.col_vals_ne.html#parameters",
    "title": "Validate.col_vals_ne",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#returns",
    "href": "reference/Validate.col_vals_ne.html#returns",
    "title": "Validate.col_vals_ne",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_ne.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_ne",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#preprocessing",
    "href": "reference/Validate.col_vals_ne.html#preprocessing",
    "title": "Validate.col_vals_ne",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#thresholds",
    "href": "reference/Validate.col_vals_ne.html#thresholds",
    "title": "Validate.col_vals_ne",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_ne.html#examples",
    "href": "reference/Validate.col_vals_ne.html#examples",
    "title": "Validate.col_vals_ne",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 5, 5, 5, 5, 5],\n        \"b\": [5, 6, 3, 6, 5, 8],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    5\n    6\n  \n  \n    3\n    5\n    3\n  \n  \n    4\n    5\n    6\n  \n  \n    5\n    5\n    5\n  \n  \n    6\n    5\n    8\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are not equal to the value of 3. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ne(columns=\"a\", value=3)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_equal\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ne()\n        \n        \n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_ne(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_ne() to check whether the values in column a aren’t equal to the values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ne(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_equal\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ne()\n        \n        \n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are in rows 0 and 4, where a is 5 and b is 5 in both cases (i.e., they are equal to each other)."
  },
  {
    "objectID": "reference/get_action_metadata.html",
    "href": "reference/get_action_metadata.html",
    "title": "get_action_metadata",
    "section": "",
    "text": "get_action_metadata()\nAccess step-level metadata when authoring custom actions.\nGet the metadata for the validation step where an action was triggered. This can be called by user functions to get the metadata for the current action."
  },
  {
    "objectID": "reference/get_action_metadata.html#returns",
    "href": "reference/get_action_metadata.html#returns",
    "title": "get_action_metadata",
    "section": "Returns",
    "text": "Returns\n\n : dict\n\nA dictionary containing the metadata for the current step."
  },
  {
    "objectID": "reference/get_action_metadata.html#description-of-the-metadata-fields",
    "href": "reference/get_action_metadata.html#description-of-the-metadata-fields",
    "title": "get_action_metadata",
    "section": "Description of the Metadata Fields",
    "text": "Description of the Metadata Fields\nThe metadata dictionary contains the following fields for a given validation step:\n\nstep: The step number.\ncolumn: The column name.\nvalue: The value being compared (only available in certain validation steps).\ntype: The assertion type (e.g., \"col_vals_gt\", etc.).\ntime: The time the validation step was executed (in ISO format).\nlevel: The severity level (\"warning\", \"error\", or \"critical\").\nlevel_num: The severity level as a numeric value (30, 40, or 50).\nautobrief: A localized and brief statement of the expectation for the step.\nfailure_text: Localized text that explains how the validation step failed."
  },
  {
    "objectID": "reference/get_action_metadata.html#examples",
    "href": "reference/get_action_metadata.html#examples",
    "title": "get_action_metadata",
    "section": "Examples",
    "text": "Examples\nWhen creating a custom action, you can access the metadata for the current step using the get_action_metadata() function. Here’s an example of a custom action that logs the metadata for the current step:\n\nimport pointblank as pb\n\ndef log_issue():\n    metadata = pb.get_action_metadata()\n    print(f\"Type: {metadata['type']}, Step: {metadata['step']}\")\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(warning=0.05, error=0.10, critical=0.15),\n        actions=pb.Actions(warning=log_issue),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=15,\n    )\n    .interrogate()\n)\n\nvalidation\n\nType: col_vals_gt, Step: 2\nType: col_vals_gt, Step: 3\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:59:06DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    16750.84\n    3250.16\n    ●\n    ●\n    ●\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:59:06 UTC&lt; 1 s2025-04-23 00:59:06 UTC\n  \n\n\n\n\n\n\n        \n\n\nKey pieces to note in the above example:\n\nlog_issue() (the custom action) collects metadata by calling get_action_metadata()\nthe metadata is a dictionary that is used to craft the log message\nthe action is passed as a bare function to the Actions object within the Validate object (placing it within Validate(actions=) ensures it’s set as an action for every validation step)"
  },
  {
    "objectID": "reference/get_row_count.html",
    "href": "reference/get_row_count.html",
    "title": "get_row_count",
    "section": "",
    "text": "get_row_count(data)\nGet the number of rows in a table.\nThe get_row_count() function returns the number of rows in a table. The function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.)."
  },
  {
    "objectID": "reference/get_row_count.html#parameters",
    "href": "reference/get_row_count.html#parameters",
    "title": "get_row_count",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to get the row count, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/get_row_count.html#returns",
    "href": "reference/get_row_count.html#returns",
    "title": "get_row_count",
    "section": "Returns",
    "text": "Returns\n\n : int\n\nThe number of rows in the table."
  },
  {
    "objectID": "reference/get_row_count.html#supported-input-table-types",
    "href": "reference/get_row_count.html#supported-input-table-types",
    "title": "get_row_count",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using get_row_count() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/get_row_count.html#examples",
    "href": "reference/get_row_count.html#examples",
    "title": "get_row_count",
    "section": "Examples",
    "text": "Examples\nGetting the number of rows in a table is easily done by using the get_row_count() function. Here’s an example using the game_revenue dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\ngame_revenue_polars = pb.load_dataset(\"game_revenue\")\n\npb.get_row_count(game_revenue_polars)\n\n2000\n\n\nThis table is a Polars DataFrame, but the get_row_count() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\ngame_revenue_duckdb = pb.load_dataset(\"game_revenue\", tbl_type=\"duckdb\")\n\npb.get_row_count(game_revenue_duckdb)\n\n2000\n\n\nThe function always returns the number of rows in the table as an integer value, which is 2000 for the game_revenue dataset."
  },
  {
    "objectID": "reference/col.html",
    "href": "reference/col.html",
    "title": "col",
    "section": "",
    "text": "col(exprs)\nHelper function for referencing a column in the input table.\nMany of the validation methods (i.e., col_vals_*() methods) in Pointblank have a value= argument. These validations are comparisons between column values and a literal value, or, between column values and adjacent values in another column. The col() helper function is used to specify that it is a column being referenced, not a literal value.\nThe col() doesn’t check that the column exists in the input table. It acts to signal that the value being compared is a column value. During validation (i.e., when interrogate() is called), Pointblank will then check that the column exists in the input table.\nFor creating expressions to use with the conjointly() validation method, use the expr_col() function instead."
  },
  {
    "objectID": "reference/col.html#parameters",
    "href": "reference/col.html#parameters",
    "title": "col",
    "section": "Parameters",
    "text": "Parameters\n\nexprs : str | ColumnSelector | ColumnSelectorNarwhals\n\nEither the name of a single column in the target table, provided as a string, or, an expression involving column selector functions (e.g., starts_with(\"a\"), ends_with(\"e\") \\| starts_with(\"a\"), etc.)."
  },
  {
    "objectID": "reference/col.html#returns",
    "href": "reference/col.html#returns",
    "title": "col",
    "section": "Returns",
    "text": "Returns\n\n : Column | ColumnLiteral | ColumnSelectorNarwhals:\n\nA column object or expression representing the column reference."
  },
  {
    "objectID": "reference/col.html#usage-with-the-columns-argument",
    "href": "reference/col.html#usage-with-the-columns-argument",
    "title": "col",
    "section": "Usage with the columns= Argument",
    "text": "Usage with the columns= Argument\nThe col() function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nIf specifying a single column with certainty (you have the exact name), col() is not necessary since you can just pass the column name as a string (though it is still valid to use col(\"column_name\"), if preferred). However, if you want to select columns based on complex logic involving multiple column selector functions (e.g., columns that start with \"a\" but don’t end with \"e\"), you need to use col() to wrap expressions involving column selector functions and logical operators such as &, |, -, and ~.\nHere is an example of such usage with the col_vals_gt() validation method:\ncol_vals_gt(columns=col(starts_with(\"a\") & ~ends_with(\"e\")), value=10)\nIf using only a single column selector function, you can pass the function directly to the columns= argument of the validation method, or, you can use col() to wrap the function (either is valid though the first is more concise). Here is an example of that simpler usage:\ncol_vals_gt(columns=starts_with(\"a\"), value=10)"
  },
  {
    "objectID": "reference/col.html#usage-with-the-value-left-and-right-arguments",
    "href": "reference/col.html#usage-with-the-value-left-and-right-arguments",
    "title": "col",
    "section": "Usage with the value=, left=, and right= Arguments",
    "text": "Usage with the value=, left=, and right= Arguments\nThe col() function can be used in the value= argument of the following validation methods\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\n\nand in the left= and right= arguments (either or both) of these two validation methods\n\ncol_vals_between()\ncol_vals_outside()\n\nYou cannot use column selector functions such as starts_with() in either of the value=, left=, or right= arguments since there would be no guarantee that a single column will be resolved from the target table with this approach. The col() function is used to signal that the value being compared is a column value and not a literal value."
  },
  {
    "objectID": "reference/col.html#available-selectors",
    "href": "reference/col.html#available-selectors",
    "title": "col",
    "section": "Available Selectors",
    "text": "Available Selectors\nThere is a collection of selectors available in pointblank, allowing you to select columns based on attributes of column names and positions. The selectors are:\n\nstarts_with()\nends_with()\ncontains()\nmatches()\neverything()\nfirst_n()\nlast_n()\n\nAlternatively, we support selectors from the Narwhals library! Those selectors can additionally take advantage of the data types of the columns. The selectors are:\n\nboolean()\nby_dtype()\ncategorical()\nmatches()\nnumeric()\nstring()\n\nHave a look at the Narwhals API documentation on selectors for more information."
  },
  {
    "objectID": "reference/col.html#examples",
    "href": "reference/col.html#examples",
    "title": "col",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns a and b and we’d like to validate that the values in column a are greater than the values in column b. We can use the col() helper function to reference the comparison column when creating the validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 6, 5],\n        \"b\": [4, 2, 3, 3, 4, 3],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom results of the validation table it can be seen that values in a were greater than values in b for every row (or test unit). Using value=pb.col(\"b\") specified that the greater-than comparison is across columns, not with a fixed literal value.\nIf you want to select an arbitrary set of columns upon which to base a validation, you can use column selector functions (e.g., starts_with(), ends_with(), etc.) to specify columns in the columns= argument of a validation method. Let’s use the starts_with() column selector function to select columns that start with \"paid\" and validate that the values in those columns are greater than 10.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [16.32, 16.25, 15.75],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.starts_with(\"paid\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the starts_with() column selector function. This is not strictly necessary when using a single column selector function, so columns=pb.starts_with(\"paid\") would be equivalent usage here. However, the use of col() is required when using multiple column selector functions with logical operators. Here is an example of that more complex usage:\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.starts_with(\"paid\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the starts_with() and matches() column selector functions, combined with the & operator. This is necessary to specify the set of columns that start with \"paid\" and match the text \"2023\" or \"2024\".\nIf you’d like to take advantage of Narwhals selectors, that’s also possible. Here is an example of using the numeric() column selector function to select all numeric columns for validation, checking that their values are greater than 0.\n\nimport narwhals.selectors as ncs\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=pb.col(ncs.numeric()), value=0)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    hours_2022\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    hours_2023\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    hours_2024\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    paid_2022\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    paid_2023\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    paid_2024\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the numeric() column selector function from Narwhals. As with the other selectors, this is not strictly necessary when using a single column selector, so columns=ncs.numeric() would also be fine here.\nNarwhals selectors can also use operators to combine multiple selectors. Here is an example of using the numeric() and matches() selectors together to select all numeric columns that fit a specific pattern.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_status\": [\"ft\", \"ft\", \"pt\"],\n        \"2023_status\": [\"ft\", \"pt\", \"ft\"],\n        \"2024_status\": [\"ft\", \"pt\", \"ft\"],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.col(ncs.numeric() & ncs.matches(\"2023|2024\")), value=30)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2023_pay_total\n    30\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_pay_total\n    30\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn the above example the col() function contains the invocation of the numeric() and matches() column selector functions from Narwhals, combined with the & operator. This is necessary to specify the set of columns that are numeric and match the text \"2023\" or \"2024\"."
  },
  {
    "objectID": "reference/col.html#see-also",
    "href": "reference/col.html#see-also",
    "title": "col",
    "section": "See Also",
    "text": "See Also\nCreate a column expression for use in conjointly() validation with the expr_col() function."
  },
  {
    "objectID": "reference/Validate.html",
    "href": "reference/Validate.html",
    "title": "Validate",
    "section": "",
    "text": "Validate(\n    self,\n    data,\n    tbl_name=None,\n    label=None,\n    thresholds=None,\n    actions=None,\n    final_actions=None,\n    brief=None,\n    lang=None,\n    locale=None,\n)\nWorkflow for defining a set of validations on a table and interrogating for results.\nThe Validate class is used for defining a set of validation steps on a table and interrogating the table with the validation plan. This class is the main entry point for the data quality reporting workflow. The overall aim of this workflow is to generate comprehensive reporting information to assess the level of data quality for a target table.\nWe can supply as many validation steps as needed, and having a large number of them should increase the validation coverage for a given table. The validation methods (e.g., col_vals_gt(), col_vals_between(), etc.) translate to discrete validation steps, where each step will be sequentially numbered (useful when viewing the reporting data). This process of calling validation methods is known as developing a validation plan.\nThe validation methods, when called, are merely instructions up to the point the concluding interrogate() method is called. That kicks off the process of acting on the validation plan by querying the target table getting reporting results for each step. Once the interrogation process is complete, we can say that the workflow now has reporting information. We can then extract useful information from the reporting data to understand the quality of the table. Printing the Validate object (or using the get_tabular_report() method) will return a table with the results of the interrogation and get_sundered_data() allows for the splitting of the table based on passing and failing rows."
  },
  {
    "objectID": "reference/Validate.html#parameters",
    "href": "reference/Validate.html#parameters",
    "title": "Validate",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to validate, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str | None = None\n\nA optional name to assign to the input table object. If no value is provided, a name will be generated based on whatever information is available. This table name will be displayed in the header area of the tabular report.\n\nlabel : str | None = None\n\nAn optional label for the validation plan. If no value is provided, a label will be generated based on the current system date and time. Markdown can be used here to make the label more visually appealing (it will appear in the header area of the tabular report).\n\nthresholds : int | float | bool | tuple | dict | Thresholds | None = None\n\nGenerate threshold failure levels so that all validation steps can report and react accordingly when exceeding the set levels. The thresholds are set at the global level and can be overridden at the validation step level (each validation step has its own thresholds= parameter). The default is None, which means that no thresholds will be set. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nThe actions to take when validation steps meet or exceed any set threshold levels. These actions are paired with the threshold levels and are executed during the interrogation process when there are exceedances. The actions are executed right after each step is evaluated. Such actions should be provided in the form of an Actions object. If None then no global actions will be set. View the Actions section for information on how to set actions.\n\nfinal_actions : FinalActions | None = None\n\nThe actions to take when the validation process is complete and the final results are available. This is useful for sending notifications or reporting the overall status of the validation process. The final actions are executed after all validation steps have been processed and the results have been collected. The final actions are not tied to any threshold levels, they are executed regardless of the validation results. Such actions should be provided in the form of a FinalActions object. If None then no finalizing actions will be set. Please see the Actions section for information on how to set final actions.\n\nbrief : str | bool | None = None\n\nA global setting for briefs, which are optional brief descriptions for validation steps (they be displayed in the reporting table). For such a global setting, templating elements like \"{step}\" (to insert the step number) or \"{auto}\" (to include an automatically generated brief) are useful. If True then each brief will be automatically generated. If None (the default) then briefs aren’t globally set.\n\nlang : str | None = None\n\nThe language to use for various reporting elements. By default, None will select English (\"en\") as the but other options include French (\"fr\"), German (\"de\"), Italian (\"it\"), Spanish (\"es\"), and several more. Have a look at the Reporting Languages section for the full list of supported languages and information on how the language setting is utilized.\n\nlocale : str | None = None\n\nAn optional locale ID to use for formatting values in the reporting table according the locale’s rules. Examples include \"en-US\" for English (United States) and \"fr-FR\" for French (France). More simply, this can be a language identifier without a designation of territory, like \"es\" for Spanish."
  },
  {
    "objectID": "reference/Validate.html#returns",
    "href": "reference/Validate.html#returns",
    "title": "Validate",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nA Validate object with the table and validations to be performed."
  },
  {
    "objectID": "reference/Validate.html#supported-input-table-types",
    "href": "reference/Validate.html#supported-input-table-types",
    "title": "Validate",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, the use of Validate with such tables requires the Ibis library v9.5.0 and above to be installed. If the input table is a Polars or Pandas DataFrame, the Ibis library is not required."
  },
  {
    "objectID": "reference/Validate.html#thresholds",
    "href": "reference/Validate.html#thresholds",
    "title": "Validate",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for all validation steps. They are set here at the global level but can be overridden at the validation step level (each validation step has its own local thresholds= parameter).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units for a validation step exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.html#actions",
    "href": "reference/Validate.html#actions",
    "title": "Validate",
    "section": "Actions",
    "text": "Actions\nThe actions= and final_actions= parameters provide mechanisms to respond to validation results. These actions can be used to notify users of validation failures, log issues, or trigger other processes when problems are detected.\nStep Actions\nThe actions= parameter allows you to define actions that are triggered when validation steps exceed specific threshold levels (warning, error, or critical). These actions are executed during the interrogation process, right after each step is evaluated.\nStep actions should be provided using the Actions class, which lets you specify different actions for different severity levels:\n# Define an action that logs a message when warning threshold is exceeded\ndef log_warning():\n    metadata = pb.get_action_metadata()\n    print(f\"WARNING: Step {metadata['step']} failed with type {metadata['type']}\")\n\n# Define actions for different threshold levels\nactions = pb.Actions(\n    warning = log_warning,\n    error = lambda: send_email(\"Error in validation\"),\n    critical = \"CRITICAL FAILURE DETECTED\"\n)\n\n# Use in Validate\nvalidation = pb.Validate(\n    data=my_data,\n    actions=actions  # Global actions for all steps\n)\nYou can also provide step-specific actions in individual validation methods:\nvalidation.col_vals_gt(\n    columns=\"revenue\",\n    value=0,\n    actions=pb.Actions(warning=log_warning)  # Only applies to this step\n)\nStep actions have access to step-specific context through the get_action_metadata() function, which provides details about the current validation step that triggered the action.\nFinal Actions\nThe final_actions= parameter lets you define actions that execute after all validation steps have completed. These are useful for providing summaries, sending notifications based on overall validation status, or performing cleanup operations.\nFinal actions should be provided using the FinalActions class:\ndef send_report():\n    summary = pb.get_validation_summary()\n    if summary[\"status\"] == \"CRITICAL\":\n        send_alert_email(\n            subject=f\"CRITICAL validation failures in {summary['table_name']}\",\n            body=f\"{summary['critical_steps']} steps failed with critical severity.\"\n        )\n\nvalidation = pb.Validate(\n    data=my_data,\n    final_actions=pb.FinalActions(send_report)\n)\nFinal actions have access to validation-wide summary information through the get_validation_summary() function, which provides a comprehensive overview of the entire validation process.\nThe combination of step actions and final actions provides a flexible system for responding to data quality issues at both the individual step level and the overall validation level."
  },
  {
    "objectID": "reference/Validate.html#reporting-languages",
    "href": "reference/Validate.html#reporting-languages",
    "title": "Validate",
    "section": "Reporting Languages",
    "text": "Reporting Languages\nVarious pieces of reporting in Pointblank can be localized to a specific language. This is done by setting the lang= parameter in Validate. Any of the following languages can be used (just provide the language code):\n\nEnglish (\"en\")\nFrench (\"fr\")\nGerman (\"de\")\nItalian (\"it\")\nSpanish (\"es\")\nPortuguese (\"pt\")\nDutch (\"nl\")\nSwedish (\"sv\")\nDanish (\"da\")\nNorwegian Bokmål (\"nb\")\nIcelandic (\"is\")\nFinnish (\"fi\")\nPolish (\"pl\")\nCzech (\"cs\")\nRomanian (\"ro\")\nGreek (\"el\")\nRussian (\"ru\")\nTurkish (\"tr\")\nArabic (\"ar\")\nHindi (\"hi\")\nSimplified Chinese (\"zh-Hans\")\nTraditional Chinese (\"zh-Hant\")\nJapanese (\"ja\")\nKorean (\"ko\")\nVietnamese (\"vi\")\n\nAutomatically generated briefs (produced by using brief=True or brief=\"...{auto}...\") will be written in the selected language. The language setting will also used when generating the validation report table through get_tabular_report() (or printing the Validate object in a notebook environment)."
  },
  {
    "objectID": "reference/Validate.html#examples",
    "href": "reference/Validate.html#examples",
    "title": "Validate",
    "section": "Examples",
    "text": "Examples\n\nCreating a validation plan and interrogating\nLet’s walk through a data quality analysis of an extremely small table. It’s actually called \"small_table\" and it’s accessible through the load_dataset() function.\n\nimport pointblank as pb\n\n# Load the small_table dataset\nsmall_table = pb.load_dataset()\n\n# Preview the table\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nWe ought to think about what’s tolerable in terms of data quality so let’s designate proportional failure thresholds to the ‘warning’, ‘error’, and ‘critical’ states. This can be done by using the Thresholds class.\n\nthresholds = pb.Thresholds(warning=0.10, error=0.25, critical=0.35)\n\nNow, we use the Validate class and give it the thresholds object (which serves as a default for all validation steps but can be overridden). The static thresholds provided in thresholds= will make the reporting a bit more useful. We also need to provide a target table and we’ll use small_table for this.\n\nvalidation = (\n    pb.Validate(\n        data=small_table,\n        tbl_name=\"small_table\",\n        label=\"`Validate` example.\",\n        thresholds=thresholds\n    )\n)\n\nThen, as with any Validate object, we can add steps to the validation plan by using as many validation methods as we want. To conclude the process (and actually query the data table), we use the interrogate() method.\n\nvalidation = (\n    validation\n    .col_vals_gt(columns=\"d\", value=100)\n    .col_vals_le(columns=\"c\", value=5)\n    .col_vals_between(columns=\"c\", left=3, right=10, na_pass=True)\n    .col_vals_regex(columns=\"b\", pattern=r\"[0-9]-[a-z]{3}-[0-9]{3}\")\n    .col_exists(columns=[\"date\", \"date_time\"])\n    .interrogate()\n)\n\nThe validation object can be printed as a reporting table.\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    `Validate` example.Polarssmall_tableWARNING0.1ERROR0.25CRITICAL0.35\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    2\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    c\n    [3, 10]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    [0-9]-[a-z]{3}-[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:58:46 UTC&lt; 1 s2025-04-23 00:58:46 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe report could be further customized by using the get_tabular_report() method, which contains options for modifying the display of the table.\n\n\nAdding briefs\nBriefs are short descriptions of the validation steps. While they can be set for each step individually, they can also be set globally. The global setting is done by using the brief= argument in Validate. The global setting can be as simple as True to have automatically-generated briefs for each step. Alternatively, we can use templating elements like \"{step}\" (to insert the step number) or \"{auto}\" (to include an automatically generated brief). Here’s an example of a global setting for briefs:\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(),\n        tbl_name=\"small_table\",\n        label=\"Validation example with briefs\",\n        brief=\"Step {step}: {auto}\",\n    )\n    .col_vals_gt(columns=\"d\", value=100)\n    .col_vals_between(columns=\"c\", left=3, right=10, na_pass=True)\n    .col_vals_regex(\n        columns=\"b\",\n        pattern=r\"[0-9]-[a-z]{3}-[0-9]{3}\",\n        brief=\"Regex check for column {col}\"\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Validation example with briefsPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        Step 1: Expect that values in d should be &gt; 100.\n\n        \n    d\n    100\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        Step 2: Expect that values in c should be between 3 and 10.\n\n        \n    c\n    [3, 10]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        Regex check for column b\n\n        \n    b\n    [0-9]-[a-z]{3}-[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:58:46 UTC&lt; 1 s2025-04-23 00:58:46 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe see the text of the briefs appear in the STEP column of the reporting table. Furthermore, the global brief’s template (\"Step {step}: {auto}\") is applied to all steps except for the final step, where the step-level brief= argument provided an override.\nIf you should want to cancel the globally-defined brief for one or more validation steps, you can set brief=False in those particular steps.\n\n\nPost-interrogation methods\nThe Validate class has a number of post-interrogation methods that can be used to extract useful information from the validation results. For example, the get_data_extracts() method can be used to get the data extracts for each validation step.\n\nvalidation.get_data_extracts()\n\n{1: shape: (0, 9)\n ┌───────────┬──────────────┬──────┬─────┬───┬─────┬─────┬──────┬─────┐\n │ _row_num_ ┆ date_time    ┆ date ┆ a   ┆ … ┆ c   ┆ d   ┆ e    ┆ f   │\n │ ---       ┆ ---          ┆ ---  ┆ --- ┆   ┆ --- ┆ --- ┆ ---  ┆ --- │\n │ u32       ┆ datetime[μs] ┆ date ┆ i64 ┆   ┆ i64 ┆ f64 ┆ bool ┆ str │\n ╞═══════════╪══════════════╪══════╪═════╪═══╪═════╪═════╪══════╪═════╡\n └───────────┴──────────────┴──────┴─────┴───┴─────┴─────┴──────┴─────┘,\n 2: shape: (1, 9)\n ┌───────────┬─────────────────────┬────────────┬─────┬───┬─────┬─────────┬───────┬─────┐\n │ _row_num_ ┆ date_time           ┆ date       ┆ a   ┆ … ┆ c   ┆ d       ┆ e     ┆ f   │\n │ ---       ┆ ---                 ┆ ---        ┆ --- ┆   ┆ --- ┆ ---     ┆ ---   ┆ --- │\n │ u32       ┆ datetime[μs]        ┆ date       ┆ i64 ┆   ┆ i64 ┆ f64     ┆ bool  ┆ str │\n ╞═══════════╪═════════════════════╪════════════╪═════╪═══╪═════╪═════════╪═══════╪═════╡\n │ 8         ┆ 2016-01-17 11:27:00 ┆ 2016-01-17 ┆ 4   ┆ … ┆ 2   ┆ 1035.64 ┆ false ┆ low │\n └───────────┴─────────────────────┴────────────┴─────┴───┴─────┴─────────┴───────┴─────┘,\n 3: shape: (0, 9)\n ┌───────────┬──────────────┬──────┬─────┬───┬─────┬─────┬──────┬─────┐\n │ _row_num_ ┆ date_time    ┆ date ┆ a   ┆ … ┆ c   ┆ d   ┆ e    ┆ f   │\n │ ---       ┆ ---          ┆ ---  ┆ --- ┆   ┆ --- ┆ --- ┆ ---  ┆ --- │\n │ u32       ┆ datetime[μs] ┆ date ┆ i64 ┆   ┆ i64 ┆ f64 ┆ bool ┆ str │\n ╞═══════════╪══════════════╪══════╪═════╪═══╪═════╪═════╪══════╪═════╡\n └───────────┴──────────────┴──────┴─────┴───┴─────┴─────┴──────┴─────┘}\n\n\nWe can also view step reports for each validation step using the get_step_report() method. This method adapts to the type of validation step and shows the relevant information for a step’s validation.\n\nvalidation.get_step_report(i=2)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION 3 ≤ c ≤ 101 / 13 TEST UNIT FAILURES IN COLUMN 5 EXTRACT OF ALL 1 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\nThe Validate class also has a method for getting the sundered data, which is the data that passed or failed the validation steps. This can be done using the get_sundered_data() method.\n\npb.preview(validation.get_sundered_data())\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows12Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    8\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    11\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    12\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nThe sundered data is a DataFrame that contains the rows that passed or failed the validation. The default behavior is to return the rows that failed the validation, as shown above."
  },
  {
    "objectID": "reference/Validate.interrogate.html",
    "href": "reference/Validate.interrogate.html",
    "title": "Validate.interrogate",
    "section": "",
    "text": "Validate.interrogate(\n    collect_extracts=True,\n    collect_tbl_checked=True,\n    get_first_n=None,\n    sample_n=None,\n    sample_frac=None,\n    extract_limit=500,\n)\nExecute each validation step against the table and store the results.\nWhen a validation plan has been set with a series of validation steps, the interrogation process through interrogate() should then be invoked. Interrogation will evaluate each validation step against the table and store the results.\nThe interrogation process will collect extracts of failing rows if the collect_extracts= option is set to True (the default). We can control the number of rows collected using the get_first_n=, sample_n=, and sample_frac= options. The extract_limit= option will enforce a hard limit on the number of rows collected when collect_extracts=True.\nAfter interrogation is complete, the Validate object will have gathered information, and we can use methods like n_passed(), f_failed(), etc., to understand how the table performed against the validation plan. A visual representation of the validation results can be viewed by printing theValidate` object; this will display the validation table in an HTML viewing environment."
  },
  {
    "objectID": "reference/Validate.interrogate.html#parameters",
    "href": "reference/Validate.interrogate.html#parameters",
    "title": "Validate.interrogate",
    "section": "Parameters",
    "text": "Parameters\n\ncollect_extracts : bool = True\n\nAn option to collect rows of the input table that didn’t pass a particular validation step. The default is True and further options (i.e., get_first_n=, sample_*=) allow for fine control of how these rows are collected.\n\ncollect_tbl_checked : bool = True\n\nThe processed data frames produced by executing the validation steps is collected and stored in the Validate object if collect_tbl_checked=True. This information is necessary for some methods (e.g., get_sundered_data()), but it can potentially make the object grow to a large size. To opt out of attaching this data, set this to False.\n\nget_first_n : int | None = None\n\nIf the option to collect rows where test units is chosen, there is the option here to collect the first n rows. Supply an integer number of rows to extract from the top of subset table containing non-passing rows (the ordering of data from the original table is retained).\n\nsample_n : int | None = None\n\nIf the option to collect non-passing rows is chosen, this option allows for the sampling of n rows. Supply an integer number of rows to sample from the subset table. If n happens to be greater than the number of non-passing rows, then all such rows will be returned.\n\nsample_frac : int | float | None = None\n\nIf the option to collect non-passing rows is chosen, this option allows for the sampling of a fraction of those rows. Provide a number in the range of 0 and 1. The number of rows to return could be very large, however, the extract_limit= option will apply a hard limit to the returned rows.\n\nextract_limit : int = 500\n\nA value that limits the possible number of rows returned when extracting non-passing rows. The default is 500 rows. This limit is applied after any sampling or limiting options are applied. If the number of rows to be returned is greater than this limit, then the number of rows returned will be limited to this value. This is useful for preventing the collection of too many rows when the number of non-passing rows is very large."
  },
  {
    "objectID": "reference/Validate.interrogate.html#returns",
    "href": "reference/Validate.interrogate.html#returns",
    "title": "Validate.interrogate",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the results of the interrogation."
  },
  {
    "objectID": "reference/Validate.interrogate.html#examples",
    "href": "reference/Validate.interrogate.html#examples",
    "title": "Validate.interrogate",
    "section": "Examples",
    "text": "Examples\nLet’s use a built-in dataset (\"game_revenue\") to demonstrate some of the options of the interrogation process. A series of validation steps will populate our validation plan. After setting up the plan, the next step is to interrogate the table and see how well it aligns with our expectations. We’ll use the get_first_n= option so that any extracts of failing rows are limited to the first n rows.\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"game_revenue\"))\n    .col_vals_lt(columns=\"item_revenue\", value=200)\n    .col_vals_gt(columns=\"item_revenue\", value=0)\n    .col_vals_gt(columns=\"session_duration\", value=5)\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}[0-9]{3}\")\n)\n\nvalidation.interrogate(get_first_n=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:58:39Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    item_revenue\n    200\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}[0-9]{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:58:39 UTC&lt; 1 s2025-04-23 00:58:39 UTC\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that step 3 (checking for session_duration greater than 5) has 18 failing test units. This means that 18 rows in the table are problematic. We’d like to see the rows that failed this validation step and we can do that with the get_data_extracts() method.\n\npb.preview(validation.get_data_extracts(i=3, frame=True))\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows10Columns12\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    620\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:25:18+00:00\n    iap\n    offer4\n    17.991\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    621\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:26:24+00:00\n    iap\n    offer5\n    26.09\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    622\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:28:36+00:00\n    ad\n    ad_15sec\n    0.53\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n\n\n\n\n\n\n        \n\n\nThe get_data_extracts() method will return a Polars DataFrame here with the first 10 rows that failed the validation step (we passed that into the preview() function for a better display). There are actually 18 rows that failed but we limited the collection of extracts with get_first_n=10."
  },
  {
    "objectID": "reference/Validate.row_count_match.html",
    "href": "reference/Validate.row_count_match.html",
    "title": "Validate.row_count_match",
    "section": "",
    "text": "Validate.row_count_match(\n    count,\n    tol=0,\n    inverse=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether the row count of the table matches a specified count.\nThe row_count_match() method checks whether the row count of the target table matches a specified count. This validation will operate over a single test unit, which is whether the row count matches the specified count.\nWe also have the option to invert the validation step by setting inverse=True. This will make the expectation that the row count of the target table does not match the specified count."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#parameters",
    "href": "reference/Validate.row_count_match.html#parameters",
    "title": "Validate.row_count_match",
    "section": "Parameters",
    "text": "Parameters\n\ncount : int | FrameT | Any\n\nThe expected row count of the table. This can be an integer value, a Polars or Pandas DataFrame object, or an Ibis backend table. If a DataFrame/table is provided, the row count of that object will be used as the expected count.\n\ntol : Tolerance = 0\n\nThe tolerance allowable for the row count match. This can be specified as a single numeric value (integer or float) or as a tuple of two integers representing the lower and upper bounds of the tolerance range. If a single integer value (greater than 1) is provided, it represents the absolute bounds of the tolerance, ie. plus or minus the value. If a float value (between 0-1) is provided, it represents the relative tolerance, ie. plus or minus the relative percentage of the target. If a tuple is provided, it represents the lower and upper absolute bounds of the tolerance range. See the examples for more.\n\ninverse : bool = False\n\nShould the validation step be inverted? If True, then the expectation is that the row count of the target table should not match the specified count= value.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#returns",
    "href": "reference/Validate.row_count_match.html#returns",
    "title": "Validate.row_count_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#preprocessing",
    "href": "reference/Validate.row_count_match.html#preprocessing",
    "title": "Validate.row_count_match",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#thresholds",
    "href": "reference/Validate.row_count_match.html#thresholds",
    "title": "Validate.row_count_match",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.row_count_match.html#examples",
    "href": "reference/Validate.row_count_match.html#examples",
    "title": "Validate.row_count_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use the built in dataset \"small_table\". The table can be obtained by calling load_dataset(\"small_table\").\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(\"small_table\")\n\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the number of rows in the table matches a fixed value. In this case, we will use the value 13 as the expected row count.\n\nvalidation = (\n    pb.Validate(data=small_table)\n    .row_count_match(count=13)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the expectation value of 13 matches the actual count of rows in the target table. So, the single test unit passed.\nLet’s modify our example to show the different ways we can allow some tolerance to our validation by using the tol argument.\n\nsmaller_small_table = small_table.sample(n = 12) # within the lower bound\nvalidation = (\n    pb.Validate(data=smaller_small_table)\n    .row_count_match(count=13,tol=(2, 0)) # minus 2 but plus 0, ie. 11-13\n    .interrogate()\n)\n\nvalidation\n\nvalidation = (\n    pb.Validate(data=smaller_small_table)\n    .row_count_match(count=13,tol=.05) # .05% tolerance of 13\n    .interrogate()\n)\n\neven_smaller_table = small_table.sample(n = 2)\nvalidation = (\n    pb.Validate(data=even_smaller_table)\n    .row_count_match(count=13,tol=5) # plus or minus 5; this test will fail\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    13\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —"
  },
  {
    "objectID": "reference/Validate.col_exists.html",
    "href": "reference/Validate.col_exists.html",
    "title": "Validate.col_exists",
    "section": "",
    "text": "Validate.col_exists(\n    columns,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether one or more columns exist in the table.\nThe col_exists() method checks whether one or more columns exist in the target table. The only requirement is specification of the column names. Each validation step or expectation will operate over a single test unit, which is whether the column exists or not."
  },
  {
    "objectID": "reference/Validate.col_exists.html#parameters",
    "href": "reference/Validate.col_exists.html#parameters",
    "title": "Validate.col_exists",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_exists.html#returns",
    "href": "reference/Validate.col_exists.html#returns",
    "title": "Validate.col_exists",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_exists.html#thresholds",
    "href": "reference/Validate.col_exists.html#thresholds",
    "title": "Validate.col_exists",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_exists.html#examples",
    "href": "reference/Validate.col_exists.html#examples",
    "title": "Validate.col_exists",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with a string columns (a) and a numeric column (b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bInt64\n\n\n\n  \n    1\n    apple\n    1\n  \n  \n    2\n    banana\n    6\n  \n  \n    3\n    cherry\n    3\n  \n  \n    4\n    date\n    5\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the columns a and b actually exist in the table. We’ll determine if this validation had any failing test units (each validation will have a single test unit).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_exists(columns=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows two entries (one check per column) generated by the col_exists() validation step. Both steps passed since both columns provided in columns= are present in the table.\nNow, let’s check for the existence of a different set of columns.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_exists(columns=[\"b\", \"c\"])\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports one passing validation step (the check for column b) and one failing validation step (the check for column c, which doesn’t exist)."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html",
    "href": "reference/Validate.col_vals_null.html",
    "title": "Validate.col_vals_null",
    "section": "",
    "text": "Validate.col_vals_null(\n    columns,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether values in a column are NULL.\nThe col_vals_null() validation method checks whether column values in a table are NULL. This validation will operate over the number of test units that is equal to the number of rows in the table."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#parameters",
    "href": "reference/Validate.col_vals_null.html#parameters",
    "title": "Validate.col_vals_null",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#returns",
    "href": "reference/Validate.col_vals_null.html#returns",
    "title": "Validate.col_vals_null",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#preprocessing",
    "href": "reference/Validate.col_vals_null.html#preprocessing",
    "title": "Validate.col_vals_null",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#thresholds",
    "href": "reference/Validate.col_vals_null.html#thresholds",
    "title": "Validate.col_vals_null",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_null.html#examples",
    "href": "reference/Validate.col_vals_null.html#examples",
    "title": "Validate.col_vals_null",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [None, None, None, None],\n        \"b\": [None, 2, None, 9],\n    }\n).with_columns(pl.col(\"a\").cast(pl.Int64))\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    None\n    None\n  \n  \n    2\n    None\n    2\n  \n  \n    3\n    None\n    None\n  \n  \n    4\n    None\n    9\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all Null values. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_null(columns=\"a\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_null(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_null(columns=\"b\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the two non-Null values in column b."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html",
    "href": "reference/Validate.col_vals_regex.html",
    "title": "Validate.col_vals_regex",
    "section": "",
    "text": "Validate.col_vals_regex(\n    columns,\n    pattern,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether column values match a regular expression pattern.\nThe col_vals_regex() validation method checks whether column values in a table correspond to a pattern= matching expression. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#parameters",
    "href": "reference/Validate.col_vals_regex.html#parameters",
    "title": "Validate.col_vals_regex",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npattern : str\n\nA regular expression pattern to compare against.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#returns",
    "href": "reference/Validate.col_vals_regex.html#returns",
    "title": "Validate.col_vals_regex",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#preprocessing",
    "href": "reference/Validate.col_vals_regex.html#preprocessing",
    "title": "Validate.col_vals_regex",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#thresholds",
    "href": "reference/Validate.col_vals_regex.html#thresholds",
    "title": "Validate.col_vals_regex",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_regex.html#examples",
    "href": "reference/Validate.col_vals_regex.html#examples",
    "title": "Validate.col_vals_regex",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two string columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"rb-0343\", \"ra-0232\", \"ry-0954\", \"rc-1343\"],\n        \"b\": [\"ra-0628\", \"ra-583\", \"rya-0826\", \"rb-0735\"],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bString\n\n\n\n  \n    1\n    rb-0343\n    ra-0628\n  \n  \n    2\n    ra-0232\n    ra-583\n  \n  \n    3\n    ry-0954\n    rya-0826\n  \n  \n    4\n    rc-1343\n    rb-0735\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that all of the values in column a match a particular regex pattern. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=\"a\", pattern=r\"r[a-z]-[0-9]{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    a\n    r[a-z]-[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_regex(). All test units passed, and there are no failing test units.\nNow, let’s use the same regex for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=\"b\", pattern=r\"r[a-z]-[0-9]{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    r[a-z]-[0-9]{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the string values of rows 1 and 2 in column b."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "When peforming data validation, you’ll need the Validate class to get the process started. It’s given the target table and you can optionally provide some metadata and/or failure thresholds (using the Thresholds class or through shorthands for this task). The Validate class has numerous methods for defining validation steps and for obtaining post-interrogation metrics and data.\n\n\n\nValidate\nWorkflow for defining a set of validations on a table and interrogating for results.\n\n\nThresholds\nDefinition of threshold values.\n\n\nActions\nDefinition of action values.\n\n\nFinalActions\nDefine actions to be taken after validation is complete.\n\n\nSchema\nDefinition of a schema object.\n\n\nDraftValidation\nDraft a validation plan for a given table using an LLM.\n\n\n\n\n\n\nValidation steps can be thought of as sequential validations on the target data. We call Validate’s validation methods to build up a validation plan: a collection of steps that, in the aggregate, provides good validation coverage.\n\n\n\nValidate.col_vals_gt\nAre column data greater than a fixed value or data in another column?\n\n\nValidate.col_vals_lt\nAre column data less than a fixed value or data in another column?\n\n\nValidate.col_vals_ge\nAre column data greater than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_le\nAre column data less than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_eq\nAre column data equal to a fixed value or data in another column?\n\n\nValidate.col_vals_ne\nAre column data not equal to a fixed value or data in another column?\n\n\nValidate.col_vals_between\nDo column data lie between two specified values or data in other columns?\n\n\nValidate.col_vals_outside\nDo column data lie outside of two specified values or data in other columns?\n\n\nValidate.col_vals_in_set\nValidate whether column values are in a set of values.\n\n\nValidate.col_vals_not_in_set\nValidate whether column values are not in a set of values.\n\n\nValidate.col_vals_null\nValidate whether values in a column are NULL.\n\n\nValidate.col_vals_not_null\nValidate whether values in a column are not NULL.\n\n\nValidate.col_vals_regex\nValidate whether column values match a regular expression pattern.\n\n\nValidate.col_vals_expr\nValidate column values using a custom expression.\n\n\nValidate.col_exists\nValidate whether one or more columns exist in the table.\n\n\nValidate.rows_distinct\nValidate whether rows in the table are distinct.\n\n\nValidate.col_schema_match\nDo columns in the table (and their types) match a predefined schema?\n\n\nValidate.row_count_match\nValidate whether the row count of the table matches a specified count.\n\n\nValidate.col_count_match\nValidate whether the column count of the table matches a specified count.\n\n\nValidate.conjointly\nPerform multiple row-wise validations for joint validity.\n\n\n\n\n\n\nA flexible way to select columns for validation is to use the col() function along with column selection helper functions. A combination of col() + starts_with(), matches(), etc., allows for the selection of multiple target columns (mapping a validation across many steps). Furthermore, the col() function can be used to declare a comparison column (e.g., for the value= argument in many col_vals_*() methods) when you can’t use a fixed value for comparison.\n\n\n\ncol\nHelper function for referencing a column in the input table.\n\n\nstarts_with\nSelect columns that start with specified text.\n\n\nends_with\nSelect columns that end with specified text.\n\n\ncontains\nSelect columns that contain specified text.\n\n\nmatches\nSelect columns that match a specified regular expression pattern.\n\n\neverything\nSelect all columns.\n\n\nfirst_n\nSelect the first n columns in the column list.\n\n\nlast_n\nSelect the last n columns in the column list.\n\n\nexpr_col\nCreate a column expression for use in conjointly() validation.\n\n\n\n\n\n\nThe validation plan is put into action when interrogate() is called. The workflow for performing a comprehensive validation is then: (1) Validate(), (2) adding validation steps, (3) interrogate(). After interrogation of the data, we can view a validation report table (by printing the object or using get_tabular_report()), extract key metrics, or we can split the data based on the validation results (with get_sundered_data()).\n\n\n\nValidate.interrogate\nExecute each validation step against the table and store the results.\n\n\nValidate.get_tabular_report\nValidation report as a GT table.\n\n\nValidate.get_step_report\nGet a detailed report for a single validation step.\n\n\nValidate.get_json_report\nGet a report of the validation results as a JSON-formatted string.\n\n\nValidate.get_sundered_data\nGet the data that passed or failed the validation steps.\n\n\nValidate.get_data_extracts\nGet the rows that failed for each validation step.\n\n\nValidate.all_passed\nDetermine if every validation step passed perfectly, with no failing test units.\n\n\nValidate.assert_passing\nRaise an AssertionError if all tests are not passing.\n\n\nValidate.n\nProvides a dictionary of the number of test units for each validation step.\n\n\nValidate.n_passed\nProvides a dictionary of the number of test units that passed for each validation step.\n\n\nValidate.n_failed\nProvides a dictionary of the number of test units that failed for each validation step.\n\n\nValidate.f_passed\nProvides a dictionary of the fraction of test units that passed for each validation step.\n\n\nValidate.f_failed\nProvides a dictionary of the fraction of test units that failed for each validation step.\n\n\nValidate.warning\nGet the ‘warning’ level status for each validation step.\n\n\nValidate.error\nGet the ‘error’ level status for each validation step.\n\n\nValidate.critical\nGet the ‘critical’ level status for each validation step.\n\n\n\n\n\n\nThe Inspection and Assistance group contains functions that are helpful for getting to grips on a new data table. Use the DataScan class to get a quick overview of the data, preview() to see the first and last few rows of a table, col_summary_tbl() for a column-level summary of a table, and missing_vals_tbl() to see where there are missing values in a table. Several datasets included in the package can be accessed via the load_dataset() function. On the assistance side, the assistant() function can be used to get help with Pointblank.\n\n\n\nDataScan\nGet a summary of a dataset.\n\n\npreview\nDisplay a table preview that shows some rows from the top, some from the bottom.\n\n\ncol_summary_tbl\nGenerate a column-level summary table of a dataset.\n\n\nmissing_vals_tbl\nDisplay a table that shows the missing values in the input table.\n\n\nassistant\nChat with the PbA (Pointblank Assistant) about your data validation needs.\n\n\nload_dataset\nLoad a dataset hosted in the library as specified table type.\n\n\n\n\n\n\nThe Utility Functions group contains functions that are useful accessing metadata about the target data. Use get_column_count() or get_row_count() to get the number of columns or rows in a table. The get_action_metadata() function is useful when building custom actions since it returns metadata about the validation step that’s triggering the action. Lastly, the config() utility lets us set global configuration parameters.\n\n\n\nget_column_count\nGet the number of columns in a table.\n\n\nget_row_count\nGet the number of rows in a table.\n\n\nget_action_metadata\nAccess step-level metadata when authoring custom actions.\n\n\nget_validation_summary\nAccess validation summary information when authoring final actions.\n\n\nconfig\nConfiguration settings for the Pointblank library.\n\n\n\n\n\n\nThe Prebuilt Actions group contains a function that can be used to send a Slack notification when validation steps exceed failure threshold levels or just to provide a summary of the validation results, including the status, number of steps, passing and failing steps, table information, and timing details.\n\n\n\nsend_slack_notification\nCreate a Slack notification function using a webhook URL."
  },
  {
    "objectID": "reference/index.html#validate",
    "href": "reference/index.html#validate",
    "title": "API Reference",
    "section": "",
    "text": "When peforming data validation, you’ll need the Validate class to get the process started. It’s given the target table and you can optionally provide some metadata and/or failure thresholds (using the Thresholds class or through shorthands for this task). The Validate class has numerous methods for defining validation steps and for obtaining post-interrogation metrics and data.\n\n\n\nValidate\nWorkflow for defining a set of validations on a table and interrogating for results.\n\n\nThresholds\nDefinition of threshold values.\n\n\nActions\nDefinition of action values.\n\n\nFinalActions\nDefine actions to be taken after validation is complete.\n\n\nSchema\nDefinition of a schema object.\n\n\nDraftValidation\nDraft a validation plan for a given table using an LLM."
  },
  {
    "objectID": "reference/index.html#validation-steps",
    "href": "reference/index.html#validation-steps",
    "title": "API Reference",
    "section": "",
    "text": "Validation steps can be thought of as sequential validations on the target data. We call Validate’s validation methods to build up a validation plan: a collection of steps that, in the aggregate, provides good validation coverage.\n\n\n\nValidate.col_vals_gt\nAre column data greater than a fixed value or data in another column?\n\n\nValidate.col_vals_lt\nAre column data less than a fixed value or data in another column?\n\n\nValidate.col_vals_ge\nAre column data greater than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_le\nAre column data less than or equal to a fixed value or data in another column?\n\n\nValidate.col_vals_eq\nAre column data equal to a fixed value or data in another column?\n\n\nValidate.col_vals_ne\nAre column data not equal to a fixed value or data in another column?\n\n\nValidate.col_vals_between\nDo column data lie between two specified values or data in other columns?\n\n\nValidate.col_vals_outside\nDo column data lie outside of two specified values or data in other columns?\n\n\nValidate.col_vals_in_set\nValidate whether column values are in a set of values.\n\n\nValidate.col_vals_not_in_set\nValidate whether column values are not in a set of values.\n\n\nValidate.col_vals_null\nValidate whether values in a column are NULL.\n\n\nValidate.col_vals_not_null\nValidate whether values in a column are not NULL.\n\n\nValidate.col_vals_regex\nValidate whether column values match a regular expression pattern.\n\n\nValidate.col_vals_expr\nValidate column values using a custom expression.\n\n\nValidate.col_exists\nValidate whether one or more columns exist in the table.\n\n\nValidate.rows_distinct\nValidate whether rows in the table are distinct.\n\n\nValidate.col_schema_match\nDo columns in the table (and their types) match a predefined schema?\n\n\nValidate.row_count_match\nValidate whether the row count of the table matches a specified count.\n\n\nValidate.col_count_match\nValidate whether the column count of the table matches a specified count.\n\n\nValidate.conjointly\nPerform multiple row-wise validations for joint validity."
  },
  {
    "objectID": "reference/index.html#column-selection",
    "href": "reference/index.html#column-selection",
    "title": "API Reference",
    "section": "",
    "text": "A flexible way to select columns for validation is to use the col() function along with column selection helper functions. A combination of col() + starts_with(), matches(), etc., allows for the selection of multiple target columns (mapping a validation across many steps). Furthermore, the col() function can be used to declare a comparison column (e.g., for the value= argument in many col_vals_*() methods) when you can’t use a fixed value for comparison.\n\n\n\ncol\nHelper function for referencing a column in the input table.\n\n\nstarts_with\nSelect columns that start with specified text.\n\n\nends_with\nSelect columns that end with specified text.\n\n\ncontains\nSelect columns that contain specified text.\n\n\nmatches\nSelect columns that match a specified regular expression pattern.\n\n\neverything\nSelect all columns.\n\n\nfirst_n\nSelect the first n columns in the column list.\n\n\nlast_n\nSelect the last n columns in the column list.\n\n\nexpr_col\nCreate a column expression for use in conjointly() validation."
  },
  {
    "objectID": "reference/index.html#interrogation-and-reporting",
    "href": "reference/index.html#interrogation-and-reporting",
    "title": "API Reference",
    "section": "",
    "text": "The validation plan is put into action when interrogate() is called. The workflow for performing a comprehensive validation is then: (1) Validate(), (2) adding validation steps, (3) interrogate(). After interrogation of the data, we can view a validation report table (by printing the object or using get_tabular_report()), extract key metrics, or we can split the data based on the validation results (with get_sundered_data()).\n\n\n\nValidate.interrogate\nExecute each validation step against the table and store the results.\n\n\nValidate.get_tabular_report\nValidation report as a GT table.\n\n\nValidate.get_step_report\nGet a detailed report for a single validation step.\n\n\nValidate.get_json_report\nGet a report of the validation results as a JSON-formatted string.\n\n\nValidate.get_sundered_data\nGet the data that passed or failed the validation steps.\n\n\nValidate.get_data_extracts\nGet the rows that failed for each validation step.\n\n\nValidate.all_passed\nDetermine if every validation step passed perfectly, with no failing test units.\n\n\nValidate.assert_passing\nRaise an AssertionError if all tests are not passing.\n\n\nValidate.n\nProvides a dictionary of the number of test units for each validation step.\n\n\nValidate.n_passed\nProvides a dictionary of the number of test units that passed for each validation step.\n\n\nValidate.n_failed\nProvides a dictionary of the number of test units that failed for each validation step.\n\n\nValidate.f_passed\nProvides a dictionary of the fraction of test units that passed for each validation step.\n\n\nValidate.f_failed\nProvides a dictionary of the fraction of test units that failed for each validation step.\n\n\nValidate.warning\nGet the ‘warning’ level status for each validation step.\n\n\nValidate.error\nGet the ‘error’ level status for each validation step.\n\n\nValidate.critical\nGet the ‘critical’ level status for each validation step."
  },
  {
    "objectID": "reference/index.html#inspection-and-assistance",
    "href": "reference/index.html#inspection-and-assistance",
    "title": "API Reference",
    "section": "",
    "text": "The Inspection and Assistance group contains functions that are helpful for getting to grips on a new data table. Use the DataScan class to get a quick overview of the data, preview() to see the first and last few rows of a table, col_summary_tbl() for a column-level summary of a table, and missing_vals_tbl() to see where there are missing values in a table. Several datasets included in the package can be accessed via the load_dataset() function. On the assistance side, the assistant() function can be used to get help with Pointblank.\n\n\n\nDataScan\nGet a summary of a dataset.\n\n\npreview\nDisplay a table preview that shows some rows from the top, some from the bottom.\n\n\ncol_summary_tbl\nGenerate a column-level summary table of a dataset.\n\n\nmissing_vals_tbl\nDisplay a table that shows the missing values in the input table.\n\n\nassistant\nChat with the PbA (Pointblank Assistant) about your data validation needs.\n\n\nload_dataset\nLoad a dataset hosted in the library as specified table type."
  },
  {
    "objectID": "reference/index.html#utility-functions",
    "href": "reference/index.html#utility-functions",
    "title": "API Reference",
    "section": "",
    "text": "The Utility Functions group contains functions that are useful accessing metadata about the target data. Use get_column_count() or get_row_count() to get the number of columns or rows in a table. The get_action_metadata() function is useful when building custom actions since it returns metadata about the validation step that’s triggering the action. Lastly, the config() utility lets us set global configuration parameters.\n\n\n\nget_column_count\nGet the number of columns in a table.\n\n\nget_row_count\nGet the number of rows in a table.\n\n\nget_action_metadata\nAccess step-level metadata when authoring custom actions.\n\n\nget_validation_summary\nAccess validation summary information when authoring final actions.\n\n\nconfig\nConfiguration settings for the Pointblank library."
  },
  {
    "objectID": "reference/index.html#prebuilt-actions",
    "href": "reference/index.html#prebuilt-actions",
    "title": "API Reference",
    "section": "",
    "text": "The Prebuilt Actions group contains a function that can be used to send a Slack notification when validation steps exceed failure threshold levels or just to provide a summary of the validation results, including the status, number of steps, passing and failing steps, table information, and timing details.\n\n\n\nsend_slack_notification\nCreate a Slack notification function using a webhook URL."
  },
  {
    "objectID": "reference/Validate.error.html",
    "href": "reference/Validate.error.html",
    "title": "Validate.error",
    "section": "",
    "text": "Validate.error(i=None, scalar=False)\nGet the ‘error’ level status for each validation step.\nThe ‘error’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the ‘error’ level. Otherwise, the status is False.\nThe ascribed name of ‘error’ is semantic and does not imply that the validation process is halted, it is simply a status indicator that could be used to trigger some action to be taken. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the ‘error’ status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.error.html#parameters",
    "href": "reference/Validate.error.html#parameters",
    "title": "Validate.error",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the ‘error’ status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.error.html#returns",
    "href": "reference/Validate.error.html#returns",
    "title": "Validate.error",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the ‘error’ status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.error.html#examples",
    "href": "reference/Validate.error.html#examples",
    "title": "Validate.error",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have some failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the error() method is used to determine the ‘error’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [3, 4, 9, 7, 2, 3, 8],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.error()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘error’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘error’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘error’ level.\nWe can also visually inspect the ‘error’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:58:05PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #EBBC14\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    30.43\n    40.57\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:58:05 UTC&lt; 1 s2025-04-23 00:58:05 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there are filled gray and yellow circles in the first step (far right side, in the W and E columns) indicating that the ‘warning’ and ‘error’ thresholds were met. The other steps have empty gray and yellow circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘error’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.error(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had the ‘error’ threshold met."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html",
    "href": "reference/Validate.col_schema_match.html",
    "title": "Validate.col_schema_match",
    "section": "",
    "text": "Validate.col_schema_match(\n    schema,\n    complete=True,\n    in_order=True,\n    case_sensitive_colnames=True,\n    case_sensitive_dtypes=True,\n    full_match_dtypes=True,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo columns in the table (and their types) match a predefined schema?\nThe col_schema_match() method works in conjunction with an object generated by the Schema class. That class object is the expectation for the actual schema of the target table. The validation step operates over a single test unit, which is whether the schema matches that of the table (within the constraints enforced by the complete=, and in_order= options)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#parameters",
    "href": "reference/Validate.col_schema_match.html#parameters",
    "title": "Validate.col_schema_match",
    "section": "Parameters",
    "text": "Parameters\n\nschema : Schema\n\nA Schema object that represents the expected schema of the table. This object is generated by the Schema class.\n\ncomplete : bool = True\n\nShould the schema match be complete? If True, then the target table must have all columns specified in the schema. If False, then the table can have additional columns not in the schema (i.e., the schema is a subset of the target table’s columns).\n\nin_order : bool = True\n\nShould the schema match be in order? If True, then the columns in the schema must appear in the same order as they do in the target table. If False, then the order of columns in the schema and the target table can differ.\n\ncase_sensitive_colnames : bool = True\n\nShould the schema match be case-sensitive with regard to column names? If True, then the column names in the schema and the target table must match exactly. If False, then the column names are compared in a case-insensitive manner.\n\ncase_sensitive_dtypes : bool = True\n\nShould the schema match be case-sensitive with regard to column data types? If True, then the column data types in the schema and the target table must match exactly. If False, then the column data types are compared in a case-insensitive manner.\n\nfull_match_dtypes : bool = True\n\nShould the schema match require a full match of data types? If True, then the column data types in the schema and the target table must match exactly. If False then substring matches are allowed, so a schema data type of Int would match a target table data type of Int64.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#returns",
    "href": "reference/Validate.col_schema_match.html#returns",
    "title": "Validate.col_schema_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#preprocessing",
    "href": "reference/Validate.col_schema_match.html#preprocessing",
    "title": "Validate.col_schema_match",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#thresholds",
    "href": "reference/Validate.col_schema_match.html#thresholds",
    "title": "Validate.col_schema_match",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_schema_match.html#examples",
    "href": "reference/Validate.col_schema_match.html#examples",
    "title": "Validate.col_schema_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three columns (string, integer, and float). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n        \"c\": [1.1, 2.2, 3.3, 4.4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aString\n  bInt64\n  cFloat64\n\n\n\n  \n    1\n    apple\n    1\n    1.1\n  \n  \n    2\n    banana\n    6\n    2.2\n  \n  \n    3\n    cherry\n    3\n    3.3\n  \n  \n    4\n    date\n    5\n    4.4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the columns in the table match a predefined schema. A schema can be defined using the Schema class.\n\nschema = pb.Schema(\n    columns=[(\"a\", \"String\"), (\"b\", \"Int64\"), (\"c\", \"Float64\")]\n)\n\nYou can print the schema object to verify that the expected schema is as intended.\n\nprint(schema)\n\nPointblank Schema\n  a: String\n  b: Int64\n  c: Float64\n\n\nNow, we’ll use the col_schema_match() method to validate the table against the expected schema object. There is a single test unit for this validation step (whether the schema matches the table or not).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the schema matches the table. The single test unit passed since the table columns and their types match the schema."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html",
    "href": "reference/Validate.col_vals_lt.html",
    "title": "Validate.col_vals_lt",
    "section": "",
    "text": "Validate.col_vals_lt(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data less than a fixed value or data in another column?\nThe col_vals_lt() validation method checks whether column values in a table are less than a specified value= (the exact comparison used in this function is col_val &lt; value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#parameters",
    "href": "reference/Validate.col_vals_lt.html#parameters",
    "title": "Validate.col_vals_lt",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#returns",
    "href": "reference/Validate.col_vals_lt.html#returns",
    "title": "Validate.col_vals_lt",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_lt.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_lt",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#preprocessing",
    "href": "reference/Validate.col_vals_lt.html#preprocessing",
    "title": "Validate.col_vals_lt",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#thresholds",
    "href": "reference/Validate.col_vals_lt.html#thresholds",
    "title": "Validate.col_vals_lt",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_lt.html#examples",
    "href": "reference/Validate.col_vals_lt.html#examples",
    "title": "Validate.col_vals_lt",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [1, 2, 1, 2, 2, 2],\n        \"c\": [2, 1, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    2\n    1\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    2\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    2\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all less than the value of 10. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=\"a\", value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_lt(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_lt() to check whether the values in column b are less than values in column c.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=\"b\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 1: b is 2 and c is 1.\nRow 2: b is 1 and c is 1."
  },
  {
    "objectID": "blog/intro-pointblank/index.html",
    "href": "blog/intro-pointblank/index.html",
    "title": "Introducing Pointblank",
    "section": "",
    "text": "If you have tabular data (and who doesn’t?) this is the package for you! I’ve long been interested in data quality and so I’ve spent a lot of time building tooling that makes it possible to perform data quality checks. And there’s so many reasons to care about data quality. If I were to put down just one good reason for why data quality is worth your time it is because having good data quality strongly determines the quality of decisions.\nHaving the ability to distinguish bad data from good data is the first step in solving DQ issues, and the sustained practice of doing data validation will guard against intrusions of poor-quality data. Pointblank has been designed to really help here. Though it’s a fairly new package it is currently quite capable. And it’s available in PyPI, so you can install it by using:"
  },
  {
    "objectID": "blog/intro-pointblank/index.html#how-pointblank-transforms-your-data-validation-workflow",
    "href": "blog/intro-pointblank/index.html#how-pointblank-transforms-your-data-validation-workflow",
    "title": "Introducing Pointblank",
    "section": "How Pointblank Transforms Your Data Validation Workflow",
    "text": "How Pointblank Transforms Your Data Validation Workflow\nWhat sets Pointblank apart is its intuitive, expressive approach to data validation. Rather than writing dozens of ad-hoc checks scattered throughout your codebase, Pointblank lets you define a comprehensive validation plan with just a few lines of code. The fluent API makes your validation intentions crystal clear, whether you’re ensuring numeric values fall within expected ranges, text fields match specific patterns, or relationships between columns remain consistent.\nBut say you find problems. What are you gonna do about it? Well, Pointblank wants to help at not just finding problems but helping you understand them. When validation failures occur, the detailed reporting capabilities (in the form of beautiful, sharable tables) show you exactly where issues are. Right down to the specific rows and columns. This transforms data validation from a binary pass/fail exercise into a super-insightful diagnostic tool.\n\nHere’s the the best part: Pointblank is designed to work with your existing data stack. Whether you’re using Polars, Pandas, DuckDB, or other database systems, Pointblank tries hard to integrate without forcing you to change your workflow. We also have international spoken language support for reporting, meaning that validation reports can be localized to your team’s preferred language. This making data quality accessible to everyone in your organization (like a team sport!).\n\nAlright! Let’s look at a few demonstrations of Pointblank’s capabilities for data validation."
  },
  {
    "objectID": "blog/intro-pointblank/index.html#the-data-validation-workflow",
    "href": "blog/intro-pointblank/index.html#the-data-validation-workflow",
    "title": "Introducing Pointblank",
    "section": "The Data Validation Workflow",
    "text": "The Data Validation Workflow\nLet’s get right to performing a basic check of a Polars DataFrame. We’ll make use of the included small_table dataset.\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n\nvalidation_1 = (\n    pb.Validate(\n        data=small_table,\n        tbl_name=\"small_table\",\n        label=\"Example Validation\"\n    )\n    .col_vals_lt(columns=\"a\", value=10)\n    .col_vals_between(columns=\"d\", left=0, right=5000)\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])\n    .col_vals_regex(columns=\"b\", pattern=r\"^[0-9]-[a-z]{3}-[0-9]{3}$\")\n    .interrogate()\n)\n\nvalidation_1\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example ValidationPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [0, 5000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^[0-9]-[a-z]{3}-[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:47 UTC&lt; 1 s2025-04-23 00:57:47 UTC\n  \n\n\n\n\n\n\n        \n\n\nThere’s a lot to take in here so let’s break down the code first! Note these three key pieces:\n\nthe Validate(data=...) argument takes a DataFrame (or database table) that you want to validate\nthe methods starting with col_* specify validation steps that run on specific columns\nthe interrogate() method executes the validation plan on the table (it’s the finishing step)\n\nThis common pattern is used in a validation workflow, where Validate and interrogate() bookend a validation plan generated through calling validation methods.\nNow, onto the result: it’s a table! Naturally, we’re using the awesome Great Tables package here in Pointblank to really give you the goods on how the validation went down. Each row in this reporting table represents a single validation step (one for each invocation of a col_vals_*() validation method). Generally speaking, the left side of the validation report tables outlines the key validation rules, and the right side provides the results of each validation step.\nWe tried to keep it simple in principle, but a lot of useful information can be packed into this validation table. Here’s a diagram that describes a few of the important parts of the validation report table:\n\nAll of those numbers under the UNITS, PASS, and FAIL columns have to do with test units, a measure of central importance in Pointblank. Each validation step will execute a type of validation test on the target table. For example, a col_vals_lt() validation step can test that each value in a column is less than a specified number. The key finding that’s reported as a result of this test is the number of test units that pass or fail. This little diagram explains what those numbers mean:\n\nFailing test units can be tied to threshold levels, which can provide a better indication of whether failures should raise some basic awareness or spur you into action. Here’s a validation workflow that sets three failure threshold levels that signal the severity of data quality problems:\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation_2 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\"),\n        tbl_name=\"game_revenue\",\n        label=\"Data validation with threshold levels set.\",\n        thresholds=pb.Thresholds(warning=1, error=20, critical=0.10),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"^[A-Z]{12}[0-9]{3}$\")        # STEP 1\n    .col_vals_gt(columns=\"session_duration\", value=5)                           # STEP 2\n    .col_vals_ge(columns=\"item_revenue\", value=0.02)                            # STEP 3\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])                    # STEP 4\n    .col_vals_in_set(                                                           # STEP 5\n        columns=\"acquisition\",\n        set=[\"google\", \"facebook\", \"organic\", \"crosspromo\", \"other_campaign\"]\n    )\n    .col_vals_not_in_set(columns=\"country\", set=[\"Mongolia\", \"Germany\"])        # STEP 6\n    .col_vals_between(                                                          # STEP 7\n        columns=\"session_duration\",\n        left=10, right=50,\n        pre = lambda df: df.select(pl.median(\"session_duration\"))\n    )\n    .rows_distinct(columns_subset=[\"player_id\", \"session_id\", \"time\"])          # STEP 8\n    .row_count_match(count=2000)                                                # STEP 9\n    .col_exists(columns=\"start_day\")                                            # STEP 10\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Data validation with threshold levels set.Polarsgame_revenueWARNING1ERROR20CRITICAL0.1\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    ^[A-Z]{12}[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #AAAAAA\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #EBBC14\n    3\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    item_revenue\n    0.02\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19410.97\n    590.03\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    5\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    acquisition\n    google, facebook, organic, crosspromo, other_campaign\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19750.99\n    250.01\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #FF3300\n    6\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    country\n    Mongolia, Germany\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17750.89\n    2250.11\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    session_duration\n    [10, 50]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    8\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    player_id, session_id, time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19780.99\n    220.01\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #4CA64C\n    9\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:47 UTC&lt; 1 s2025-04-23 00:57:47 UTC\n  \n\n\n\n\n\n\n        \n\n\nThis data validation makes use of the many validation methods available in the library. Because thresholds have been set at the Validate(thresholds=) parameter, we can now see where certain validation steps have greater amounts of failures. Any validation steps with green indicators passed with flying colors, whereas: (1) gray indicates the ‘warning’ condition was met (at least one test unit failing), (2) yellow is for the ‘error’ condition (20 or more test units failing), and (3) red means ‘critical’ and that’s tripped when 10% of all test units are failing ones.\nReporting tables are essential to the package and they help communicate what went wrong (or well) in a validation workflow. Now let’s look at some additional reporting that Pointblank can give you to better understand where things might’ve gone wrong."
  },
  {
    "objectID": "blog/intro-pointblank/index.html#reporting-for-individual-validation-steps",
    "href": "blog/intro-pointblank/index.html#reporting-for-individual-validation-steps",
    "title": "Introducing Pointblank",
    "section": "Reporting for Individual Validation Steps",
    "text": "Reporting for Individual Validation Steps\nThe second validation step of the previous data validation showed 18 failing test units. That translates to 18 spots in a 2,000 row DataFrame where a data quality assertion failed. We often would like to know exactly what that failing data is; it’s usually the next step toward addressing data quality issues.\nPointblank offers a method that gives you a tabular report on a specific step: get_step_report(). The previous tables you’ve seen (the validation report table) dealt with providing a summary of all validation steps. In contrast, a focused report on a single step can help to get to the heart of a data quality issue. Here’s how that looks for Step 2:\n\nvalidation_2.get_step_report(i=2)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2ASSERTION session_duration &gt; 518 / 2000 TEST UNIT FAILURES IN COLUMN 8 EXTRACT OF FIRST 10 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    620\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:25:18+00:00\n    iap\n    offer4\n    17.991\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    621\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:26:24+00:00\n    iap\n    offer5\n    26.09\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    622\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-t4y8bjcu\n    2015-01-11 07:24:24+00:00\n    2015-01-11 07:28:36+00:00\n    ad\n    ad_15sec\n    0.53\n    5.0\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n\n\n\n\n\n\n        \n\n\nThis report provides the 18 rows where the failure occurred. If you scroll the table to the right you’ll see the column that underwent testing (session_duration) is highlighted in red. All of these values are 5.0 or less, which is in violation of the assertion (in the header) that session_duration &gt; 5.\nThese types of bespoke reports are useful for finding a needle in a haystack. Another good use for a step report is when validating a table schema. Using the col_schema_match() validation method with a table schema prepared with the Schema class allows us to verify our understanding of the table structure. Here is a validation that performs a schema validation with the small_table dataset prepared as a DuckDB table:\n\nimport pointblank as pb\n\n# Create a schema for the target table (`small_table` as a DuckDB table)\nschema = pb.Schema(\n    columns=[\n        (\"date_time\", \"timestamp(6)\"),\n        (\"dates\", \"date\"),\n        (\"a\", \"int64\"),\n        (\"b\",),\n        (\"c\",),\n        (\"d\", \"float64\"),\n        (\"e\", [\"bool\", \"boolean\"]),\n        (\"f\", \"str\"),\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform a schema check\nvalidation_3 = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"duckdb\"),\n        tbl_name=\"small_table\",\n        label=\"Schema check\"\n    )\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation_3\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Schema checkDuckDBsmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:48 UTC&lt; 1 s2025-04-23 00:57:48 UTC\n  \n\n\n\n\n\n\n        \n\n\nThis step fails, but the validation report table doesn’t tell us how (or where). Using `get_step_report() will show us what the underlying issues are:\n\nvalidation_3.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✗COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    date_time\n    timestamp(6)\n    1\n    date_time\n    ✓\n    timestamp(6)\n    ✓\n  \n  \n    2\n    date\n    date\n    2\n    dates\n    ✗\n    date\n    —\n  \n  \n    3\n    a\n    int64\n    3\n    a\n    ✓\n    int64\n    ✓\n  \n  \n    4\n    b\n    string\n    4\n    b\n    ✓\n    —\n    \n  \n  \n    5\n    c\n    int64\n    5\n    c\n    ✓\n    —\n    \n  \n  \n    6\n    d\n    float64\n    6\n    d\n    ✓\n    float64\n    ✓\n  \n  \n    7\n    e\n    boolean\n    7\n    e\n    ✓\n    bool | boolean\n    ✓\n  \n  \n    8\n    f\n    string\n    8\n    f\n    ✓\n    str\n    ✗\n  \n\n  \n  \n  \n    Supplied Column Schema:[('date_time', 'timestamp(6)'), ('dates', 'date'), ('a', 'int64'), ('b',), ('c',), ('d', 'float64'), ('e', ['bool', 'boolean']), ('f', 'str')]\n  \n\n\n\n\n\n\n        \n\n\nThe step report here shows the target table’s schema on the left side and the expectation of the schema on the right side. There appears to be two problems with our supplied schema:\n\nthe second column is actually date instead of dates\nthe dtype of the f column is \"string\" and not \"str\"\n\nThe convenience of this step report means we only have to look at one display of information, rather than having to collect up the individual pieces and make careful comparisons."
  },
  {
    "objectID": "blog/intro-pointblank/index.html#much-more-in-store",
    "href": "blog/intro-pointblank/index.html#much-more-in-store",
    "title": "Introducing Pointblank",
    "section": "Much More in Store",
    "text": "Much More in Store\nPointblank tries really hard to make it easy for you to test your data. All sorts of input tables are supported since we integrate with the brilliant Narwhals and Ibis libraries. And even through the project has only started four months ago, we already have an extensive catalog of well-tested validation methods.\nWe care a great deal about documentation so much recent effort has been placed on getting the User Guide written. We hope it provides for gentle introduction to the major features of the library. If you want some quick examples to get your imagination going, check out our gallery of examples.\nWe really care about what you want in a validation package, so talk to us :) We just started a Discord so feel free to hop on and ask us anything. Alternatively, we always like to get issues so don’t be shy in letting us know how we could improve!"
  },
  {
    "objectID": "demos/04-sundered-data/index.html",
    "href": "demos/04-sundered-data/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Sundered Data\nSplitting your data into ‘pass’ and ‘fail’ subsets.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Sundering DataPandassmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-04-23 00:57:43 UTC&lt; 1 s2025-04-23 00:57:43 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows4Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    3\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    4\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Sundering Data\"\n    )\n    .col_vals_gt(columns=\"d\", value=1000)\n    .col_vals_le(columns=\"c\", value=5)\n    .interrogate()\n)\n\nvalidation\npb.preview(validation.get_sundered_data(type=\"pass\"))\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/apply-checks-to-several-columns/index.html",
    "href": "demos/apply-checks-to-several-columns/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Apply Validation Rules to Multiple Columns\nCreate multiple validation steps by using a list of column names with columns=.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:36Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    a\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    c\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    d\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:36 UTC&lt; 1 s2025-04-23 00:57:36 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_ge(columns=[\"a\", \"c\", \"d\"], value=0)   # check values in 'a', 'c', and 'd'\n    .col_exists(columns=[\"date_time\", \"date\"])       # check for the existence of two columns\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/column-selector-functions/index.html",
    "href": "demos/column-selector-functions/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Column Selector Functions: Easily Pick Columns\nUse column selector functions in the columns= argument to conveniently choose columns.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:30Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    session_duration\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    ^[A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    session_id\n    ^[A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    acquisition\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    6\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    country\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    8\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    session_id\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    9\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    item_type\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    item_name\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    11\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    acquisition\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    12\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    country\n    (.|\\s)*\\S(.|\\s)*\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:30 UTC&lt; 1 s2025-04-23 00:57:30 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport narwhals.selectors as ncs\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\")\n    )\n    .col_vals_ge(\n        columns=pb.matches(\"rev|dur\"),  # check values in columns having 'rev' or 'dur' in name\n        value=0\n    )\n    .col_vals_regex(\n        columns=pb.ends_with(\"_id\"),    # check values in columns with names ending in '_id'\n        pattern=r\"^[A-Z]{12}\\d{3}\"\n    )\n    .col_vals_not_null(\n        columns=pb.last_n(2)            # check that the last two columns don't have Null values\n    )\n    .col_vals_regex(\n        columns=ncs.string(),           # check that all string columns are non-empty strings\n        pattern=r\"(.|\\s)*\\S(.|\\s)*\"\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/02-advanced/index.html",
    "href": "demos/02-advanced/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Advanced Validation\nA validation with a comprehensive set of rules.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Comprehensive validation examplePolarsgame_revenueWARNING0.1ERROR0.25CRITICAL0.35\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    ^[A-Z]{12}[0-9]{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    item_revenue\n    0.02\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19410.97\n    590.03\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    5\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    acquisition\n    google, facebook, organic, crosspromo, other_campaign\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19750.99\n    250.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #AAAAAA\n    6\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    country\n    Mongolia, Germany\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17750.89\n    2250.11\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    7\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    session_duration\n    [10, 50]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C66\n    8\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    player_id, session_id, time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19780.99\n    220.01\n    ○\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    9\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    10\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    11\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_type\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    12\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_name\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    13\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    item_revenue\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    14\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    start_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:22 UTC&lt; 1 s2025-04-23 00:57:22 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\nimport narwhals as nw\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"polars\"),\n        tbl_name=\"game_revenue\",\n        label=\"Comprehensive validation example\",\n        thresholds=pb.Thresholds(warning=0.10, error=0.25, critical=0.35),\n    )\n    .col_vals_regex(columns=\"player_id\", pattern=r\"^[A-Z]{12}[0-9]{3}$\")        # STEP 1\n    .col_vals_gt(columns=\"session_duration\", value=5)                           # STEP 2\n    .col_vals_ge(columns=\"item_revenue\", value=0.02)                            # STEP 3\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])                    # STEP 4\n    .col_vals_in_set(                                                           # STEP 5\n        columns=\"acquisition\",\n        set=[\"google\", \"facebook\", \"organic\", \"crosspromo\", \"other_campaign\"]\n    )\n    .col_vals_not_in_set(columns=\"country\", set=[\"Mongolia\", \"Germany\"])        # STEP 6\n    .col_vals_between(                                                          # STEP 7\n        columns=\"session_duration\",\n        left=10, right=50,\n        pre = lambda df: df.select(pl.median(\"session_duration\"))\n    )\n    .rows_distinct(columns_subset=[\"player_id\", \"session_id\", \"time\"])          # STEP 8\n    .row_count_match(count=2000)                                                # STEP 9\n    .col_count_match(count=11)                                                  # STEP 10\n    .col_vals_not_null(columns=pb.starts_with(\"item\"))                          # STEPS 11-13\n    .col_exists(columns=\"start_day\")                                            # STEP 14\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    6\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:08:56+00:00\n    ad\n    ad_10sec\n    0.07\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    7\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:14:08+00:00\n    ad\n    ad_10sec\n    0.08\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    8\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:21:44+00:00\n    ad\n    ad_30sec\n    1.17\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    9\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 12:24:20+00:00\n    ad\n    ad_10sec\n    0.14\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    10\n    FXWUORGYNJAE271\n    FXWUORGYNJAE271-et7bs639\n    2015-01-01 15:17:18+00:00\n    2015-01-01 15:19:36+00:00\n    ad\n    ad_5sec\n    0.08\n    30.7\n    2015-01-01\n    organic\n    Canada\n  \n  \n    1991\n    VPNRYLMBKJGT925\n    VPNRYLMBKJGT925-vt26q9gb\n    2015-01-21 01:07:24+00:00\n    2015-01-21 01:26:12+00:00\n    ad\n    ad_survey\n    0.72\n    24.9\n    2015-01-21\n    other_campaign\n    Germany\n  \n  \n    1992\n    JVBZCPKXHFMU491\n    JVBZCPKXHFMU491-wvi6hs2t\n    2015-01-21 01:49:36+00:00\n    2015-01-21 01:53:36+00:00\n    iap\n    gold6\n    41.99\n    7.1\n    2015-01-07\n    organic\n    United States\n  \n  \n    1993\n    JVBZCPKXHFMU491\n    JVBZCPKXHFMU491-wvi6hs2t\n    2015-01-21 01:49:36+00:00\n    2015-01-21 01:55:42+00:00\n    iap\n    gems3\n    17.49\n    7.1\n    2015-01-07\n    organic\n    United States\n  \n  \n    1994\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:01:20+00:00\n    ad\n    ad_playable\n    1.116\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1995\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:14+00:00\n    ad\n    ad_15sec\n    0.225\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/check-row-column-counts/index.html",
    "href": "demos/check-row-column-counts/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Verifying Row and Column Counts\nCheck the dimensions of the table with the *_count_match() validation methods.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:15DuckDB\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    2000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    row_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            row_count_match()\n        \n        \n        \n    —\n    ≠ 0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:15 UTC&lt; 1 s2025-04-23 00:57:15 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\")\n    )\n    .col_count_match(count=11)                       # expect 11 columns in the table\n    .row_count_match(count=2000)                     # expect 2,000 rows in the table\n    .row_count_match(count=0, inverse=True)          # expect that the table has rows\n    .col_count_match(                                # compare column count against\n        count=pb.load_dataset(                       # that of another table\n            dataset=\"game_revenue\", tbl_type=\"pandas\"\n        )\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/schema-check/index.html",
    "href": "demos/schema-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Check the Schema of a Table\nThe schema of a table can be flexibly defined with Schema and verified with col_schema_match().\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:09Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:09 UTC&lt; 1 s2025-04-23 00:57:09 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [\"apple\", \"banana\", \"cherry\", \"date\"],\n        \"b\": [1, 6, 3, 5],\n        \"c\": [1.1, 2.2, 3.3, 4.4],\n    }\n)\n\n# Use the Schema class to define the column schema as loosely or rigorously as required\nschema = pb.Schema(\n    columns=[\n        (\"a\", \"String\"),          # Column 'a' has dtype 'String'\n        (\"b\", [\"Int\", \"Int64\"]),  # Column 'b' has dtype 'Int' or 'Int64'\n        (\"c\", )                   # Column 'c' follows 'b' but we don't specify a dtype here\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform the schema check\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation"
  },
  {
    "objectID": "demos/06-step-report-schema-check/index.html",
    "href": "demos/06-step-report-schema-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Step Report: Schema Check\nWhen a schema doesn’t match, a step report gives you the details.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Step report for a schema checkDuckDBsmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:02 UTC&lt; 1 s2025-04-23 00:57:03 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1 ✗COLUMN SCHEMA MATCHCOMPLETEIN ORDERCOLUMN ≠ columnDTYPE ≠ dtypefloat ≠ float64\n  \n\n  \n    TARGET\n  \n  \n    EXPECTED\n  \n\n\n  \n  COLUMN\n  DATA TYPE\n  \n  COLUMN\n  \n  DATA TYPE\n  \n\n\n\n  \n    1\n    date_time\n    timestamp(6)\n    1\n    date_time\n    ✓\n    timestamp\n    ✗\n  \n  \n    2\n    date\n    date\n    2\n    dates\n    ✗\n    date\n    —\n  \n  \n    3\n    a\n    int64\n    3\n    a\n    ✓\n    int64\n    ✓\n  \n  \n    4\n    b\n    string\n    4\n    b\n    ✓\n    —\n    \n  \n  \n    5\n    c\n    int64\n    5\n    c\n    ✓\n    —\n    \n  \n  \n    6\n    d\n    float64\n    6\n    d\n    ✓\n    float64\n    ✓\n  \n  \n    7\n    e\n    boolean\n    7\n    e\n    ✓\n    bool | boolean\n    ✓\n  \n  \n    8\n    f\n    string\n    8\n    f\n    ✓\n    str\n    ✗\n  \n\n  \n  \n  \n    Supplied Column Schema:[('date_time', 'timestamp'), ('dates', 'date'), ('a', 'int64'), ('b',), ('c',), ('d', 'float64'), ('e', ['bool', 'boolean']), ('f', 'str')]\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\n# Create a schema for the target table (`small_table` as a DuckDB table)\nschema = pb.Schema(\n    columns=[\n        (\"date_time\", \"timestamp\"),     # this dtype doesn't match\n        (\"dates\", \"date\"),              # this column name doesn't match\n        (\"a\", \"int64\"),\n        (\"b\",),                         # omit dtype to not check for it\n        (\"c\",),                         # \"\"   \"\"   \"\"  \"\"\n        (\"d\", \"float64\"),\n        (\"e\", [\"bool\", \"boolean\"]),     # try several dtypes (second one matches)\n        (\"f\", \"str\"),                   # this dtype doesn't match\n    ]\n)\n\n# Use the `col_schema_match()` validation method to perform a schema check\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"duckdb\"),\n        tbl_name=\"small_table\",\n        label=\"Step report for a schema check\"\n    )\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\nvalidation.get_step_report(i=1)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows13Columns8\n  \n\n  \n  date_timetimestamp\n  datedate\n  aint64\n  bstring\n  cint64\n  dfloat64\n  eboolean\n  fstring\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    NULL\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    NULL\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/expect-no-duplicate-values/index.html",
    "href": "demos/expect-no-duplicate-values/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Checking for Duplicate Values\nTo check for duplicate values down a column, use rows_distinct() with a columns_subset= value.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:56:56Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-04-23 00:56:56 UTC&lt; 1 s2025-04-23 00:56:56 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .rows_distinct(columns_subset=\"b\")   # expect no duplicate values in 'b'\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/mutate-table-in-step/index.html",
    "href": "demos/mutate-table-in-step/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Mutate the Table in a Validation Step\nFor far more specialized validations, modify the table with the pre= argument before checking it.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:56:50Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    a\n    [3, 6]\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_equal\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_eq()\n        \n        \n        \n    b_len\n    9\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:56:50 UTC&lt; 1 s2025-04-23 00:56:50 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport polars as pl\nimport narwhals as nw\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_between(\n        columns=\"a\",\n        left=3, right=6,\n        pre=lambda df: df.select(pl.median(\"a\"))    # Use a Polars expression to aggregate\n    )\n    .col_vals_eq(\n        columns=\"b_len\",\n        value=9,\n        pre=lambda dfn: dfn.with_columns(           # Use a Narwhals expression, identified\n            b_len=nw.col(\"b\").str.len_chars()       # by the 'dfn' here\n        )\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/01-starter/index.html",
    "href": "demos/01-starter/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Starter Validation\nA validation with the basics.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    A starter validationPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    date_time\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:56:44 UTC&lt; 1 s2025-04-23 00:56:44 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate( # Use pb.Validate to start\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"),\n        tbl_name=\"small_table\",\n        label=\"A starter validation\"\n    )\n    .col_vals_gt(columns=\"d\", value=1000)       # STEP 1 |\n    .col_vals_le(columns=\"c\", value=5)          # STEP 2 | &lt;-- Build up a validation plan\n    .col_exists(columns=[\"date\", \"date_time\"])  # STEP 3 |\n    .interrogate()  # This will execute all validation steps and collect intel\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/index.html",
    "href": "demos/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "A Selection of Examples\n\n\n\n\n\n\nStarter Validation\n\n\n\nA validation with the basics.\n\n\n\n\n\n\nAdvanced Validation\n\n\n\nA validation with a comprehensive set of rules.\n\n\n\n\n\n\nData Extracts\n\n\n\nPulling out data extracts that highlight rows with validation failures.\n\n\n\n\n\n\nSundered Data\n\n\n\nSplitting your data into ‘pass’ and ‘fail’ subsets.\n\n\n\n\n\n\nStep Reports for Column Data Checks\n\n\n\nA step report for column checks shows what went wrong.\n\n\n\n\n\n\nStep Report for a Schema Check\n\n\n\nWhen a schema doesn’t match, a step report gives you the details.\n\n\n\n\n\n\n\nNumeric Comparisons Perform comparisons of values in columns to fixed values.\nComparison Checks Across Columns Perform comparisons of values in columns to values in other columns.\nApply Validation Rules to Multiple Columns Create multiple validation steps by using a list of column names with columns=.\nChecks for Missing Values Perform validations that check whether missing/NA/Null values are present.\nExpectations with a Text Pattern With col_vals_regex(), check for conformance to a regular expression.\nSet Membership Perform validations that check whether values are part of a set (or not part of one).\nExpect No Duplicate Rows We can check for duplicate rows in the table with rows_distinct().\nChecking for Duplicate Values To check for duplicate values down a column, use rows_distinct() with a columns_subset= value.\nCustom Expression for Checking Column Values A column expression can be used to check column values. Just use col_vals_expr() for this.\nMutate the Table in a Validation Step For far more specialized validations, modify the table with the pre= argument before checking it.\nVerifying Row and Column Counts Check the dimensions of the table with the *_count_match() validation methods.\nSet Failure Threshold Levels Set threshold levels to better gauge adverse data quality.\nColumn Selector Functions: Easily Pick Columns Use column selector functions in the columns= argument to conveniently choose columns.\nCheck the Schema of a Table The schema of a table can be flexibly defined with Schema and verified with col_schema_match().\nUsing Parquet Data A Parquet dataset can be used for data validation, thanks to Ibis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Data validation made beautiful and powerful\nPointblank is a powerful, yet elegant data validation framework for Python that transforms how you ensure data quality. With its intuitive, chainable API, you can quickly validate your data against comprehensive quality checks and visualize results through stunning, interactive reports that make data issues immediately actionable.\nWhether you’re a data scientist, data engineer, or analyst, Pointblank helps you catch data quality issues before they impact your analyses or downstream systems."
  },
  {
    "objectID": "index.html#getting-started-in-30-seconds",
    "href": "index.html#getting-started-in-30-seconds",
    "title": "Pointblank",
    "section": "Getting Started in 30 Seconds",
    "text": "Getting Started in 30 Seconds\nimport pointblank as pb\n\nvalidation = (\n   pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n   .col_vals_gt(columns=\"d\", value=100)             # Validate values &gt; 100\n   .col_vals_le(columns=\"c\", value=5)               # Validate values &lt;= 5\n   .col_exists(columns=[\"date\", \"date_time\"])       # Check columns exist\n   .interrogate()                                   # Execute and collect results\n)\n\n# Get the validation report from the REPL with:\nvalidation.get_tabular_report().show()\n\n# From a notebook simply use:\nvalidation"
  },
  {
    "objectID": "index.html#real-world-example",
    "href": "index.html#real-world-example",
    "title": "Pointblank",
    "section": "Real-World Example",
    "text": "Real-World Example\nimport pointblank as pb\nimport polars as pl\n\n# Load your data\nsales_data = pl.read_csv(\"sales_data.csv\")\n\n# Create a comprehensive validation\nvalidation = (\n   pb.Validate(\n      data=sales_data,\n      tbl_name=\"sales_data\",           # Name of the table for reporting\n      label=\"Real-world example.\",     # Label for the validation, appears in reports\n      thresholds=(0.01, 0.02, 0.05),   # Set thresholds for warnings, errors, and critical issues\n      actions=pb.Actions(              # Define actions for any threshold exceedance\n         critical=\"Major data quality issue found in step {step} ({time}).\"\n      ),\n      final_actions=pb.FinalActions(   # Define final actions for the entire validation\n         pb.send_slack_notification(\n            webhook_url=\"https://hooks.slack.com/services/your/webhook/url\"\n         )\n      ),\n      brief=True,                      # Add automatically-generated briefs for each step\n   )\n   .col_vals_between(            # Check numeric ranges with precision\n      columns=[\"price\", \"quantity\"],\n      left=0, right=1000\n   )\n   .col_vals_not_null(           # Ensure that columns ending with '_id' don't have null values\n      columns=pb.ends_with(\"_id\")\n   )\n   .col_vals_regex(              # Validate patterns with regex\n      columns=\"email\",\n      pattern=\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n   )\n   .col_vals_in_set(             # Check categorical values\n      columns=\"status\",\n      set=[\"pending\", \"shipped\", \"delivered\", \"returned\"]\n   )\n   .conjointly(                  # Combine multiple conditions\n      lambda df: pb.expr_col(\"revenue\") == pb.expr_col(\"price\") * pb.expr_col(\"quantity\"),\n      lambda df: pb.expr_col(\"tax\") &gt;= pb.expr_col(\"revenue\") * 0.05\n   )\n   .interrogate()\n)\nMajor data quality issue found in step 7 (2025-04-16 15:03:04.685612+00:00).\n# Get an HTML report you can share with your team\nvalidation.get_tabular_report().show(\"browser\")\n\n\n\n# Get a report of failing records from a specific step\nvalidation.get_step_report(i=3).show(\"browser\")  # Get failing records from step 3"
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "Pointblank",
    "section": "Join the Community",
    "text": "Join the Community\nWe’d love to hear from you! Connect with us:\n\nGitHub Issues for bug reports and feature requests\nDiscord server for discussions and help\nContributing guidelines if you’d like to help improve Pointblank"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Pointblank",
    "section": "Installation",
    "text": "Installation\nYou can install Pointblank using pip:\npip install pointblank\nYou can also install Pointblank from Conda-Forge by using:\nconda install conda-forge::pointblank\nIf you don’t have Polars or Pandas installed, you’ll need to install one of them to use Pointblank.\npip install \"pointblank[pl]\" # Install Pointblank with Polars\npip install \"pointblank[pd]\" # Install Pointblank with Pandas\nTo use Pointblank with DuckDB, MySQL, PostgreSQL, or SQLite, install Ibis with the appropriate backend:\npip install \"pointblank[duckdb]\"   # Install Pointblank with Ibis + DuckDB\npip install \"pointblank[mysql]\"    # Install Pointblank with Ibis + MySQL\npip install \"pointblank[postgres]\" # Install Pointblank with Ibis + PostgreSQL\npip install \"pointblank[sqlite]\"   # Install Pointblank with Ibis + SQLite"
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "Pointblank",
    "section": "Technical Details",
    "text": "Technical Details\nPointblank uses Narwhals to work with Polars and Pandas DataFrames, and integrates with Ibis for database and file format support. This architecture provides a consistent API for validating tabular data from various sources."
  },
  {
    "objectID": "index.html#contributing-to-pointblank",
    "href": "index.html#contributing-to-pointblank",
    "title": "Pointblank",
    "section": "Contributing to Pointblank",
    "text": "Contributing to Pointblank\nThere are many ways to contribute to the ongoing development of Pointblank. Some contributions can be simple (like fixing typos, improving documentation, filing issues for feature requests or problems, etc.) and others might take more time and care (like answering questions and submitting PRs with code changes). Just know that anything you can do to help would be very much appreciated!\nPlease read over the contributing guidelines for information on how to get started."
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "Pointblank",
    "section": "Roadmap",
    "text": "Roadmap\nWe’re actively working on enhancing Pointblank with:\n\nAdditional validation methods for comprehensive data quality checks\nAdvanced logging capabilities\nMessaging actions (Slack, email) for threshold exceedances\nLLM-powered validation suggestions and data dictionary generation\nJSON/YAML configuration for pipeline portability\nCLI utility for validation from the command line\nExpanded backend support and certification\nHigh-quality documentation and examples\n\nIf you have any ideas for features or improvements, don’t hesitate to share them with us! We are always looking for ways to make Pointblank better."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Pointblank",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that the Pointblank project is released with a contributor code of conduct. By participating in this project you agree to abide by its terms."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Pointblank",
    "section": "📄 License",
    "text": "📄 License\nPointblank is licensed under the MIT license.\n© Posit Software, PBC."
  },
  {
    "objectID": "index.html#governance",
    "href": "index.html#governance",
    "title": "Pointblank",
    "section": "🏛️ Governance",
    "text": "🏛️ Governance\nThis project is primarily maintained by Rich Iannone. Other authors may occasionally assist with some of these duties."
  },
  {
    "objectID": "demos/expect-no-duplicate-rows/index.html",
    "href": "demos/expect-no-duplicate-rows/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Expect No Duplicate Rows\nWe can check for duplicate rows in the table with rows_distinct().\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:56:37Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    rows_distinct\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            rows_distinct()\n        \n        \n        \n    ALL COLUMNS\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-04-23 00:56:37 UTC&lt; 1 s2025-04-23 00:56:37 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .rows_distinct()    # expect no duplicate rows\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/numeric-comparisons/index.html",
    "href": "demos/numeric-comparisons/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Numeric Comparisons\nPerform comparisons of values in columns to fixed values.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:56:41Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    10000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    a\n    1\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    4\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    50.38\n    80.62\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    5\n    \n        \n            \n\n    col_vals_not_equal\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ne()\n        \n        \n        \n    a\n    7\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    120.92\n    10.08\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    6\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    c\n    [0, 15]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-04-23 00:56:41 UTC&lt; 1 s2025-04-23 00:56:41 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_gt(columns=\"d\", value=1000)            # values in 'd' &gt; 1000\n    .col_vals_lt(columns=\"d\", value=10000)           # values in 'd' &lt; 10000\n    .col_vals_ge(columns=\"a\", value=1)               # values in 'a' &gt;= 1\n    .col_vals_le(columns=\"c\", value=5)               # values in 'c' &lt;= 5\n    .col_vals_ne(columns=\"a\", value=7)               # values in 'a' not equal to 7\n    .col_vals_between(columns=\"c\", left=0, right=15) # 0 &lt;= 'c' values &lt;= 15\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/col-vals-custom-expr/index.html",
    "href": "demos/col-vals-custom-expr/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Custom Expression for Checking Column Values\nA column expression can be used to check column values. Just use col_vals_expr() for this.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:56:47Pandas\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        \n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:56:47 UTC&lt; 1 s2025-04-23 00:56:47 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\n    )\n    .col_vals_expr(expr=lambda df: (df[\"d\"] % 1 != 0) & (df[\"a\"] &lt; 10))  # Pandas column expr\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/05-step-report-column-check/index.html",
    "href": "demos/05-step-report-column-check/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Step Report: Column Data Checks\nA step report for column checks shows what went wrong.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Step reports for column data checksPolarssmall_table\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    c\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    \\d-[a-z]{3}-\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:56:53 UTC&lt; 1 s2025-04-23 00:56:53 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION c ≥ 44 / 13 TEST UNIT FAILURES IN COLUMN 5 EXTRACT OF ALL 4 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 2 ✓ASSERTION b matches regex \\d-[a-z]{3}-\\d{3}13 TEST UNITS ALL PASSED IN COLUMN 4PREVIEW OF TARGET TABLE:\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\"),\n        tbl_name=\"small_table\",\n        label=\"Step reports for column data checks\"\n    )\n    .col_vals_ge(columns=\"c\", value=4, na_pass=True)                # has failing test units\n    .col_vals_regex(columns=\"b\", pattern=r\"\\d-[a-z]{3}-\\d{3}\")      # no failing test units\n    .interrogate()\n)\n\nvalidation\nvalidation.get_step_report(i=1)\nvalidation.get_step_report(i=2)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/comparisons-across-columns/index.html",
    "href": "demos/comparisons-across-columns/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Comparison Checks Across Columns\nPerform comparisons of values in columns to values in other columns.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:56:59Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    a\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [c, 12000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:56:59 UTC&lt; 1 s2025-04-23 00:56:59 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_lt(columns=\"a\", value=pb.col(\"c\"))     # values in 'a' &gt; values in 'c'\n    .col_vals_between(\n        columns=\"d\",                                 # values in 'd' are between values\n        left=pb.col(\"c\"),                            # in 'c' and the fixed value of 12,000;\n        right=12000,                                 # any missing values encountered result\n        na_pass=True                                 # in a passing test unit\n    )\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/expect-text-pattern/index.html",
    "href": "demos/expect-text-pattern/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Expectations with a Text Pattern\nWith the col_vals_regex(), check for conformance to a regular expression.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:06Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    ^\\d-[a-z]{3}-\\d{3}$\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    f\n    high|low|mid\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:06 UTC&lt; 1 s2025-04-23 00:57:06 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_regex(columns=\"b\", pattern=r\"^\\d-[a-z]{3}-\\d{3}$\")  # check pattern in 'b'\n    .col_vals_regex(columns=\"f\", pattern=r\"high|low|mid\")         # check pattern in 'f'\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/03-data-extracts/index.html",
    "href": "demos/03-data-extracts/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Data Extracts\nPulling out data extracts that highlight rows with validation failures.\n\nValidation with failures at Step 2:\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Validation with test unit failures available as an extractPolarsgame_revenue\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19860.99\n    140.01\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-04-23 00:57:11 UTC&lt; 1 s2025-04-23 00:57:12 UTC\n  \n\n\n\n\n\n\n        \n\n\n\n\nExtract from Step 2 (which has 14 failing test units):\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows14Columns12\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    549\n    QNLVRDEOXFYJ892\n    QNLVRDEOXFYJ892-lz5fmr6k\n    2015-01-10 16:44:17+00:00\n    2015-01-10 16:45:29+00:00\n    iap\n    gold3\n    3.49\n    3.7\n    2015-01-09\n    crosspromo\n    Australia\n  \n  \n    663\n    GFLYJHAPMZWD631\n    GFLYJHAPMZWD631-i2v1bl7a\n    2015-01-11 16:13:24+00:00\n    2015-01-11 16:14:54+00:00\n    iap\n    gems2\n    3.99\n    3.6\n    2015-01-09\n    organic\n    India\n  \n  \n    772\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:39:27+00:00\n    iap\n    offer5\n    11.59\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    773\n    BFNLURISJXTH647\n    BFNLURISJXTH647-6o5hx27z\n    2015-01-12 17:37:39+00:00\n    2015-01-12 17:41:45+00:00\n    iap\n    gems3\n    9.99\n    4.1\n    2015-01-10\n    organic\n    India\n  \n  \n    908\n    KILWZYHRSJEG316\n    KILWZYHRSJEG316-uke7dhqj\n    2015-01-13 22:16:29+00:00\n    2015-01-13 22:17:35+00:00\n    iap\n    offer2\n    10.99\n    3.2\n    2015-01-04\n    organic\n    Denmark\n  \n  \n    1037\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:08:43+00:00\n    iap\n    offer5\n    8.69\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1038\n    JUBDVFHCNQWT198\n    JUBDVFHCNQWT198-9h4xs2pb\n    2015-01-14 16:08:25+00:00\n    2015-01-14 16:11:01+00:00\n    iap\n    offer4\n    5.99\n    3.3\n    2015-01-14\n    organic\n    Philippines\n  \n  \n    1455\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-46cdjzy7\n    2015-01-17 11:25:25+00:00\n    2015-01-17 11:28:01+00:00\n    iap\n    offer4\n    13.99\n    4.6\n    2015-01-14\n    organic\n    United States\n  \n  \n    1516\n    OMCVUAIKSDTR651\n    OMCVUAIKSDTR651-yso9e1b2\n    2015-01-17 20:58:34+00:00\n    2015-01-17 21:01:34+00:00\n    iap\n    offer3\n    10.49\n    4.2\n    2015-01-07\n    other_campaign\n    United States\n  \n  \n    1517\n    OMCVUAIKSDTR651\n    OMCVUAIKSDTR651-yso9e1b2\n    2015-01-17 20:58:34+00:00\n    2015-01-17 21:02:34+00:00\n    iap\n    offer5\n    20.29\n    4.2\n    2015-01-07\n    other_campaign\n    United States\n  \n  \n    1913\n    MTCIWKOVASYP925\n    MTCIWKOVASYP925-1q3xvfmp\n    2015-01-20 12:34:43+00:00\n    2015-01-20 12:35:37+00:00\n    iap\n    offer5\n    26.09\n    3.9\n    2015-01-14\n    organic\n    Germany\n  \n  \n    1914\n    MTCIWKOVASYP925\n    MTCIWKOVASYP925-1q3xvfmp\n    2015-01-20 12:34:43+00:00\n    2015-01-20 12:37:25+00:00\n    iap\n    gold2\n    1.79\n    3.9\n    2015-01-14\n    organic\n    Germany\n  \n  \n    1919\n    BFNLURISJXTH647\n    BFNLURISJXTH647-len6vujd\n    2015-01-20 14:09:51+00:00\n    2015-01-20 14:10:03+00:00\n    iap\n    gold7\n    47.99\n    4.5\n    2015-01-10\n    organic\n    India\n  \n  \n    1920\n    BFNLURISJXTH647\n    BFNLURISJXTH647-len6vujd\n    2015-01-20 14:09:51+00:00\n    2015-01-20 14:14:21+00:00\n    iap\n    gold6\n    23.99\n    4.5\n    2015-01-10\n    organic\n    India\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\"),\n        tbl_name=\"game_revenue\",\n        label=\"Validation with test unit failures available as an extract\"\n    )\n    .col_vals_gt(columns=\"item_revenue\", value=0)      # STEP 1: no test unit failures\n    .col_vals_ge(columns=\"session_duration\", value=5)  # STEP 2: 14 test unit failures -&gt; extract\n    .interrogate()\n)\npb.preview(validation.get_data_extracts(i=2, frame=True), n_head=20, n_tail=20)\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/checks-for-missing/index.html",
    "href": "demos/checks-for-missing/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Checks for Missing Values\nPerform validations that check whether missing/NA/Null values are present.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:19Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    d\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    5\n    \n        \n            \n\n    col_vals_null\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    00.00\n    131.00\n    —\n    —\n    —\n    CSV\n  \n\n  \n  \n  \n    2025-04-23 00:57:19 UTC&lt; 1 s2025-04-23 00:57:19 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_not_null(columns=\"a\")                  # expect no Null values\n    .col_vals_not_null(columns=\"b\")                  # \"\" \"\"\n    .col_vals_not_null(columns=\"c\")                  # \"\" \"\"\n    .col_vals_not_null(columns=\"d\")                  # \"\" \"\"\n    .col_vals_null(columns=\"a\")                      # expect all values to be Null\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/failure-thresholds/index.html",
    "href": "demos/failure-thresholds/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Set Failure Threshold Levels\nSet threshold levels to better gauge adverse data quality.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:26DuckDBWARNING0.05ERROR0.1CRITICAL0.15\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #EBBC14\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0.05\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    17010.85\n    2990.15\n    ●\n    ●\n    ○\n    —\n  \n  \n    #AAAAAA\n    4\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19931.00\n    70.00\n    ●\n    ○\n    ○\n    —\n  \n  \n    #FF3300\n    5\n    \n        \n            \n\n    col_exists\n    \n        \n            \n            \n            \n        \n    \n\n        \n        \n            col_exists()\n        \n        \n        \n    end_day\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    00.00\n    11.00\n    ●\n    ●\n    ●\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:26 UTC&lt; 1 s2025-04-23 00:57:26 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\"),\n        thresholds=pb.Thresholds(  # setting relative threshold defaults for all steps\n            warning=0.05,          # 5% failing test units: warning threshold (gray)\n            error=0.10,            # 10% failed test units: error threshold (yellow)\n            critical=0.15          # 15% failed test units: critical threshold (red)\n        ),\n    )\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .col_vals_gt(columns=\"item_revenue\", value=0.05)\n    .col_vals_gt(\n        columns=\"session_duration\",\n        value=4,\n        thresholds=(5, 10, 20)     # setting absolute thresholds for *this* step (W, E, C)\n    )\n    .col_exists(columns=\"end_day\")\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "demos/set-membership/index.html",
    "href": "demos/set-membership/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Set Membership\nPerform validations that check whether values are part of a set (or not part of one).\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:57:33Polars\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    f\n    low, mid, high\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_not_in_set\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_in_set()\n        \n        \n        \n    f\n    zero, infinity\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:33 UTC&lt; 1 s2025-04-23 00:57:33 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n    )\n    .col_vals_in_set(columns=\"f\", set=[\"low\", \"mid\", \"high\"])    # part of this set\n    .col_vals_not_in_set(columns=\"f\", set=[\"zero\", \"infinity\"])  # not part of this set\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high"
  },
  {
    "objectID": "demos/using-parquet-data/index.html",
    "href": "demos/using-parquet-data/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Using Parquet Data\nA Parquet dataset can be used for data validation, thanks to Ibis.\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    Example using a Parquet dataset.Parquet\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    item_revenue\n    200\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    item_revenue\n    0\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    session_duration\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    19820.99\n    180.01\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    item_type\n    iap, ad\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    5\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    player_id\n    [A-Z]{12}\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    2000\n    20001.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:57:40 UTC&lt; 1 s2025-04-23 00:57:40 UTC\n  \n\n\n\n\n\n\n        \n\n\nimport pointblank as pb\nimport ibis\n\ngame_revenue = ibis.read_parquet(\"data/game_revenue.parquet\")\n\nvalidation = (\n    pb.Validate(data=game_revenue, label=\"Example using a Parquet dataset.\")\n    .col_vals_lt(columns=\"item_revenue\", value=200)\n    .col_vals_gt(columns=\"item_revenue\", value=0)\n    .col_vals_gt(columns=\"session_duration\", value=5)\n    .col_vals_in_set(columns=\"item_type\", set=[\"iap\", \"ad\"])\n    .col_vals_regex(columns=\"player_id\", pattern=r\"[A-Z]{12}\\d{3}\")\n    .interrogate()\n)\n\nvalidation\n\n\nPreview of Input Table\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    ParquetRows2,000Columns11\n  \n\n  \n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Pointblank",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 4, 2025\n\n\nIntroducing Pointblank\n\n\nRich Iannone\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "reference/Schema.html",
    "href": "reference/Schema.html",
    "title": "Schema",
    "section": "",
    "text": "Schema(self, columns=None, tbl=None, **kwargs)\nDefinition of a schema object.\nThe schema object defines the structure of a table. Once it is defined, the object can be used in a validation workflow, using Validate and its methods, to ensure that the structure of a table matches the expected schema. The validation method that works with the schema object is called col_schema_match().\nA schema for a table can be constructed with the Schema class in a number of ways:\nThe schema object can also be constructed by providing a DataFrame or Ibis table object (using the tbl= parameter) and the schema will be collected from either type of object. The schema object can be printed to display the column names and dtypes. Note that if tbl= is provided then there shouldn’t be any other inputs provided through either columns= or **kwargs."
  },
  {
    "objectID": "reference/Schema.html#parameters",
    "href": "reference/Schema.html#parameters",
    "title": "Schema",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | list[tuple[str, str]] | list[tuple[str]] | dict[str, str] | None = None\n\nA list of strings (representing column names), a list of tuples (for column names and column dtypes), or a dictionary containing column and dtype information. If any of these inputs are provided here, it will take precedence over any column arguments provided via **kwargs.\n\ntbl : any | None = None\n\nA DataFrame (Polars or Pandas) or an Ibis table object from which the schema will be collected. Read the Supported Input Table Types section for details on the supported table types.\n\n****kwargs** :  = {}\n\nIndividual column arguments that are in the form of column=dtype or column=[dtype1, dtype2, ...]. These will be ignored if the columns= parameter is not None."
  },
  {
    "objectID": "reference/Schema.html#returns",
    "href": "reference/Schema.html#returns",
    "title": "Schema",
    "section": "Returns",
    "text": "Returns\n\n : Schema\n\nA schema object."
  },
  {
    "objectID": "reference/Schema.html#supported-input-table-types",
    "href": "reference/Schema.html#supported-input-table-types",
    "title": "Schema",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe tbl= parameter, if used, can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using Schema(tbl=) with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/Schema.html#additional-notes-on-schema-construction",
    "href": "reference/Schema.html#additional-notes-on-schema-construction",
    "title": "Schema",
    "section": "Additional Notes on Schema Construction",
    "text": "Additional Notes on Schema Construction\nWhile there is flexibility in how a schema can be constructed, there is the potential for some confusion. So let’s go through each of the methods of constructing a schema in more detail and single out some important points.\nWhen providing a list of column names to columns=, a col_schema_match() validation step will only check the column names. Any arguments pertaining to dtypes will be ignored.\nWhen using a list of tuples in columns=, the tuples could contain the column name and dtype or just the column name. This construction allows for more flexibility in constructing the schema as some columns will be checked for dtypes and others will not. This method is the only way to have mixed checks of column names and dtypes in col_schema_match().\nWhen providing a dictionary to columns=, the keys are the column names and the values are the dtypes. This method of input is useful in those cases where you might already have a dictionary of column names and dtypes that you want to use as the schema.\nIf using individual column arguments in the form of keyword arguments, the column names are the keyword arguments and the dtypes are the values. This method emphasizes readability and is perhaps more convenient when manually constructing a schema with a small number of columns.\nFinally, multiple dtypes can be provided for a single column by providing a list or tuple of dtypes in place of a scalar string value. Having multiple dtypes for a column allows for the dtype check via col_schema_match() to make multiple attempts at matching the column dtype. Should any of the dtypes match the column dtype, that part of the schema check will pass. Here are some examples of how you could provide single and multiple dtypes for a column:\n# list of tuples\nschema_1 = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", [\"Float64\", \"Int64\"])])\n\n# dictionary\nschema_2 = pb.Schema(columns={\"name\": \"String\", \"age\": [\"Float64\", \"Int64\"]})\n\n# keyword arguments\nschema_3 = pb.Schema(name=\"String\", age=[\"Float64\", \"Int64\"])\nAll of the above examples will construct the same schema object."
  },
  {
    "objectID": "reference/Schema.html#examples",
    "href": "reference/Schema.html#examples",
    "title": "Schema",
    "section": "Examples",
    "text": "Examples\nA schema can be constructed via the Schema class in multiple ways. Let’s use the following Polars DataFrame as a basis for constructing a schema:\n\nimport pointblank as pb\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"height\": [5.6, 6.0, 5.8]\n})\n\nYou could provide Schema(columns=) a list of tuples containing column names and data types:\n\nschema = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", \"Int64\"), (\"height\", \"Float64\")])\n\nAlternatively, a dictionary containing column names and dtypes also works:\n\nschema = pb.Schema(columns={\"name\": \"String\", \"age\": \"Int64\", \"height\": \"Float64\"})\n\nAnother input method involves using individual column arguments in the form of keyword arguments:\n\nschema = pb.Schema(name=\"String\", age=\"Int64\", height=\"Float64\")\n\nFinally, could also provide a DataFrame (Polars and Pandas) or an Ibis table object to tbl= and the schema will be collected:\nschema = pb.Schema(tbl=df)\nWhichever method you choose, you can verify the schema inputs by printing the schema object:\n\nprint(schema)\n\nPointblank Schema\n  name: String\n  age: Int64\n  height: Float64\n\n\nThe Schema object can be used to validate the structure of a table against the schema. The relevant Validate method for this is col_schema_match(). In a validation workflow, you’ll have a target table (defined at the beginning of the workflow) and you might want to ensure that your expectations of the table structure are met. The col_schema_match() method works with a Schema object to validate the structure of the table. Here’s an example of how you could use col_schema_match() in a validation workflow:\n\n# Define the schema\nschema = pb.Schema(name=\"String\", age=\"Int64\", height=\"Float64\")\n\n# Define a validation that checks the schema against the table (`df`)\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\n# Display the validation results\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe col_schema_match() validation method will validate the structure of the table against the schema during interrogation. If the structure of the table does not match the schema, the single test unit will fail. In this case, the defined schema matched the structure of the table, so the validation passed.\nWe can also choose to check only the column names of the target table. This can be done by providing a simplified Schema object, which is given a list of column names:\n\nschema = pb.Schema(columns=[\"name\", \"age\", \"height\"])\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nIn this case, the schema only checks the column names of the table against the schema during interrogation. If the column names of the table do not match the schema, the single test unit will fail. In this case, the defined schema matched the column names of the table, so the validation passed.\nIf you wanted to check column names and dtypes only for a subset of columns (and just the column names for the rest), you could use a list of mixed one- or two-item tuples in columns=:\n\nschema = pb.Schema(columns=[(\"name\", \"String\"), (\"age\", ), (\"height\", )])\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nNot specifying a dtype for a column (as is the case for the age and height columns in the above example) will only check the column name.\nThere may also be the case where you want to check the column names and specify multiple dtypes for a column to have several attempts at matching the dtype. This can be done by providing a list of dtypes where there would normally be a single dtype:\n\nschema = pb.Schema(\n  columns=[(\"name\", \"String\"), (\"age\", [\"Float64\", \"Int64\"]), (\"height\", \"Float64\")]\n)\n\nvalidation = (\n    pb.Validate(data=df)\n    .col_schema_match(schema=schema)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_schema_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_schema_match()\n        \n        \n        \n    —\n    SCHEMA\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFor the age column, the schema will check for both Float64 and Int64 dtypes. If either of these dtypes is found in the column, the portion of the schema check will succeed."
  },
  {
    "objectID": "reference/Schema.html#see-also",
    "href": "reference/Schema.html#see-also",
    "title": "Schema",
    "section": "See Also",
    "text": "See Also\nThe col_schema_match() validation method, where a Schema object is used in a validation workflow."
  },
  {
    "objectID": "reference/DataScan.html",
    "href": "reference/DataScan.html",
    "title": "DataScan",
    "section": "",
    "text": "DataScan(self, data, tbl_name=None)\nGet a summary of a dataset.\nThe DataScan class provides a way to get a summary of a dataset. The summary includes the following information:\n\nthe name of the table (if provided)\nthe type of the table (e.g., \"polars\", \"pandas\", etc.)\nthe number of rows and columns in the table\ncolumn-level information, including:\n\nthe column name\nthe column type\nmeasures of missingness and distinctness\nmeasures of negative, zero, and positive values (for numerical columns)\na sample of the data (the first 5 values)\nstatistics (if the column contains numbers, strings, or datetimes)\n\n\nTo obtain a dictionary representation of the summary, you can use the to_dict() method. To get a JSON representation of the summary, you can use the to_json() method. To save the JSON text to a file, the save_to_json() method could be used.\n\n\n\n\n\n\nWarning\n\n\n\nThe DataScan() class is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\ndata : FrameT | Any\n\nThe data to scan and summarize.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name.\n\n\n\n\n\nFor each column, the following measures are provided:\n\nn_missing_values: the number of missing values in the column\nf_missing_values: the fraction of missing values in the column\nn_unique_values: the number of unique values in the column\nf_unique_values: the fraction of unique values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset.\n\n\n\nFor numerical columns, the following measures are provided:\n\nn_negative_values: the number of negative values in the column\nf_negative_values: the fraction of negative values in the column\nn_zero_values: the number of zero values in the column\nf_zero_values: the fraction of zero values in the column\nn_positive_values: the number of positive values in the column\nf_positive_values: the fraction of positive values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset.\n\n\n\nFor numerical and string columns, several statistical measures are provided. Please note that for string columms, the statistics are based on the lengths of the strings in the column.\nThe following descriptive statistics are provided:\n\nmean: the mean of the column\nstd_dev: the standard deviation of the column\n\nAdditionally, the following quantiles are provided:\n\nmin: the minimum value in the column\np05: the 5th percentile of the column\nq_1: the first quartile of the column\nmed: the median of the column\nq_3: the third quartile of the column\np95: the 95th percentile of the column\nmax: the maximum value in the column\niqr: the interquartile range of the column\n\n\n\n\nFor date/datetime columns, the following statistics are provided:\n\nmin: the minimum date/datetime in the column\nmax: the maximum date/datetime in the column\n\n\n\n\n\n : DataScan\n\nA DataScan object."
  },
  {
    "objectID": "reference/DataScan.html#parameters",
    "href": "reference/DataScan.html#parameters",
    "title": "DataScan",
    "section": "",
    "text": "data : FrameT | Any\n\nThe data to scan and summarize.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name."
  },
  {
    "objectID": "reference/DataScan.html#measures-of-missingness-and-distinctness",
    "href": "reference/DataScan.html#measures-of-missingness-and-distinctness",
    "title": "DataScan",
    "section": "",
    "text": "For each column, the following measures are provided:\n\nn_missing_values: the number of missing values in the column\nf_missing_values: the fraction of missing values in the column\nn_unique_values: the number of unique values in the column\nf_unique_values: the fraction of unique values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset."
  },
  {
    "objectID": "reference/DataScan.html#counts-and-fractions-of-negative-zero-and-positive-values",
    "href": "reference/DataScan.html#counts-and-fractions-of-negative-zero-and-positive-values",
    "title": "DataScan",
    "section": "",
    "text": "For numerical columns, the following measures are provided:\n\nn_negative_values: the number of negative values in the column\nf_negative_values: the fraction of negative values in the column\nn_zero_values: the number of zero values in the column\nf_zero_values: the fraction of zero values in the column\nn_positive_values: the number of positive values in the column\nf_positive_values: the fraction of positive values in the column\n\nThe fractions are calculated as the ratio of the measure to the total number of rows in the dataset."
  },
  {
    "objectID": "reference/DataScan.html#statistics-for-numerical-and-string-columns",
    "href": "reference/DataScan.html#statistics-for-numerical-and-string-columns",
    "title": "DataScan",
    "section": "",
    "text": "For numerical and string columns, several statistical measures are provided. Please note that for string columms, the statistics are based on the lengths of the strings in the column.\nThe following descriptive statistics are provided:\n\nmean: the mean of the column\nstd_dev: the standard deviation of the column\n\nAdditionally, the following quantiles are provided:\n\nmin: the minimum value in the column\np05: the 5th percentile of the column\nq_1: the first quartile of the column\nmed: the median of the column\nq_3: the third quartile of the column\np95: the 95th percentile of the column\nmax: the maximum value in the column\niqr: the interquartile range of the column"
  },
  {
    "objectID": "reference/DataScan.html#statistics-for-date-and-datetime-columns",
    "href": "reference/DataScan.html#statistics-for-date-and-datetime-columns",
    "title": "DataScan",
    "section": "",
    "text": "For date/datetime columns, the following statistics are provided:\n\nmin: the minimum date/datetime in the column\nmax: the maximum date/datetime in the column"
  },
  {
    "objectID": "reference/DataScan.html#returns",
    "href": "reference/DataScan.html#returns",
    "title": "DataScan",
    "section": "",
    "text": ": DataScan\n\nA DataScan object."
  },
  {
    "objectID": "reference/last_n.html",
    "href": "reference/last_n.html",
    "title": "last_n",
    "section": "",
    "text": "last_n(n, offset=0)\nSelect the last n columns in the column list.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The last_n() selector function can be used to select n columns positioned at the end of the column list. So if the set of table columns consists of\n[age, rev_01, rev_02, profit_01, profit_02]\nand you want to validate the last two columns, you can use columns=last_n(2). This will select the profit_01 and profit_02 columns and a validation step will be created for each.\nThe offset= parameter can be used to skip a certain number of columns from the end of the column list. So if you want to select the third and fourth columns from the end, you can use columns=last_n(2, offset=2)."
  },
  {
    "objectID": "reference/last_n.html#parameters",
    "href": "reference/last_n.html#parameters",
    "title": "last_n",
    "section": "Parameters",
    "text": "Parameters\n\nn : int\n\nThe number of columns to select from the end of the column list. Should be a positive integer value. If n is greater than the number of columns in the table, all columns will be selected.\n\noffset : int = 0\n\nThe offset from the end of the column list. The default is 0. If offset is greater than the number of columns in the table, no columns will be selected."
  },
  {
    "objectID": "reference/last_n.html#returns",
    "href": "reference/last_n.html#returns",
    "title": "last_n",
    "section": "Returns",
    "text": "Returns\n\n : LastN\n\nA LastN object, which can be used to select the last n columns."
  },
  {
    "objectID": "reference/last_n.html#relevant-validation-methods-where-last_n-can-be-used",
    "href": "reference/last_n.html#relevant-validation-methods-where-last_n-can-be-used",
    "title": "last_n",
    "section": "Relevant Validation Methods where last_n() can be Used",
    "text": "Relevant Validation Methods where last_n() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe last_n() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/last_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/last_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "last_n",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe last_n() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names starting with “rev” along with the last two columns, you can use the last_n() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(last_n(2) | starts_with(\"rev\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/last_n.html#examples",
    "href": "reference/last_n.html#examples",
    "title": "last_n",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, paid_2021, paid_2022, paid_2023, and paid_2024 and we’d like to validate that the values in the last four columns are greater than 10. We can use the last_n() column selector function to specify that the last four columns in the table are the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.last_n(4), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps. The values in all those columns were all greater than 10.\nWe can also use the last_n() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select the last four columns but also omit those columns that end with \"2023\", we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.last_n(4) - pb.ends_with(\"2023\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get three validation steps, one for paid_2021, paid_2022, and paid_2024."
  },
  {
    "objectID": "reference/load_dataset.html",
    "href": "reference/load_dataset.html",
    "title": "load_dataset",
    "section": "",
    "text": "load_dataset(dataset='small_table', tbl_type='polars')\nLoad a dataset hosted in the library as specified table type.\nThe Pointblank library includes several datasets that can be loaded using the load_dataset() function. The datasets can be loaded as a Polars DataFrame, a Pandas DataFrame, or as a DuckDB table (which uses the Ibis library backend). These datasets are used throughout the documentation’s examples to demonstrate the functionality of the library. They’re also useful for experimenting with the library and trying out different validation scenarios."
  },
  {
    "objectID": "reference/load_dataset.html#parameters",
    "href": "reference/load_dataset.html#parameters",
    "title": "load_dataset",
    "section": "Parameters",
    "text": "Parameters\n\ndataset : Literal['small_table', 'game_revenue', 'nycflights'] = 'small_table'\n\nThe name of the dataset to load. Current options are \"small_table\", \"game_revenue\", and \"nycflights\".\n\ntbl_type : Literal['polars', 'pandas', 'duckdb'] = 'polars'\n\nThe type of table to generate from the dataset. The named options are \"polars\", \"pandas\", and \"duckdb\"."
  },
  {
    "objectID": "reference/load_dataset.html#returns",
    "href": "reference/load_dataset.html#returns",
    "title": "load_dataset",
    "section": "Returns",
    "text": "Returns\n\n : FrameT | Any\n\nThe dataset for the Validate object. This could be a Polars DataFrame, a Pandas DataFrame, or a DuckDB table as an Ibis table."
  },
  {
    "objectID": "reference/load_dataset.html#included-datasets",
    "href": "reference/load_dataset.html#included-datasets",
    "title": "load_dataset",
    "section": "Included Datasets",
    "text": "Included Datasets\nThere are three included datasets that can be loaded using the load_dataset() function:\n\n\"small_table\": A small dataset with 13 rows and 8 columns. This dataset is useful for testing and demonstration purposes.\n\"game_revenue\": A dataset with 2000 rows and 11 columns. Provides revenue data for a game development company. For the particular game, there are records of player sessions, the items they purchased, ads viewed, and the revenue generated.\n\"nycflights\": A dataset with 336,776 rows and 18 columns. This dataset provides information about flights departing from New York City airports (JFK, LGA, or EWR) in 2013."
  },
  {
    "objectID": "reference/load_dataset.html#supported-dataframe-types",
    "href": "reference/load_dataset.html#supported-dataframe-types",
    "title": "load_dataset",
    "section": "Supported DataFrame Types",
    "text": "Supported DataFrame Types\nThe tbl_type= parameter can be set to one of the following:\n\n\"polars\": A Polars DataFrame.\n\"pandas\": A Pandas DataFrame.\n\"duckdb\": An Ibis table for a DuckDB database."
  },
  {
    "objectID": "reference/load_dataset.html#examples",
    "href": "reference/load_dataset.html#examples",
    "title": "load_dataset",
    "section": "Examples",
    "text": "Examples\nLoad the \"small_table\" dataset as a Polars DataFrame by calling load_dataset() with its defaults:\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset()\n\npb.preview(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nNote that the \"small_table\" dataset is a simple Polars DataFrame and using the preview() function will display the table in an HTML viewing environment.\nThe \"game_revenue\" dataset can be loaded as a Pandas DataFrame by specifying the dataset name and setting tbl_type=\"pandas\":\n\ngame_revenue = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"pandas\")\n\npb.preview(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows2,000Columns11\n  \n\n  \n  player_idobject\n  session_idobject\n  session_startdatetime64[ns, UTC]\n  timedatetime64[ns, UTC]\n  item_typeobject\n  item_nameobject\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydatetime64[ns]\n  acquisitionobject\n  countryobject\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01 00:00:00\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11 00:00:00\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11 00:00:00\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10 00:00:00\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10 00:00:00\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14 00:00:00\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nThe \"game_revenue\" dataset is a more real-world dataset with a mix of data types, and it’s significantly larger than the small_table dataset at 2000 rows and 11 columns.\nThe \"nycflights\" dataset can be loaded as a DuckDB table by specifying the dataset name and setting tbl_type=\"duckdb\":\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\npb.preview(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows336,776Columns18\n  \n\n  \n  yearint64\n  monthint64\n  dayint64\n  dep_timeint64\n  sched_dep_timeint64\n  dep_delayint64\n  arr_timeint64\n  sched_arr_timeint64\n  arr_delayint64\n  carrierstring\n  flightint64\n  tailnumstring\n  originstring\n  deststring\n  air_timeint64\n  distanceint64\n  hourint64\n  minuteint64\n\n\n\n  \n    1\n    2013\n    1\n    1\n    517\n    515\n    2\n    830\n    819\n    11\n    UA\n    1545\n    N14228\n    EWR\n    IAH\n    227\n    1400\n    5\n    15\n  \n  \n    2\n    2013\n    1\n    1\n    533\n    529\n    4\n    850\n    830\n    20\n    UA\n    1714\n    N24211\n    LGA\n    IAH\n    227\n    1416\n    5\n    29\n  \n  \n    3\n    2013\n    1\n    1\n    542\n    540\n    2\n    923\n    850\n    33\n    AA\n    1141\n    N619AA\n    JFK\n    MIA\n    160\n    1089\n    5\n    40\n  \n  \n    4\n    2013\n    1\n    1\n    544\n    545\n    -1\n    1004\n    1022\n    -18\n    B6\n    725\n    N804JB\n    JFK\n    BQN\n    183\n    1576\n    5\n    45\n  \n  \n    5\n    2013\n    1\n    1\n    554\n    600\n    -6\n    812\n    837\n    -25\n    DL\n    461\n    N668DN\n    LGA\n    ATL\n    116\n    762\n    6\n    0\n  \n  \n    336772\n    2013\n    9\n    30\n    NULL\n    1455\n    NULL\n    NULL\n    1634\n    NULL\n    9E\n    3393\n    NULL\n    JFK\n    DCA\n    NULL\n    213\n    14\n    55\n  \n  \n    336773\n    2013\n    9\n    30\n    NULL\n    2200\n    NULL\n    NULL\n    2312\n    NULL\n    9E\n    3525\n    NULL\n    LGA\n    SYR\n    NULL\n    198\n    22\n    0\n  \n  \n    336774\n    2013\n    9\n    30\n    NULL\n    1210\n    NULL\n    NULL\n    1330\n    NULL\n    MQ\n    3461\n    N535MQ\n    LGA\n    BNA\n    NULL\n    764\n    12\n    10\n  \n  \n    336775\n    2013\n    9\n    30\n    NULL\n    1159\n    NULL\n    NULL\n    1344\n    NULL\n    MQ\n    3572\n    N511MQ\n    LGA\n    CLE\n    NULL\n    419\n    11\n    59\n  \n  \n    336776\n    2013\n    9\n    30\n    NULL\n    840\n    NULL\n    NULL\n    1020\n    NULL\n    MQ\n    3531\n    N839MQ\n    LGA\n    RDU\n    NULL\n    431\n    8\n    40\n  \n\n\n\n\n\n\n        \n\n\nThe \"nycflights\" dataset is a large dataset with 336,776 rows and 18 columns. This dataset is truly a real-world dataset and provides information about flights originating from New York City airports in 2013."
  },
  {
    "objectID": "reference/Validate.col_count_match.html",
    "href": "reference/Validate.col_count_match.html",
    "title": "Validate.col_count_match",
    "section": "",
    "text": "Validate.col_count_match(\n    count,\n    inverse=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether the column count of the table matches a specified count.\nThe col_count_match() method checks whether the column count of the target table matches a specified count. This validation will operate over a single test unit, which is whether the column count matches the specified count.\nWe also have the option to invert the validation step by setting inverse=True. This will make the expectation that column row count of the target table does not match the specified count."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#parameters",
    "href": "reference/Validate.col_count_match.html#parameters",
    "title": "Validate.col_count_match",
    "section": "Parameters",
    "text": "Parameters\n\ncount : int | FrameT | Any\n\nThe expected column count of the table. This can be an integer value, a Polars or Pandas DataFrame object, or an Ibis backend table. If a DataFrame/table is provided, the column count of that object will be used as the expected count.\n\ninverse : bool = False\n\nShould the validation step be inverted? If True, then the expectation is that the column count of the target table should not match the specified count= value.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#returns",
    "href": "reference/Validate.col_count_match.html#returns",
    "title": "Validate.col_count_match",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#preprocessing",
    "href": "reference/Validate.col_count_match.html#preprocessing",
    "title": "Validate.col_count_match",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#thresholds",
    "href": "reference/Validate.col_count_match.html#thresholds",
    "title": "Validate.col_count_match",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_count_match.html#examples",
    "href": "reference/Validate.col_count_match.html#examples",
    "title": "Validate.col_count_match",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use the built in dataset \"game_revenue\". The table can be obtained by calling load_dataset(\"game_revenue\").\n\nimport pointblank as pb\n\ngame_revenue = pb.load_dataset(\"game_revenue\")\n\npb.preview(game_revenue)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows2,000Columns11\n  \n\n  \n  player_idString\n  session_idString\n  session_startDatetime\n  timeDatetime\n  item_typeString\n  item_nameString\n  item_revenueFloat64\n  session_durationFloat64\n  start_dayDate\n  acquisitionString\n  countryString\n\n\n\n  \n    1\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    2\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    3\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    4\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    5\n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    1996\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1997\n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    1998\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    1999\n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    2000\n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the number of columns in the table matches a fixed value. In this case, we will use the value 11 as the expected column count.\n\nvalidation = (\n    pb.Validate(data=game_revenue)\n    .col_count_match(count=11)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_count_match\n    \n        \n            \n            \n            \n                \n                \n            \n            \n        \n    \n\n        \n        \n            col_count_match()\n        \n        \n        \n    —\n    11\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    1\n    11.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that the expectation value of 11 matches the actual count of columns in the target table. So, the single test unit passed."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html",
    "href": "reference/Validate.col_vals_eq.html",
    "title": "Validate.col_vals_eq",
    "section": "",
    "text": "Validate.col_vals_eq(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data equal to a fixed value or data in another column?\nThe col_vals_eq() validation method checks whether column values in a table are equal to a specified value= (the exact comparison used in this function is col_val == value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#parameters",
    "href": "reference/Validate.col_vals_eq.html#parameters",
    "title": "Validate.col_vals_eq",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#returns",
    "href": "reference/Validate.col_vals_eq.html#returns",
    "title": "Validate.col_vals_eq",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_eq.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_eq",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#preprocessing",
    "href": "reference/Validate.col_vals_eq.html#preprocessing",
    "title": "Validate.col_vals_eq",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#thresholds",
    "href": "reference/Validate.col_vals_eq.html#thresholds",
    "title": "Validate.col_vals_eq",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_eq.html#examples",
    "href": "reference/Validate.col_vals_eq.html#examples",
    "title": "Validate.col_vals_eq",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 5, 5, 5, 5, 5],\n        \"b\": [5, 5, 5, 6, 5, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    5\n    5\n  \n  \n    2\n    5\n    5\n  \n  \n    3\n    5\n    5\n  \n  \n    4\n    5\n    6\n  \n  \n    5\n    5\n    5\n  \n  \n    6\n    5\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all equal to the value of 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_eq(columns=\"a\", value=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_equal\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_eq()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_eq(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_eq() to check whether the values in column a are equal to the values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_eq(columns=\"a\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_equal\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_eq()\n        \n        \n        \n    a\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 3: a is 5 and b is 6.\nRow 5: a is 5 and b is 4."
  },
  {
    "objectID": "reference/Validate.get_json_report.html",
    "href": "reference/Validate.get_json_report.html",
    "title": "Validate.get_json_report",
    "section": "",
    "text": "Validate.get_json_report(use_fields=None, exclude_fields=None)\nGet a report of the validation results as a JSON-formatted string.\n\n\n\nuse_fields : list[str] | None = None\n\nA list of fields to include in the report. If None, all fields are included.\n\nexclude_fields : list[str] | None = None\n\nA list of fields to exclude from the report. If None, no fields are excluded.\n\n\n\n\n\n\n : str\n\nA JSON-formatted string representing the validation report."
  },
  {
    "objectID": "reference/Validate.get_json_report.html#parameters",
    "href": "reference/Validate.get_json_report.html#parameters",
    "title": "Validate.get_json_report",
    "section": "",
    "text": "use_fields : list[str] | None = None\n\nA list of fields to include in the report. If None, all fields are included.\n\nexclude_fields : list[str] | None = None\n\nA list of fields to exclude from the report. If None, no fields are excluded."
  },
  {
    "objectID": "reference/Validate.get_json_report.html#returns",
    "href": "reference/Validate.get_json_report.html#returns",
    "title": "Validate.get_json_report",
    "section": "",
    "text": ": str\n\nA JSON-formatted string representing the validation report."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html",
    "href": "reference/Validate.col_vals_gt.html",
    "title": "Validate.col_vals_gt",
    "section": "",
    "text": "Validate.col_vals_gt(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data greater than a fixed value or data in another column?\nThe col_vals_gt() validation method checks whether column values in a table are greater than a specified value= (the exact comparison used in this function is col_val &gt; value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#parameters",
    "href": "reference/Validate.col_vals_gt.html#parameters",
    "title": "Validate.col_vals_gt",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#returns",
    "href": "reference/Validate.col_vals_gt.html#returns",
    "title": "Validate.col_vals_gt",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_gt.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_gt",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#preprocessing",
    "href": "reference/Validate.col_vals_gt.html#preprocessing",
    "title": "Validate.col_vals_gt",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#thresholds",
    "href": "reference/Validate.col_vals_gt.html#thresholds",
    "title": "Validate.col_vals_gt",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_gt.html#examples",
    "href": "reference/Validate.col_vals_gt.html#examples",
    "title": "Validate.col_vals_gt",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 6, 5],\n        \"b\": [1, 2, 1, 2, 2, 2],\n        \"c\": [2, 1, 2, 2, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    2\n    1\n  \n  \n    3\n    5\n    1\n    2\n  \n  \n    4\n    7\n    2\n    2\n  \n  \n    5\n    6\n    2\n    3\n  \n  \n    6\n    5\n    2\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all greater than the value of 4. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=4)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    4\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_gt(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_gt() to check whether the values in column c are greater than values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"c\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    c\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 1: c is 1 and b is 2.\nRow 3: c is 2 and b is 2."
  },
  {
    "objectID": "reference/Validate.warning.html",
    "href": "reference/Validate.warning.html",
    "title": "Validate.warning",
    "section": "",
    "text": "Validate.warning(i=None, scalar=False)\nGet the ‘warning’ level status for each validation step.\nThe ‘warning’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the ‘warning’ level. Otherwise, the status is False.\nThe ascribed name of ‘warning’ is semantic and does not imply that a warning message is generated, it is simply a status indicator that could be used to trigger some action to be taken. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the ‘warning’ status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.warning.html#parameters",
    "href": "reference/Validate.warning.html#parameters",
    "title": "Validate.warning",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the ‘warning’ status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.warning.html#returns",
    "href": "reference/Validate.warning.html#returns",
    "title": "Validate.warning",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the ‘warning’ status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.warning.html#examples",
    "href": "reference/Validate.warning.html#examples",
    "title": "Validate.warning",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have some failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the warning() method is used to determine the ‘warning’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12, 3, 10],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.warning()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘warning’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘warning’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘warning’ level.\nWe can also visually inspect the ‘warning’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:58:36PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    50.71\n    20.29\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:58:36 UTC&lt; 1 s2025-04-23 00:58:36 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there’s a filled gray circle in the first step (look to the far right side, in the W column) indicating that the ‘warning’ threshold was met. The other steps have empty gray circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘warning’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.warning(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had met the ‘warning’ threshold."
  },
  {
    "objectID": "reference/Validate.get_step_report.html",
    "href": "reference/Validate.get_step_report.html",
    "title": "Validate.get_step_report",
    "section": "",
    "text": "Validate.get_step_report(i, columns_subset=None, header=':default:', limit=10)\nGet a detailed report for a single validation step.\nThe get_step_report() method returns a report of what went well—or what failed spectacularly—for a given validation step. The report includes a summary of the validation step and a detailed breakdown of the interrogation results. The report is presented as a GT table object, which can be displayed in a notebook or exported to an HTML file."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#parameters",
    "href": "reference/Validate.get_step_report.html#parameters",
    "title": "Validate.get_step_report",
    "section": "Parameters",
    "text": "Parameters\n\ni : int\n\nThe step number for which to get the report.\n\ncolumns_subset : str | list[str] | Column | None = None\n\nThe columns to display in a step report that shows errors in the input table. By default all columns are shown (None). If a subset of columns is desired, we can provide a list of column names, a string with a single column name, a Column object, or a ColumnSelector object. The last two options allow for more flexible column selection using column selector functions. Errors are raised if the column names provided don’t match any columns in the table (when provided as a string or list of strings) or if column selector expressions don’t resolve to any columns.\n\nheader : str = ':default:'\n\nOptions for customizing the header of the step report. The default is the \":default:\" value which produces a header with a standard title and set of details underneath. Aside from this default, free text can be provided for the header. This will be interpreted as Markdown text and transformed internally to HTML. You can provide one of two templating elements: {title} and {details}. The default header has the template \"{title}{details}\" so you can easily start from that and modify as you see fit. If you don’t want a header at all, you can set header=None to remove it entirely.\n\nlimit : int | None = 10\n\nThe number of rows to display for those validation steps that check values in rows (the col_vals_*() validation steps). The default is 10 rows and the limit can be removed entirely by setting limit=None."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#returns",
    "href": "reference/Validate.get_step_report.html#returns",
    "title": "Validate.get_step_report",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT table object that represents the detailed report for the validation step."
  },
  {
    "objectID": "reference/Validate.get_step_report.html#examples",
    "href": "reference/Validate.get_step_report.html#examples",
    "title": "Validate.get_step_report",
    "section": "Examples",
    "text": "Examples\nLet’s create a validation plan with a few validation steps and interrogate the data. With that, we’ll have a look at the validation reporting table for the entire collection of steps and what went well or what failed.\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(\n        data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\"),\n        tbl_name=\"small_table\",\n        label=\"Example for the get_step_report() method\",\n        thresholds=(1, 0.20, 0.40)\n    )\n    .col_vals_lt(columns=\"d\", value=3500)\n    .col_vals_between(columns=\"c\", left=1, right=8)\n    .col_vals_gt(columns=\"a\", value=3)\n    .col_vals_regex(columns=\"b\", pattern=r\"\\d-[a-z]{3}-\\d{3}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #AAAAAA\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    3500\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    ●\n    ○\n    ○\n    CSV\n  \n  \n    #EBBC14\n    2\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    c\n    [1, 8]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    ●\n    ●\n    ○\n    CSV\n  \n  \n    #FF3300\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    3\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    60.46\n    70.54\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    b\n    \\d-[a-z]{3}-\\d{3}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n\n\n\n\n\n        \n\n\nThere were four validation steps performed, where the first three steps had failing test units and the last step had no failures. Let’s get a detailed report for the first step by using the get_step_report() method.\n\nvalidation.get_step_report(i=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION d &lt; 35002 / 13 TEST UNIT FAILURES IN COLUMN 6 EXTRACT OF ALL 2 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    \n    3892.4\n    False\n    mid\n  \n\n\n\n\n\n\n        \n\n\nThe report for the first step is displayed. The report includes a summary of the validation step and a detailed breakdown of the interrogation results. The report provides details on what the validation step was checking, the extent to which the test units failed, and a table that shows the failing rows of the data with the column of interest highlighted.\nThe second and third steps also had failing test units. Reports for those steps can be viewed by using get_step_report(i=2) and get_step_report(i=3) respectively.\nThe final step did not have any failing test units. A report for the final step can still be viewed by using get_step_report(i=4). The report will indicate that every test unit passed and a prview of the target table will be provided.\n\nvalidation.get_step_report(i=4)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 4 ✓ASSERTION b matches regex \\d-[a-z]{3}-\\d{3}13 TEST UNITS ALL PASSED IN COLUMN 4PREVIEW OF TARGET TABLE:\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIf you’d like to trim down the number of columns shown in the report, you can provide a subset of columns to display. For example, if you only want to see the columns a, b, and c, you can provide those column names as a list.\n\nvalidation.get_step_report(i=1, columns_subset=[\"a\", \"b\", \"c\"])\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 1ASSERTION d &lt; 35002 / 13 TEST UNIT FAILURES IN COLUMN 6 (NOT SHOWN)EXTRACT OF ALL 2 ROWS :\n  \n\n  \n  aint64\n  bobject\n  cfloat64\n\n\n\n  \n    2\n    3\n    5-egh-163\n    8.0\n  \n  \n    4\n    2\n    5-jdo-903\n    \n  \n\n\n\n\n\n\n        \n\n\nIf you’d like to increase or reduce the maximum number of rows shown in the report, you can provide a different value for the limit parameter. For example, if you’d like to see only up to 5 rows, you can set limit=5.\n\nvalidation.get_step_report(i=3, limit=5)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Report for Validation Step 3ASSERTION a &gt; 37 / 13 TEST UNIT FAILURES IN COLUMN 3 EXTRACT OF FIRST 5 ROWS (WITH TEST UNIT FAILURES IN RED):\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    \n    3892.4\n    False\n    mid\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n\n\n\n\n\n\n        \n\n\nStep 3 actually had 7 failing test units, but only the first 5 rows are shown in the step report because of the limit=5 parameter."
  },
  {
    "objectID": "reference/Validate.assert_passing.html",
    "href": "reference/Validate.assert_passing.html",
    "title": "Validate.assert_passing",
    "section": "",
    "text": "Validate.assert_passing()\nRaise an AssertionError if all tests are not passing.\nThe assert_passing() method will raise an AssertionError if a test does not pass. This method simply wraps all_passed for more ready use in test suites. The step number and assertion made is printed in the AssertionError message if a failure occurs, ensuring some details are preserved."
  },
  {
    "objectID": "reference/Validate.assert_passing.html#raises",
    "href": "reference/Validate.assert_passing.html#raises",
    "title": "Validate.assert_passing",
    "section": "Raises",
    "text": "Raises\n\n: AssertionError\n\nIf any validation step has failing test units."
  },
  {
    "objectID": "reference/Validate.assert_passing.html#examples",
    "href": "reference/Validate.assert_passing.html#examples",
    "title": "Validate.assert_passing",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the second step will have a failing test unit (the value 10 isn’t less than 9). After interrogation, the assert_passing() method is used to assert that all validation steps passed perfectly.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n    \"a\": [1, 2, 9, 5],\n    \"b\": [5, 6, 10, 3],\n    \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_vals_lt(columns=\"b\", value=9) # this assertion is false\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.assert_passing()\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n/tmp/ipykernel_3978/335715443.py in ?()\n     16     .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n     17     .interrogate()\n     18 )\n     19 \n---&gt; 20 validation.assert_passing()\n\n/opt/hostedtoolcache/Python/3.10.17/x64/lib/python3.10/site-packages/pointblank/validate.py in ?(self)\n   7463             ]\n   7464             msg = \"The following assertions failed:\\n\" + \"\\n\".join(\n   7465                 [f\"- Step {i + 1}: {autobrief}\" for i, autobrief in failed_steps]\n   7466             )\n-&gt; 7467             raise AssertionError(msg)\n\nAssertionError: The following assertions failed:\n- Step 2: Expect that values in `b` should be &lt; `9`."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html",
    "href": "reference/Validate.col_vals_outside.html",
    "title": "Validate.col_vals_outside",
    "section": "",
    "text": "Validate.col_vals_outside(\n    columns,\n    left,\n    right,\n    inclusive=(True, True),\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nDo column data lie outside of two specified values or data in other columns?\nThe col_vals_between() validation method checks whether column values in a table do not fall within a certain range. The range is specified with three arguments: left=, right=, and inclusive=. The left= and right= values specify the lower and upper bounds. These bounds can be specified as literal values or as column names provided within col(). The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#parameters",
    "href": "reference/Validate.col_vals_outside.html#parameters",
    "title": "Validate.col_vals_outside",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nleft : float | int | Column\n\nThe lower bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\nright : float | int | Column\n\nThe upper bound of the range. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison for this bound. See the What Can Be Used in left= and right=? section for details on this.\n\ninclusive : tuple[bool, bool] = (True, True)\n\nA tuple of two boolean values indicating whether the comparison should be inclusive. The position of the boolean values correspond to the left= and right= values, respectively. By default, both values are True.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#returns",
    "href": "reference/Validate.col_vals_outside.html#returns",
    "title": "Validate.col_vals_outside",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#what-can-be-used-in-left-and-right",
    "href": "reference/Validate.col_vals_outside.html#what-can-be-used-in-left-and-right",
    "title": "Validate.col_vals_outside",
    "section": "What Can Be Used in left= and right=?",
    "text": "What Can Be Used in left= and right=?\nThe left= and right= arguments both allow for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column in the target table\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value within left= and right=. There is flexibility in how you provide the date or datetime values for the bounds; they can be:\n\nstring-based dates or datetimes (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\ndate or datetime objects using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in either left= or right= (or both), it must be specified within col(). This facilitates column-to-column comparisons and, crucially, the columns being compared to either/both of the bounds must be of the same type as the column data (e.g., all numeric, all dates, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#preprocessing",
    "href": "reference/Validate.col_vals_outside.html#preprocessing",
    "title": "Validate.col_vals_outside",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and left=col(...)/right=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#thresholds",
    "href": "reference/Validate.col_vals_outside.html#thresholds",
    "title": "Validate.col_vals_outside",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_outside.html#examples",
    "href": "reference/Validate.col_vals_outside.html#examples",
    "title": "Validate.col_vals_outside",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 7, 5, 5],\n        \"b\": [2, 3, 6, 4, 3, 6],\n        \"c\": [9, 8, 8, 9, 9, 7],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    2\n    9\n  \n  \n    2\n    6\n    3\n    8\n  \n  \n    3\n    5\n    6\n    8\n  \n  \n    4\n    7\n    4\n    9\n  \n  \n    5\n    5\n    3\n    9\n  \n  \n    6\n    5\n    6\n    7\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all outside the fixed boundary values of 1 and 4. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_outside(columns=\"a\", left=1, right=4)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_between\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_outside()\n        \n        \n        \n    a\n    [1, 4]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_outside(). All test units passed, and there are no failing test units.\nAside from checking a column against two literal values representing the lower and upper bounds, we can also provide column names to the left= and/or right= arguments (by using the helper function col(). In this way, we can perform three additional comparison types:\n\nleft=column, right=column\nleft=literal, right=column\nleft=column, right=literal\n\nFor the next example, we’ll use col_vals_outside() to check whether the values in column b are outside of the range formed by the corresponding values in columns a (lower bound) and c (upper bound).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_outside(columns=\"b\", left=pb.col(\"a\"), right=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_between\n    \n        \n            \n            \n                \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_outside()\n        \n        \n        \n    b\n    [a, c]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 2: b is 6 and the bounds are 5 (a) and 8 (c).\nRow 5: b is 6 and the bounds are 5 (a) and 7 (c)."
  },
  {
    "objectID": "reference/everything.html",
    "href": "reference/everything.html",
    "title": "everything",
    "section": "",
    "text": "everything()\nSelect all columns.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The everything() selector function can be used to select every column in the table. If you have a table with six columns and they’re all suitable for a specific type of validation, you can use columns=everything()) and all six columns will be selected for validation."
  },
  {
    "objectID": "reference/everything.html#returns",
    "href": "reference/everything.html#returns",
    "title": "everything",
    "section": "Returns",
    "text": "Returns\n\n : Everything\n\nAn Everything object, which can be used to select all columns."
  },
  {
    "objectID": "reference/everything.html#relevant-validation-methods-where-everything-can-be-used",
    "href": "reference/everything.html#relevant-validation-methods-where-everything-can-be-used",
    "title": "everything",
    "section": "Relevant Validation Methods where everything() can be Used",
    "text": "Relevant Validation Methods where everything() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe everything() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/everything.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/everything.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "everything",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe everything() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names except those having starting with “id_”, you can use the everything() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(everything() - starts_with(\"id_\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/everything.html#examples",
    "href": "reference/everything.html#examples",
    "title": "everything",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with several numeric columns and we’d like to validate that all these columns have less than 1000. We can use the everything() column selector function to select all columns for validation.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.everything(), value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2023_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2023_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps, one each column in the table. The values in every column were all lower than 1000.\nWe can also use the everything() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select every column except those that begin with \"2023\" we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_lt(columns=pb.col(pb.everything() - pb.starts_with(\"2023\")), value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_hours\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    2024_pay_total\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2024_hours and one for 2024_pay_total."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html",
    "href": "reference/Validate.col_vals_ge.html",
    "title": "Validate.col_vals_ge",
    "section": "",
    "text": "Validate.col_vals_ge(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data greater than or equal to a fixed value or data in another column?\nThe col_vals_ge() validation method checks whether column values in a table are greater than or equal to a specified value= (the exact comparison used in this function is col_val &gt;= value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#parameters",
    "href": "reference/Validate.col_vals_ge.html#parameters",
    "title": "Validate.col_vals_ge",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#returns",
    "href": "reference/Validate.col_vals_ge.html#returns",
    "title": "Validate.col_vals_ge",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_ge.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_ge",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#preprocessing",
    "href": "reference/Validate.col_vals_ge.html#preprocessing",
    "title": "Validate.col_vals_ge",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#thresholds",
    "href": "reference/Validate.col_vals_ge.html#thresholds",
    "title": "Validate.col_vals_ge",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_ge.html#examples",
    "href": "reference/Validate.col_vals_ge.html#examples",
    "title": "Validate.col_vals_ge",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [5, 3, 1, 8, 2, 3],\n        \"c\": [2, 3, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    5\n    2\n  \n  \n    2\n    6\n    3\n    3\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    8\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    3\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all greater than or equal to the value of 5. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=\"a\", value=5)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_ge(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_ge() to check whether the values in column b are greater than values in column c.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_ge(columns=\"b\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    b\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 0: b is 2 and c is 3.\nRow 4: b is 3 and c is 4."
  },
  {
    "objectID": "reference/Validate.n_failed.html",
    "href": "reference/Validate.n_failed.html",
    "title": "Validate.n_failed",
    "section": "",
    "text": "Validate.n_failed(i=None, scalar=False)\nProvides a dictionary of the number of test units that failed for each validation step.\nThe n_failed() method provides the number of test units that failed for each validation step. This is the number of test units that did not pass in the the validation step. It is always some integer value between 0 and the total number of test units.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of failing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the n_passed() method (i.e., n - n_passed)."
  },
  {
    "objectID": "reference/Validate.n_failed.html#parameters",
    "href": "reference/Validate.n_failed.html#parameters",
    "title": "Validate.n_failed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of failing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n_failed.html#returns",
    "href": "reference/Validate.n_failed.html#returns",
    "title": "Validate.n_failed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of failing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n_failed.html#examples",
    "href": "reference/Validate.n_failed.html#examples",
    "title": "Validate.n_failed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps and, as it turns out, all of them will have failing test units. After interrogation, the n_failed() method is used to determine the number of failing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12],\n        \"b\": [9, 8, 10, 5, 10],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.n_failed()\n\n{1: 1, 2: 2, 3: 1}\n\n\nThe returned dictionary shows that all validation steps had failing test units.\nIf we wanted to check the number of failing test units for a single validation step, we can provide the step number. Also, we could forego the dictionary and get a scalar value by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.n_failed(i=1)\n\n{1: 1}\n\n\nThe returned value of 1 is the number of failing test units for the first validation step."
  },
  {
    "objectID": "reference/get_column_count.html",
    "href": "reference/get_column_count.html",
    "title": "get_column_count",
    "section": "",
    "text": "get_column_count(data)\nGet the number of columns in a table.\nThe get_column_count() function returns the number of columns in a table. The function works with any table that is supported by the pointblank library, including Pandas, Polars, and Ibis backend tables (e.g., DuckDB, MySQL, PostgreSQL, SQLite, Parquet, etc.)."
  },
  {
    "objectID": "reference/get_column_count.html#parameters",
    "href": "reference/get_column_count.html#parameters",
    "title": "get_column_count",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to get the column count, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/get_column_count.html#returns",
    "href": "reference/get_column_count.html#returns",
    "title": "get_column_count",
    "section": "Returns",
    "text": "Returns\n\n : int\n\nThe number of columns in the table."
  },
  {
    "objectID": "reference/get_column_count.html#supported-input-table-types",
    "href": "reference/get_column_count.html#supported-input-table-types",
    "title": "get_column_count",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using get_column_count() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/get_column_count.html#examples",
    "href": "reference/get_column_count.html#examples",
    "title": "get_column_count",
    "section": "Examples",
    "text": "Examples\nTo get the number of columns in a table, we can use the get_column_count() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table_polars = pb.load_dataset(\"small_table\")\n\npb.get_column_count(small_table_polars)\n\n8\n\n\nThis table is a Polars DataFrame, but the get_column_count() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nsmall_table_duckdb = pb.load_dataset(\"small_table\", tbl_type=\"duckdb\")\n\npb.get_column_count(small_table_duckdb)\n\n8\n\n\nThe function always returns the number of columns in the table as an integer value, which is 8 for the small_table dataset."
  },
  {
    "objectID": "reference/Validate.critical.html",
    "href": "reference/Validate.critical.html",
    "title": "Validate.critical",
    "section": "",
    "text": "Validate.critical(i=None, scalar=False)\nGet the ‘critical’ level status for each validation step.\nThe ‘critical’ status for a validation step is True if the fraction of failing test units meets or exceeds the threshold for the notification level. Otherwise, the status is False.\nThe ascribed name of ‘critical’ is semantic and is thus simply a status indicator that could be used to trigger some action to be take. Here’s how it fits in with other status indicators:\nThis method provides a dictionary of the notification status for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.critical.html#parameters",
    "href": "reference/Validate.critical.html#parameters",
    "title": "Validate.critical",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the notification status is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.critical.html#returns",
    "href": "reference/Validate.critical.html#returns",
    "title": "Validate.critical",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, bool] | bool\n\nA dictionary of the notification status for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.critical.html#examples",
    "href": "reference/Validate.critical.html#examples",
    "title": "Validate.critical",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the first step will have many failing test units, the rest will be completely passing. We’ve set thresholds here for each of the steps by using thresholds=(2, 4, 5), which means:\n\nthe ‘warning’ threshold is 2 failing test units\nthe ‘error’ threshold is 4 failing test units\nthe ‘critical’ threshold is 5 failing test units\n\nAfter interrogation, the critical() method is used to determine the ‘critical’ status for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [2, 4, 4, 7, 2, 3, 8],\n        \"b\": [9, 8, 10, 5, 10, 6, 2],\n        \"c\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl, thresholds=(2, 4, 5))\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_lt(columns=\"b\", value=15)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.critical()\n\n{1: True, 2: False, 3: False}\n\n\nThe returned dictionary provides the ‘critical’ status for each validation step. The first step has a True value since the number of failing test units meets the threshold for the ‘critical’ level. The second and third steps have False values since the number of failing test units was 0, which is below the threshold for the ‘critical’ level.\nWe can also visually inspect the ‘critical’ status across all steps by viewing the validation table:\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Pointblank Validation\n  \n  \n    2025-04-23|00:59:27PolarsWARNING2ERROR4CRITICAL5\n  \n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #FF3300\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    5\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    20.29\n    50.71\n    ●\n    ●\n    ●\n    CSV\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    b\n    15\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_in_set\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_in_set()\n        \n        \n        \n    c\n    a, b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    7\n    71.00\n    00.00\n    ○\n    ○\n    ○\n    —\n  \n\n  \n  \n  \n    2025-04-23 00:59:27 UTC&lt; 1 s2025-04-23 00:59:27 UTC\n  \n\n\n\n\n\n\n        \n\n\nWe can see that there are filled gray, yellow, and red circles in the first step (far right side, in the W, E, and C columns) indicating that the ‘warning’, ‘error’, and ‘critical’ thresholds were met. The other steps have empty gray, yellow, and red circles. This means that thresholds were ‘set but not met’ in those steps.\nIf we wanted to check the ‘critical’ status for a single validation step, we can provide the step number. Also, we could have the value returned as a scalar by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.critical(i=1)\n\n{1: True}\n\n\nThe returned value is True, indicating that the first validation step had the ‘critical’ threshold met."
  },
  {
    "objectID": "reference/matches.html",
    "href": "reference/matches.html",
    "title": "matches",
    "section": "",
    "text": "matches(pattern, case_sensitive=False)\nSelect columns that match a specified regular expression pattern.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The matches() selector function can be used to select one or more columns matching a provided regular expression pattern. So if the set of table columns consists of\n[rev_01, rev_02, profit_01, profit_02, age]\nand you want to validate columns that have two digits at the end of the name, you can use columns=matches(r\"\\d{2}$\"). This will select the rev_01, rev_02, profit_01, and profit_02 columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using matches() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/matches.html#parameters",
    "href": "reference/matches.html#parameters",
    "title": "matches",
    "section": "Parameters",
    "text": "Parameters\n\npattern : str\n\nThe regular expression pattern that the column name should match.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/matches.html#returns",
    "href": "reference/matches.html#returns",
    "title": "matches",
    "section": "Returns",
    "text": "Returns\n\n : Matches\n\nA Matches object, which can be used to select columns that match the specified pattern."
  },
  {
    "objectID": "reference/matches.html#relevant-validation-methods-where-matches-can-be-used",
    "href": "reference/matches.html#relevant-validation-methods-where-matches-can-be-used",
    "title": "matches",
    "section": "Relevant Validation Methods where matches() can be Used",
    "text": "Relevant Validation Methods where matches() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe matches() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/matches.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/matches.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "matches",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe matches() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that have the text starting with five digits and end with \"_id\", you can use the matches() and ends_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(matches(r\"^\\d{5}\") & ends_with(\"_id\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/matches.html#examples",
    "href": "reference/matches.html#examples",
    "title": "matches",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, id_old, new_identifier, and pay_2021 and we’d like to validate that text values in columns having \"id\" or \"identifier\" in the name have a specific syntax. We can use the matches() column selector function to specify the columns that match the pattern.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"id_old\": [\"ID0021\", \"ID0032\", \"ID0043\"],\n        \"new_identifier\": [\"ID9054\", \"ID9065\", \"ID9076\"],\n        \"pay_2021\": [16.32, 16.25, 15.75],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_regex(columns=pb.matches(\"id|identifier\"), pattern=r\"ID\\d{4}\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    id_old\n    ID\\d{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_regex\n    \n        \n            \n            \n                \n                \n            \n        \n    \n\n        \n        \n            col_vals_regex()\n        \n        \n        \n    new_identifier\n    ID\\d{4}\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for id_old and one for new_identifier. The values in both columns all match the pattern \"ID\\d{4}\".\nWe can also use the matches() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that contain \"pay\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"2022_hours\": [160, 180, 160],\n        \"2023_hours\": [182, 168, 175],\n        \"2024_hours\": [200, 165, 190],\n        \"2022_pay_total\": [18.62, 16.95, 18.25],\n        \"2023_pay_total\": [19.29, 17.75, 18.35],\n        \"2024_pay_total\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.contains(\"pay\") & pb.matches(\"2023|2024\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2023_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    2024_pay_total\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for 2023_pay_total and one for 2024_pay_total."
  },
  {
    "objectID": "reference/FinalActions.html",
    "href": "reference/FinalActions.html",
    "title": "FinalActions",
    "section": "",
    "text": "FinalActions(self, *args)\nDefine actions to be taken after validation is complete.\nFinal actions are executed after all validation steps have been completed. They provide a mechanism to respond to the overall validation results, such as sending alerts when critical failures are detected or generating summary reports.\n\n\n\n*actions : \n\nOne or more actions to execute after validation. An action can be (1) a callable function that will be executed with no arguments, or (2) a string message that will be printed to the console.\n\n\n\n\n\n\n : FinalActions\n\nAn FinalActions object. This can be used when using the Validate class (to set final actions for the validation workflow).\n\n\n\n\n\nFinal actions can be defined in two different ways:\n\nString: A message to be displayed when the validation is complete.\nCallable: A function that is called when the validation is complete.\n\nThe actions are executed at the end of the validation workflow. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of validation completion. Several strings and callables can be provided to the FinalActions class, and they will be executed in the order they are provided.\n\n\n\nWhen creating a callable function to be used as a final action, you can use the get_validation_summary() function to retrieve the summary of the validation results. This summary contains information about the validation workflow, including the number of test units, the number of failing test units, and the threshold levels that were exceeded. You can use this information to craft your final action message or to take specific actions based on the validation results.\n\n\n\nFinal actions provide a powerful way to respond to the overall results of a validation workflow. They’re especially useful for sending notifications, generating reports, or taking corrective actions based on the complete validation outcome.\nThe following example shows how to create a final action that checks for critical failures and sends an alert:\nimport pointblank as pb\n\ndef send_alert():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        print(f\"ALERT: Critical validation failures found in {summary['table_name']}\")\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_alert)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nIn this example, the send_alert() function is defined to check the validation summary for critical failures. If any are found, an alert message is printed to the console. The function is passed to the FinalActions class, which ensures it will be executed after all validation steps are complete. Note that we used the get_validation_summary() function to retrieve the summary of the validation results to help craft the alert message.\nMultiple final actions can be provided in a sequence. They will be executed in the order they are specified after all validation steps have completed:\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(\n            \"Validation complete.\",  # a string message\n            send_alert,              # a callable function\n            generate_report          # another callable function\n        )\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)"
  },
  {
    "objectID": "reference/FinalActions.html#parameters",
    "href": "reference/FinalActions.html#parameters",
    "title": "FinalActions",
    "section": "",
    "text": "*actions : \n\nOne or more actions to execute after validation. An action can be (1) a callable function that will be executed with no arguments, or (2) a string message that will be printed to the console."
  },
  {
    "objectID": "reference/FinalActions.html#returns",
    "href": "reference/FinalActions.html#returns",
    "title": "FinalActions",
    "section": "",
    "text": ": FinalActions\n\nAn FinalActions object. This can be used when using the Validate class (to set final actions for the validation workflow)."
  },
  {
    "objectID": "reference/FinalActions.html#types-of-actions",
    "href": "reference/FinalActions.html#types-of-actions",
    "title": "FinalActions",
    "section": "",
    "text": "Final actions can be defined in two different ways:\n\nString: A message to be displayed when the validation is complete.\nCallable: A function that is called when the validation is complete.\n\nThe actions are executed at the end of the validation workflow. When providing a string, it will simply be printed to the console. A callable will also be executed at the time of validation completion. Several strings and callables can be provided to the FinalActions class, and they will be executed in the order they are provided."
  },
  {
    "objectID": "reference/FinalActions.html#crafting-callables-with-get_validation_summary",
    "href": "reference/FinalActions.html#crafting-callables-with-get_validation_summary",
    "title": "FinalActions",
    "section": "",
    "text": "When creating a callable function to be used as a final action, you can use the get_validation_summary() function to retrieve the summary of the validation results. This summary contains information about the validation workflow, including the number of test units, the number of failing test units, and the threshold levels that were exceeded. You can use this information to craft your final action message or to take specific actions based on the validation results."
  },
  {
    "objectID": "reference/FinalActions.html#examples",
    "href": "reference/FinalActions.html#examples",
    "title": "FinalActions",
    "section": "",
    "text": "Final actions provide a powerful way to respond to the overall results of a validation workflow. They’re especially useful for sending notifications, generating reports, or taking corrective actions based on the complete validation outcome.\nThe following example shows how to create a final action that checks for critical failures and sends an alert:\nimport pointblank as pb\n\ndef send_alert():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        print(f\"ALERT: Critical validation failures found in {summary['table_name']}\")\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_alert)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nIn this example, the send_alert() function is defined to check the validation summary for critical failures. If any are found, an alert message is printed to the console. The function is passed to the FinalActions class, which ensures it will be executed after all validation steps are complete. Note that we used the get_validation_summary() function to retrieve the summary of the validation results to help craft the alert message.\nMultiple final actions can be provided in a sequence. They will be executed in the order they are specified after all validation steps have completed:\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(\n            \"Validation complete.\",  # a string message\n            send_alert,              # a callable function\n            generate_report          # another callable function\n        )\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)"
  },
  {
    "objectID": "reference/col_summary_tbl.html",
    "href": "reference/col_summary_tbl.html",
    "title": "col_summary_tbl",
    "section": "",
    "text": "col_summary_tbl(data, tbl_name=None)\nGenerate a column-level summary table of a dataset.\nThe col_summary_tbl() function generates a summary table of a dataset, focusing on providing column-level information about the dataset. The summary includes the following information:\nThe summary table is returned as a GT object, which can be displayed in a notebook or saved to an HTML file."
  },
  {
    "objectID": "reference/col_summary_tbl.html#parameters",
    "href": "reference/col_summary_tbl.html#parameters",
    "title": "col_summary_tbl",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table to summarize, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str | None = None\n\nOptionally, the name of the table could be provided as tbl_name=."
  },
  {
    "objectID": "reference/col_summary_tbl.html#returns",
    "href": "reference/col_summary_tbl.html#returns",
    "title": "col_summary_tbl",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the column-level summaries of the table."
  },
  {
    "objectID": "reference/col_summary_tbl.html#supported-input-table-types",
    "href": "reference/col_summary_tbl.html#supported-input-table-types",
    "title": "col_summary_tbl",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using col_summary_tbl() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/col_summary_tbl.html#examples",
    "href": "reference/col_summary_tbl.html#examples",
    "title": "col_summary_tbl",
    "section": "Examples",
    "text": "Examples\nIt’s easy to get a column-level summary of a table using the col_summary_tbl() function. Here’s an example using the small_table dataset (itself loaded using the load_dataset() function):\n\nimport pointblank as pb\n\nsmall_table_polars = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\n\npb.col_summary_tbl(data=small_table_polars)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    date_timeDatetime(time_unit='us', time_zone=None)\n    0 0.00\n    12 0.92\n    —\n    —\n     2016-01-04 00:32:00 – 2016-01-30 11:23:00\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    2\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dateDate\n    0 0.00\n    11 0.85\n    —\n    —\n     2016-01-04 – 2016-01-30\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    3\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    aInt64\n    0 0.00\n    7 0.54\n    3.77\n    2.09\n    1.00\n    1.60\n    2.00\n    3.00\n    4.00\n    7.40\n    8.00\n    2.00\n  \n  \n    4\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    bString\n    0 0.00\n    12 0.92\n    9.00SL\n    0.00SL\n    9SL\n    —\n    —\n    9SL\n    —\n    —\n    9SL\n    —\n  \n  \n    5\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    cInt64\n    2 0.15\n    6 0.46\n    5.73\n    2.72\n    2.00\n    2.50\n    3.00\n    7.00\n    8.00\n    9.00\n    9.00\n    5.00\n  \n  \n    6\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dFloat64\n    0 0.00\n    12 0.92\n    2305\n    2631\n    108\n    214\n    838\n    1036\n    3291\n    6335\n    10000\n    2453\n  \n  \n    7\n    \n    boolean\n    \n        \n            \n            \n                \n            \n            \n                \n            \n            \n        \n    \n\n    eBoolean\n    0 0.00\n    T 0.61F 0.39\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n  \n    8\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    fString\n    0 0.00\n    3 0.23\n    3.46SL\n    0.52SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    4SL\n    —\n  \n\n\n\n\n\n\n        \n\n\nThis table used above was a Polars DataFrame, but the col_summary_tbl() function works with any table supported by pointblank, including Pandas DataFrames and Ibis backend tables. Here’s an example using a DuckDB table handled by Ibis:\n\nsmall_table_duckdb = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"duckdb\")\n\npb.col_summary_tbl(data=small_table_duckdb, tbl_name=\"nycflights\")\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBnycflightsRows336,776Columns18\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    yearint64\n    0 0.00\n    1&lt;0.01\n    2013\n    0.00\n    2013\n    2013\n    2013\n    2013\n    2013\n    2013\n    2013\n    0\n  \n  \n    2\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    monthint64\n    0 0.00\n    12&lt;0.01\n    6.55\n    3.41\n    1.00\n    1.00\n    4.00\n    7.00\n    9.28\n    12.0\n    12.0\n    6.00\n  \n  \n    3\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dayint64\n    0 0.00\n    31&lt;0.01\n    15.7\n    8.77\n    1.00\n    2.00\n    8.06\n    16.0\n    23.0\n    29.4\n    31.0\n    15.0\n  \n  \n    4\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dep_timeint64\n    8255 0.02\n    1317&lt;0.01\n    1349\n    488\n    1.00\n    623\n    904\n    1401\n    1744\n    2111\n    2400\n    837\n  \n  \n    5\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    sched_dep_timeint64\n    0 0.00\n    1021&lt;0.01\n    1344\n    467\n    106\n    630\n    909\n    1359\n    1729\n    2051\n    2359\n    823\n  \n  \n    6\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dep_delayint64\n    8255 0.02\n    526&lt;0.01\n    12.6\n    40.2\n    −43.0\n    −9.00\n    −5.00\n    −2.00\n    10.8\n    88.3\n    1301\n    16.0\n  \n  \n    7\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    arr_timeint64\n    8713 0.03\n    1410&lt;0.01\n    1502\n    533\n    1.00\n    734\n    1096\n    1535\n    1941\n    2248\n    2400\n    836\n  \n  \n    8\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    sched_arr_timeint64\n    0 0.00\n    1163&lt;0.01\n    1536\n    497\n    1.00\n    816\n    1124\n    1556\n    1945\n    2247\n    2359\n    821\n  \n  \n    9\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    arr_delayint64\n    9430 0.03\n    576&lt;0.01\n    6.90\n    44.6\n    −86.0\n    −32.2\n    −17.0\n    −5.00\n    13.8\n    91.2\n    1272\n    31.0\n  \n  \n    10\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    carrierstring\n    0 0.00\n    16&lt;0.01\n    2.00SL\n    0.00SL\n    2SL\n    —\n    —\n    2SL\n    —\n    —\n    2SL\n    —\n  \n  \n    11\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    flightint64\n    0 0.00\n    3844 0.01\n    1972\n    1632\n    1.00\n    93.7\n    558\n    1496\n    3465\n    4703\n    8500\n    2912\n  \n  \n    12\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    tailnumstring\n    2512&lt;0.01\n    4042 0.01\n    6.00SL\n    0.07SL\n    5SL\n    —\n    —\n    6SL\n    —\n    —\n    6SL\n    —\n  \n  \n    13\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    originstring\n    0 0.00\n    3&lt;0.01\n    3.00SL\n    0.00SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    3SL\n    —\n  \n  \n    14\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    deststring\n    0 0.00\n    105&lt;0.01\n    3.00SL\n    0.00SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    3SL\n    —\n  \n  \n    15\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    air_timeint64\n    9430 0.03\n    508&lt;0.01\n    151\n    93.7\n    20.0\n    40.0\n    82.3\n    129\n    191\n    339\n    695\n    110\n  \n  \n    16\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    distanceint64\n    0 0.00\n    214&lt;0.01\n    1040\n    733\n    17.0\n    198\n    509\n    872\n    1389\n    2477\n    4983\n    887\n  \n  \n    17\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    hourint64\n    0 0.00\n    20&lt;0.01\n    13.2\n    4.66\n    1\n    6\n    9\n    13\n    17\n    20\n    23\n    8\n  \n  \n    18\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    minuteint64\n    0 0.00\n    60&lt;0.01\n    26.2\n    19.3\n    0.00\n    0.00\n    7.94\n    29.0\n    43.4\n    58.0\n    59.0\n    36.0"
  },
  {
    "objectID": "reference/Validate.all_passed.html",
    "href": "reference/Validate.all_passed.html",
    "title": "Validate.all_passed",
    "section": "",
    "text": "Validate.all_passed()\nDetermine if every validation step passed perfectly, with no failing test units.\nThe all_passed() method determines if every validation step passed perfectly, with no failing test units. This method is useful for quickly checking if the table passed all validation steps with flying colors. If there’s even a single failing test unit in any validation step, this method will return False.\nThis validation metric might be overly stringent for some validation plans where failing test units are generally expected (and the strategy is to monitor data quality over time). However, the value of all_passed() could be suitable for validation plans designed to ensure that every test unit passes perfectly (e.g., checks for column presence, null-checking tests, etc.)."
  },
  {
    "objectID": "reference/Validate.all_passed.html#returns",
    "href": "reference/Validate.all_passed.html#returns",
    "title": "Validate.all_passed",
    "section": "Returns",
    "text": "Returns\n\n : bool\n\nTrue if all validation steps had no failing test units, False otherwise."
  },
  {
    "objectID": "reference/Validate.all_passed.html#examples",
    "href": "reference/Validate.all_passed.html#examples",
    "title": "Validate.all_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps, and the second step will have a failing test unit (the value 10 isn’t less than 9). After interrogation, the all_passed() method is used to determine if all validation steps passed perfectly.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 9, 5],\n        \"b\": [5, 6, 10, 3],\n        \"c\": [\"a\", \"b\", \"a\", \"a\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=0)\n    .col_vals_lt(columns=\"b\", value=9)\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.all_passed()\n\nFalse\n\n\nThe returned value is False since the second validation step had a failing test unit. If it weren’t for that one failing test unit, the return value would have been True."
  },
  {
    "objectID": "reference/starts_with.html",
    "href": "reference/starts_with.html",
    "title": "starts_with",
    "section": "",
    "text": "starts_with(text, case_sensitive=False)\nSelect columns that start with specified text.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The starts_with() selector function can be used to select one or more columns that start with some specified text. So if the set of table columns consists of\n[name_first, name_last, age, address]\nand you want to validate columns that start with \"name\", you can use columns=starts_with(\"name\"). This will select the name_first and name_last columns.\nThere will be a validation step created for every resolved column. Note that if there aren’t any columns resolved from using starts_with() (or any other expression using selector functions), the validation step will fail to be evaluated during the interrogation process. Such a failure to evaluate will be reported in the validation results but it won’t affect the interrogation process overall (i.e., the process won’t be halted)."
  },
  {
    "objectID": "reference/starts_with.html#parameters",
    "href": "reference/starts_with.html#parameters",
    "title": "starts_with",
    "section": "Parameters",
    "text": "Parameters\n\ntext : str\n\nThe text that the column name should start with.\n\ncase_sensitive : bool = False\n\nWhether column names should be treated as case-sensitive. The default is False."
  },
  {
    "objectID": "reference/starts_with.html#returns",
    "href": "reference/starts_with.html#returns",
    "title": "starts_with",
    "section": "Returns",
    "text": "Returns\n\n : StartsWith\n\nA StartsWith object, which can be used to select columns that start with the specified text."
  },
  {
    "objectID": "reference/starts_with.html#relevant-validation-methods-where-starts_with-can-be-used",
    "href": "reference/starts_with.html#relevant-validation-methods-where-starts_with-can-be-used",
    "title": "starts_with",
    "section": "Relevant Validation Methods where starts_with() can be Used",
    "text": "Relevant Validation Methods where starts_with() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe starts_with() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/starts_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/starts_with.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "starts_with",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe starts_with() function can be composed with other column selectors to create fine-grained column selections. For example, to select columns that start with \"a\" and end with \"e\", you can use the starts_with() and ends_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(starts_with(\"a\") & ends_with(\"e\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/starts_with.html#examples",
    "href": "reference/starts_with.html#examples",
    "title": "starts_with",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns name, paid_2021, paid_2022, and person_id and we’d like to validate that the values in columns that start with \"paid\" are greater than 10. We can use the starts_with() column selector function to specify the columns that start with \"paid\" as the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"paid_2021\": [16.32, 16.25, 15.75],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"person_id\": [\"A123\", \"B456\", \"C789\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.starts_with(\"paid\"), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for paid_2021 and one for paid_2022. The values in both columns were all greater than 10.\nWe can also use the starts_with() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select columns that start with \"paid\" and match the text \"2023\" or \"2024\", we can use the & operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"hours_2022\": [160, 180, 160],\n        \"hours_2023\": [182, 168, 175],\n        \"hours_2024\": [200, 165, 190],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(\n        columns=pb.col(pb.starts_with(\"paid\") & pb.matches(\"23|24\")),\n        value=10\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get two validation steps, one for paid_2023 and one for paid_2024."
  },
  {
    "objectID": "reference/assistant.html",
    "href": "reference/assistant.html",
    "title": "assistant",
    "section": "",
    "text": "assistant(model, data=None, tbl_name=None, api_key=None, display=None)\nChat with the PbA (Pointblank Assistant) about your data validation needs.\nThe assistant() function provides an interactive chat session with the PbA (Pointblank Assistant) to help you with your data validation needs. The PbA can help you with constructing validation plans, suggesting validation methods, and providing code snippets for using the Pointblank Python package. Feel free to ask the PbA about any aspect of the Pointblank package and it will do its best to assist you.\nThe PbA can also help you with constructing validation plans for your data tables. If you provide a data table to the PbA, it will internally generate a JSON summary of the table and use that information to suggest validation methods that can be used with the Pointblank package. If using a Polars table as the data source, the PbA will be knowledgeable about the Polars API and can smartly suggest validation steps that use aggregate measures with up-to-date Polars methods.\nThe PbA can be used with models from the following providers:\n\nAnthropic\nOpenAI\nOllama\nAmazon Bedrock\n\nThe PbA can be displayed in a browser (the default) or in the terminal. You can choose one or the other by setting the display= parameter to \"browser\" or \"terminal\".\n\n\n\n\n\n\nWarning\n\n\n\nThe assistant() function is still experimental. Please report any issues you encounter in the Pointblank issue tracker.\n\n\n\n\n\nmodel : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\ndata : FrameT | Any | None = None\n\nAn optional data table to focus on during discussion with the PbA, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str = None\n\nThe name of the data table. This is optional and is only used to provide a more detailed prompt to the PbA.\n\napi_key : str = None\n\nThe API key to be used for the model.\n\ndisplay : str = None\n\nThe display mode to use for the chat session. Supported values are \"browser\" and \"terminal\". If not provided, the default value is \"browser\".\n\n\n\n\n\n\n : None\n\nNothing is returned. Rather, you get an an interactive chat session with the PbA, which is displayed in a browser or in the terminal.\n\n\n\n\n\nThe model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names.\n\n\n\nProviding a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way.\n\n\n\nIf data= is provided then that data is sent to the model provider is a JSON summary of the table. This data summary is generated internally by use of the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information be knowledgable about the data table. Compared to the size of the entire table, the JSON summary is quite small and can be safely sent to the model provider.\nThe Amazon Bedrock provider is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally.\n\n\n\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using preview() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/assistant.html#parameters",
    "href": "reference/assistant.html#parameters",
    "title": "assistant",
    "section": "",
    "text": "model : str\n\nThe model to be used. This should be in the form of provider:model (e.g., \"anthropic:claude-3-5-sonnet-latest\"). Supported providers are \"anthropic\", \"openai\", \"ollama\", and \"bedrock\".\n\ndata : FrameT | Any | None = None\n\nAn optional data table to focus on during discussion with the PbA, which could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types.\n\ntbl_name : str = None\n\nThe name of the data table. This is optional and is only used to provide a more detailed prompt to the PbA.\n\napi_key : str = None\n\nThe API key to be used for the model.\n\ndisplay : str = None\n\nThe display mode to use for the chat session. Supported values are \"browser\" and \"terminal\". If not provided, the default value is \"browser\"."
  },
  {
    "objectID": "reference/assistant.html#returns",
    "href": "reference/assistant.html#returns",
    "title": "assistant",
    "section": "",
    "text": ": None\n\nNothing is returned. Rather, you get an an interactive chat session with the PbA, which is displayed in a browser or in the terminal."
  },
  {
    "objectID": "reference/assistant.html#constructing-the-model-argument",
    "href": "reference/assistant.html#constructing-the-model-argument",
    "title": "assistant",
    "section": "",
    "text": "The model= argument should be constructed using the provider and model name separated by a colon (provider:model). The provider text can any of:\n\n\"anthropic\" (Anthropic)\n\"openai\" (OpenAI)\n\"ollama\" (Ollama)\n\"bedrock\" (Amazon Bedrock)\n\nThe model name should be the specific model to be used from the provider. Model names are subject to change so consult the provider’s documentation for the most up-to-date model names."
  },
  {
    "objectID": "reference/assistant.html#notes-on-authentication",
    "href": "reference/assistant.html#notes-on-authentication",
    "title": "assistant",
    "section": "",
    "text": "Providing a valid API key as a string in the api_key argument is adequate for getting started but you should consider using a more secure method for handling API keys.\nOne way to do this is to load the API key from an environent variable and retrieve it using the os module (specifically the os.getenv() function). Places to store the API key might include .bashrc, .bash_profile, .zshrc, or .zsh_profile.\nAnother solution is to store one or more model provider API keys in an .env file (in the root of your project). If the API keys have correct names (e.g., ANTHROPIC_API_KEY or OPENAI_API_KEY) then DraftValidation will automatically load the API key from the .env file and there’s no need to provide the api_key argument. An .env file might look like this:\nANTHROPIC_API_KEY=\"your_anthropic_api_key_here\"\nOPENAI_API_KEY=\"your_openai_api_key_here\"\nThere’s no need to have the python-dotenv package installed when using .env files in this way."
  },
  {
    "objectID": "reference/assistant.html#notes-on-data-sent-to-the-model-provider",
    "href": "reference/assistant.html#notes-on-data-sent-to-the-model-provider",
    "title": "assistant",
    "section": "",
    "text": "If data= is provided then that data is sent to the model provider is a JSON summary of the table. This data summary is generated internally by use of the DataScan class. The summary includes the following information:\n\nthe number of rows and columns in the table\nthe type of dataset (e.g., Polars, DuckDB, Pandas, etc.)\nthe column names and their types\ncolumn level statistics such as the number of missing values, min, max, mean, and median, etc.\na short list of data values in each column\n\nThe JSON summary is used to provide the model with the necessary information be knowledgable about the data table. Compared to the size of the entire table, the JSON summary is quite small and can be safely sent to the model provider.\nThe Amazon Bedrock provider is a special case since it is a self-hosted model and security controls are in place to ensure that data is kept within the user’s AWS environment. If using an Ollama model all data is handled locally."
  },
  {
    "objectID": "reference/assistant.html#supported-input-table-types",
    "href": "reference/assistant.html#supported-input-table-types",
    "title": "assistant",
    "section": "",
    "text": "The data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using preview() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/Validate.n_passed.html",
    "href": "reference/Validate.n_passed.html",
    "title": "Validate.n_passed",
    "section": "",
    "text": "Validate.n_passed(i=None, scalar=False)\nProvides a dictionary of the number of test units that passed for each validation step.\nThe n_passed() method provides the number of test units that passed for each validation step. This is the number of test units that passed in the the validation step. It is always some integer value between 0 and the total number of test units.\nTest units are the atomic units of the validation process. Different validations can have different numbers of test units. For example, a validation that checks for the presence of a column in a table will have a single test unit. A validation that checks for the presence of a value in a column will have as many test units as there are rows in the table.\nThe method provides a dictionary of the number of passing test units for each validation step. If the scalar=True argument is provided and i= is a scalar, the value is returned as a scalar instead of a dictionary. Furthermore, a value obtained here will be the complement to the analogous value returned by the n_passed() method (i.e., n - n_failed)."
  },
  {
    "objectID": "reference/Validate.n_passed.html#parameters",
    "href": "reference/Validate.n_passed.html#parameters",
    "title": "Validate.n_passed",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the number of passing test units is obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nscalar : bool = False\n\nIf True and i= is a scalar, return the value as a scalar instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.n_passed.html#returns",
    "href": "reference/Validate.n_passed.html#returns",
    "title": "Validate.n_passed",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, int] | int\n\nA dictionary of the number of passing test units for each validation step or a scalar value."
  },
  {
    "objectID": "reference/Validate.n_passed.html#examples",
    "href": "reference/Validate.n_passed.html#examples",
    "title": "Validate.n_passed",
    "section": "Examples",
    "text": "Examples\nIn the example below, we’ll use a simple Polars DataFrame with three columns (a, b, and c). There will be three validation steps and, as it turns out, all of them will have failing test units. After interrogation, the n_passed() method is used to determine the number of passing test units for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [7, 4, 9, 7, 12],\n        \"b\": [9, 8, 10, 5, 10],\n        \"c\": [\"a\", \"b\", \"c\", \"a\", \"b\"]\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=5)\n    .col_vals_gt(columns=\"b\", value=pb.col(\"a\"))\n    .col_vals_in_set(columns=\"c\", set=[\"a\", \"b\"])\n    .interrogate()\n)\n\nvalidation.n_passed()\n\n{1: 4, 2: 3, 3: 4}\n\n\nThe returned dictionary shows that all validation steps had no passing test units (each value was less than 5, which is the total number of test units for each step).\nIf we wanted to check the number of passing test units for a single validation step, we can provide the step number. Also, we could forego the dictionary and get a scalar value by setting scalar=True (ensuring that i= is a scalar).\n\nvalidation.n_passed(i=1)\n\n{1: 4}\n\n\nThe returned value of 4 is the number of passing test units for the first validation step."
  },
  {
    "objectID": "reference/Validate.conjointly.html",
    "href": "reference/Validate.conjointly.html",
    "title": "Validate.conjointly",
    "section": "",
    "text": "Validate.conjointly(\n    *exprs,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nPerform multiple row-wise validations for joint validity.\nThe conjointly() validation method checks whether each row in the table passes multiple validation conditions simultaneously. This enables compound validation logic where a test unit (typically a row) must satisfy all specified conditions to pass the validation.\nThis method accepts multiple validation expressions as callables, which should return boolean expressions when applied to the data. You can use lambdas that incorporate Polars/Pandas/Ibis expressions (based on the target table type) or create more complex validation functions. The validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.conjointly.html#parameters",
    "href": "reference/Validate.conjointly.html#parameters",
    "title": "Validate.conjointly",
    "section": "Parameters",
    "text": "Parameters\n\n*exprs : Callable = ()\n\nMultiple validation expressions provided as callable functions. Each callable should accept a table as its single argument and return a boolean expression or Series/Column that evaluates to boolean values for each row.\n\npre : Callable | None = None\n\nAn optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.conjointly.html#returns",
    "href": "reference/Validate.conjointly.html#returns",
    "title": "Validate.conjointly",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.conjointly.html#preprocessing",
    "href": "reference/Validate.conjointly.html#preprocessing",
    "title": "Validate.conjointly",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.conjointly.html#thresholds",
    "href": "reference/Validate.conjointly.html#thresholds",
    "title": "Validate.conjointly",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.conjointly.html#examples",
    "href": "reference/Validate.conjointly.html#examples",
    "title": "Validate.conjointly",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 7, 1, 3, 9, 4],\n        \"b\": [6, 3, 0, 5, 8, 2],\n        \"c\": [10, 4, 8, 9, 10, 5],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    6\n    10\n  \n  \n    2\n    7\n    3\n    4\n  \n  \n    3\n    1\n    0\n    8\n  \n  \n    4\n    3\n    5\n    9\n  \n  \n    5\n    9\n    8\n    10\n  \n  \n    6\n    4\n    2\n    5\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the values in each row satisfy multiple conditions simultaneously:\n\nColumn a should be greater than 2\nColumn b should be less than 7\nThe sum of a and b should be less than the value in column c\n\nWe’ll use conjointly() to check all these conditions together:\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pl.col(\"a\") &gt; 2,\n        lambda df: pl.col(\"b\") &lt; 7,\n        lambda df: pl.col(\"a\") + pl.col(\"b\") &lt; pl.col(\"c\")\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    10.17\n    50.83\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table shows that not all rows satisfy all three conditions together. For a row to pass the conjoint validation, all three conditions must be true for that row.\nWe can also use preprocessing to filter the data before applying the conjoint validation:\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pl.col(\"a\") &gt; 2,\n        lambda df: pl.col(\"b\") &lt; 7,\n        lambda df: pl.col(\"a\") + pl.col(\"b\") &lt; pl.col(\"c\"),\n        pre=lambda df: df.filter(pl.col(\"c\") &gt; 5)\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    10.25\n    30.75\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThis allows for more complex validation scenarios where the data is first prepared and then validated against multiple conditions simultaneously.\nOr, you can use the backend-agnostic column expression helper expr_col() to write expressions that work across different table backends:\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 7, 1, 3, 9, 4],\n        \"b\": [6, 3, 0, 5, 8, 2],\n        \"c\": [10, 4, 8, 9, 10, 5],\n    }\n)\n\n# Using backend-agnostic syntax with expr_col()\nvalidation = (\n    pb.Validate(data=tbl)\n    .conjointly(\n        lambda df: pb.expr_col(\"a\") &gt; 2,\n        lambda df: pb.expr_col(\"b\") &lt; 7,\n        lambda df: pb.expr_col(\"a\") + pb.expr_col(\"b\") &lt; pb.expr_col(\"c\")\n    )\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    conjointly\n    \n        \n            \n            \n        \n    \n\n        \n        \n            conjointly()\n        \n        \n        \n    \n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    10.17\n    50.83\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nUsing expr_col() allows your validation code to work consistently across Pandas, Polars, and Ibis table backends without changes, making your validation pipelines more portable."
  },
  {
    "objectID": "reference/Validate.conjointly.html#see-also",
    "href": "reference/Validate.conjointly.html#see-also",
    "title": "Validate.conjointly",
    "section": "See Also",
    "text": "See Also\nLook at the documentation of the expr_col() function for more information on how to use it with different table backends."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html",
    "href": "reference/Validate.col_vals_expr.html",
    "title": "Validate.col_vals_expr",
    "section": "",
    "text": "Validate.col_vals_expr(\n    expr,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate column values using a custom expression.\nThe col_vals_expr() validation method checks whether column values in a table satisfy a custom expr= expression. This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#parameters",
    "href": "reference/Validate.col_vals_expr.html#parameters",
    "title": "Validate.col_vals_expr",
    "section": "Parameters",
    "text": "Parameters\n\nexpr : any\n\nA column expression that will evaluate each row in the table, returning a boolean value per table row. If the target table is a Polars DataFrame, the expression should either be a Polars column expression or a Narwhals one. For a Pandas DataFrame, the expression should either be a lambda expression or a Narwhals column expression.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#returns",
    "href": "reference/Validate.col_vals_expr.html#returns",
    "title": "Validate.col_vals_expr",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#preprocessing",
    "href": "reference/Validate.col_vals_expr.html#preprocessing",
    "title": "Validate.col_vals_expr",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#thresholds",
    "href": "reference/Validate.col_vals_expr.html#thresholds",
    "title": "Validate.col_vals_expr",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_expr.html#examples",
    "href": "reference/Validate.col_vals_expr.html#examples",
    "title": "Validate.col_vals_expr",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [1, 2, 1, 7, 8, 6],\n        \"b\": [0, 0, 0, 1, 1, 1],\n        \"c\": [0.5, 0.3, 0.8, 1.4, 1.9, 1.2],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cFloat64\n\n\n\n  \n    1\n    1\n    0\n    0.5\n  \n  \n    2\n    2\n    0\n    0.3\n  \n  \n    3\n    1\n    0\n    0.8\n  \n  \n    4\n    7\n    1\n    1.4\n  \n  \n    5\n    8\n    1\n    1.9\n  \n  \n    6\n    6\n    1\n    1.2\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that the values in column a are all integers. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_expr(expr=pl.col(\"a\") % 1 == 0)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_expr\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_expr()\n        \n        \n        \n    —\n    COLUMN EXPR\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_expr(). All test units passed, with no failing test units."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html",
    "href": "reference/Validate.col_vals_le.html",
    "title": "Validate.col_vals_le",
    "section": "",
    "text": "Validate.col_vals_le(\n    columns,\n    value,\n    na_pass=False,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nAre column data less than or equal to a fixed value or data in another column?\nThe col_vals_le() validation method checks whether column values in a table are less than or equal to a specified value= (the exact comparison used in this function is col_val &lt;= value). The value= can be specified as a single, literal value or as a column name given in col(). This validation will operate over the number of test units that is equal to the number of rows in the table (determined after any pre= mutation has been applied)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#parameters",
    "href": "reference/Validate.col_vals_le.html#parameters",
    "title": "Validate.col_vals_le",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\nvalue : float | int | Column\n\nThe value to compare against. This can be a single value or a single column name given in col(). The latter option allows for a column-to-column comparison. For more information on which types of values are allowed, see the What Can Be Used in value=? section.\n\nna_pass : bool = False\n\nShould any encountered None, NA, or Null values be considered as passing test units? By default, this is False. Set to True to pass test units with missing values.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#returns",
    "href": "reference/Validate.col_vals_le.html#returns",
    "title": "Validate.col_vals_le",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#what-can-be-used-in-value",
    "href": "reference/Validate.col_vals_le.html#what-can-be-used-in-value",
    "title": "Validate.col_vals_le",
    "section": "What Can Be Used in value=?",
    "text": "What Can Be Used in value=?\nThe value= argument allows for a variety of input types. The most common are:\n\na single numeric value\na single date or datetime value\nA col() object that represents a column name\n\nWhen supplying a number as the basis of comparison, keep in mind that all resolved columns must also be numeric. Should you have columns that are of the date or datetime types, you can supply a date or datetime value as the value= argument. There is flexibility in how you provide the date or datetime value, as it can be:\n\na string-based date or datetime (e.g., \"2023-10-01\", \"2023-10-01 13:45:30\", etc.)\na date or datetime object using the datetime module (e.g., datetime.date(2023, 10, 1), datetime.datetime(2023, 10, 1, 13, 45, 30), etc.)\n\nFinally, when supplying a column name in the value= argument, it must be specified within col(). This is a column-to-column comparison and, crucially, the columns being compared must be of the same type (e.g., both numeric, both date, etc.)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#preprocessing",
    "href": "reference/Validate.col_vals_le.html#preprocessing",
    "title": "Validate.col_vals_le",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to columns via columns= and value=col(...) that are expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#thresholds",
    "href": "reference/Validate.col_vals_le.html#thresholds",
    "title": "Validate.col_vals_le",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_le.html#examples",
    "href": "reference/Validate.col_vals_le.html#examples",
    "title": "Validate.col_vals_le",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with three numeric columns (a, b, and c). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 9, 7, 5],\n        \"b\": [1, 3, 1, 5, 2, 5],\n        \"c\": [2, 1, 1, 4, 3, 4],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    1\n    5\n    1\n    2\n  \n  \n    2\n    6\n    3\n    1\n  \n  \n    3\n    5\n    1\n    1\n  \n  \n    4\n    9\n    5\n    4\n  \n  \n    5\n    7\n    2\n    3\n  \n  \n    6\n    5\n    5\n    4\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that values in column a are all less than or equal to the value of 9. We’ll determine if this validation had any failing test units (there are six test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_le(columns=\"a\", value=9)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    a\n    9\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    61.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_le(). All test units passed, and there are no failing test units.\nAside from checking a column against a literal value, we can also use a column name in the value= argument (with the helper function col() to perform a column-to-column comparison. For the next example, we’ll use col_vals_le() to check whether the values in column c are less than values in column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_le(columns=\"c\", value=pb.col(\"b\"))\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_le()\n        \n        \n        \n    c\n    b\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    6\n    40.67\n    20.33\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are:\n\nRow 0: c is 2 and b is 1.\nRow 4: c is 3 and b is 2."
  },
  {
    "objectID": "reference/get_validation_summary.html",
    "href": "reference/get_validation_summary.html",
    "title": "get_validation_summary",
    "section": "",
    "text": "get_validation_summary()\nAccess validation summary information when authoring final actions.\nThis function provides a convenient way to access summary information about the validation process within a final action. It returns a dictionary with key metrics from the validation process.\n\n\n\n : dict | None\n\nA dictionary containing validation metrics, or None if called outside a final action.\n\n\n\n\n\nThe summary dictionary contains the following fields:\n\nn_steps (int): The total number of validation steps.\nn_passing_steps (int): The number of validation steps where all test units passed.\nn_failing_steps (int): The number of validation steps that had some failing test units.\nn_warning_steps (int): The number of steps that exceeded a ‘warning’ threshold.\nn_error_steps (int): The number of steps that exceeded an ‘error’ threshold.\nn_critical_steps (int): The number of steps that exceeded a ‘critical’ threshold.\nlist_passing_steps (list[int]): List of step numbers where all test units passed.\nlist_failing_steps (list[int]): List of step numbers for steps having failing test units.\ndict_n (dict): The number of test units for each validation step.\ndict_n_passed (dict): The number of test units that passed for each validation step.\ndict_n_failed (dict): The number of test units that failed for each validation step.\ndict_f_passed (dict): The fraction of test units that passed for each validation step.\ndict_f_failed (dict): The fraction of test units that failed for each validation step.\ndict_warning (dict): The ‘warning’ level status for each validation step.\ndict_error (dict): The ‘error’ level status for each validation step.\ndict_critical (dict): The ‘critical’ level status for each validation step.\nall_passed (bool): Whether or not every validation step had no failing test units.\nhighest_severity (str): The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\ntbl_row_count (int): The number of rows in the target table.\ntbl_column_count (int): The number of columns in the target table.\ntbl_name (str): The name of the target table.\nvalidation_duration (float): The duration of the validation in seconds.\n\nNote that the summary dictionary is only available within the context of a final action. If called outside of a final action (i.e., when no final action is being executed), this function will return None.\n\n\n\nFinal actions are executed after the completion of all validation steps. They provide an opportunity to take appropriate actions based on the overall validation results. Here’s an example of a final action function (send_report()) that sends an alert when critical validation failures are detected:\nimport pointblank as pb\n\ndef send_report():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        # Send an alert email\n        send_alert_email(\n            subject=f\"CRITICAL validation failures in {summary['tbl_name']}\",\n            body=f\"{summary['n_critical_steps']} steps failed with critical severity.\"\n        )\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_report)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nNote that send_alert_email() in the example above is a placeholder function that would be implemented by the user to send email alerts. This function is not provided by the Pointblank package.\nThe get_validation_summary() function can also be used to create custom reporting for validation results:\ndef log_validation_results():\n    summary = pb.get_validation_summary()\n\n    print(f\"Validation completed with status: {summary['highest_severity'].upper()}\")\n    print(f\"Steps: {summary['n_steps']} total\")\n    print(f\"  - {summary['n_passing_steps']} passing, {summary['n_failing_steps']} failing\")\n    print(\n        f\"  - Severity: {summary['n_warning_steps']} warnings, \"\n        f\"{summary['n_error_steps']} errors, \"\n        f\"{summary['n_critical_steps']} critical\"\n    )\n\n    if summary['highest_severity'] in [\"error\", \"critical\"]:\n        print(\"⚠️ Action required: Please review failing validation steps!\")\nFinal actions work well with both simple logging and more complex notification systems, allowing you to integrate validation results into your broader data quality workflows."
  },
  {
    "objectID": "reference/get_validation_summary.html#returns",
    "href": "reference/get_validation_summary.html#returns",
    "title": "get_validation_summary",
    "section": "",
    "text": ": dict | None\n\nA dictionary containing validation metrics, or None if called outside a final action."
  },
  {
    "objectID": "reference/get_validation_summary.html#description-of-the-summary-fields",
    "href": "reference/get_validation_summary.html#description-of-the-summary-fields",
    "title": "get_validation_summary",
    "section": "",
    "text": "The summary dictionary contains the following fields:\n\nn_steps (int): The total number of validation steps.\nn_passing_steps (int): The number of validation steps where all test units passed.\nn_failing_steps (int): The number of validation steps that had some failing test units.\nn_warning_steps (int): The number of steps that exceeded a ‘warning’ threshold.\nn_error_steps (int): The number of steps that exceeded an ‘error’ threshold.\nn_critical_steps (int): The number of steps that exceeded a ‘critical’ threshold.\nlist_passing_steps (list[int]): List of step numbers where all test units passed.\nlist_failing_steps (list[int]): List of step numbers for steps having failing test units.\ndict_n (dict): The number of test units for each validation step.\ndict_n_passed (dict): The number of test units that passed for each validation step.\ndict_n_failed (dict): The number of test units that failed for each validation step.\ndict_f_passed (dict): The fraction of test units that passed for each validation step.\ndict_f_failed (dict): The fraction of test units that failed for each validation step.\ndict_warning (dict): The ‘warning’ level status for each validation step.\ndict_error (dict): The ‘error’ level status for each validation step.\ndict_critical (dict): The ‘critical’ level status for each validation step.\nall_passed (bool): Whether or not every validation step had no failing test units.\nhighest_severity (str): The highest severity level encountered during validation. This can be one of the following: \"warning\", \"error\", or \"critical\", \"some failing\", or \"all passed\".\ntbl_row_count (int): The number of rows in the target table.\ntbl_column_count (int): The number of columns in the target table.\ntbl_name (str): The name of the target table.\nvalidation_duration (float): The duration of the validation in seconds.\n\nNote that the summary dictionary is only available within the context of a final action. If called outside of a final action (i.e., when no final action is being executed), this function will return None."
  },
  {
    "objectID": "reference/get_validation_summary.html#examples",
    "href": "reference/get_validation_summary.html#examples",
    "title": "get_validation_summary",
    "section": "",
    "text": "Final actions are executed after the completion of all validation steps. They provide an opportunity to take appropriate actions based on the overall validation results. Here’s an example of a final action function (send_report()) that sends an alert when critical validation failures are detected:\nimport pointblank as pb\n\ndef send_report():\n    summary = pb.get_validation_summary()\n    if summary[\"highest_severity\"] == \"critical\":\n        # Send an alert email\n        send_alert_email(\n            subject=f\"CRITICAL validation failures in {summary['tbl_name']}\",\n            body=f\"{summary['n_critical_steps']} steps failed with critical severity.\"\n        )\n\nvalidation = (\n    pb.Validate(\n        data=my_data,\n        final_actions=pb.FinalActions(send_report)\n    )\n    .col_vals_gt(columns=\"revenue\", value=0)\n    .interrogate()\n)\nNote that send_alert_email() in the example above is a placeholder function that would be implemented by the user to send email alerts. This function is not provided by the Pointblank package.\nThe get_validation_summary() function can also be used to create custom reporting for validation results:\ndef log_validation_results():\n    summary = pb.get_validation_summary()\n\n    print(f\"Validation completed with status: {summary['highest_severity'].upper()}\")\n    print(f\"Steps: {summary['n_steps']} total\")\n    print(f\"  - {summary['n_passing_steps']} passing, {summary['n_failing_steps']} failing\")\n    print(\n        f\"  - Severity: {summary['n_warning_steps']} warnings, \"\n        f\"{summary['n_error_steps']} errors, \"\n        f\"{summary['n_critical_steps']} critical\"\n    )\n\n    if summary['highest_severity'] in [\"error\", \"critical\"]:\n        print(\"⚠️ Action required: Please review failing validation steps!\")\nFinal actions work well with both simple logging and more complex notification systems, allowing you to integrate validation results into your broader data quality workflows."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html",
    "href": "reference/Validate.col_vals_not_null.html",
    "title": "Validate.col_vals_not_null",
    "section": "",
    "text": "Validate.col_vals_not_null(\n    columns,\n    pre=None,\n    thresholds=None,\n    actions=None,\n    brief=None,\n    active=True,\n)\nValidate whether values in a column are not NULL.\nThe col_vals_not_null() validation method checks whether column values in a table are not NULL. This validation will operate over the number of test units that is equal to the number of rows in the table."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#parameters",
    "href": "reference/Validate.col_vals_not_null.html#parameters",
    "title": "Validate.col_vals_not_null",
    "section": "Parameters",
    "text": "Parameters\n\ncolumns : str | list[str] | Column | ColumnSelector | ColumnSelectorNarwhals\n\nA single column or a list of columns to validate. Can also use col() with column selectors to specify one or more columns. If multiple columns are supplied or resolved, there will be a separate validation step generated for each column.\n\npre : Callable | None = None\n\nA optional preprocessing function or lambda to apply to the data table during interrogation. This function should take a table as input and return a modified table. Have a look at the Preprocessing section for more information on how to use this argument.\n\nthresholds : int | float | bool | tuple | dict | Thresholds = None\n\nSet threshold failure levels for reporting and reacting to exceedences of the levels. The thresholds are set at the step level and will override any global thresholds set in Validate(thresholds=...). The default is None, which means that no thresholds will be set locally and global thresholds (if any) will take effect. Look at the Thresholds section for information on how to set threshold levels.\n\nactions : Actions | None = None\n\nOptional actions to take when the validation step(s) meets or exceeds any set threshold levels. If provided, the Actions class should be used to define the actions.\n\nbrief : str | bool | None = None\n\nAn optional brief description of the validation step that will be displayed in the reporting table. You can use the templating elements like \"{step}\" to insert the step number, or \"{auto}\" to include an automatically generated brief. If True the entire brief will be automatically generated. If None (the default) then there won’t be a brief.\n\nactive : bool = True\n\nA boolean value indicating whether the validation step should be active. Using False will make the validation step inactive (still reporting its presence and keeping indexes for the steps unchanged)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#returns",
    "href": "reference/Validate.col_vals_not_null.html#returns",
    "title": "Validate.col_vals_not_null",
    "section": "Returns",
    "text": "Returns\n\n : Validate\n\nThe Validate object with the added validation step."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#preprocessing",
    "href": "reference/Validate.col_vals_not_null.html#preprocessing",
    "title": "Validate.col_vals_not_null",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe pre= argument allows for a preprocessing function or lambda to be applied to the data table during interrogation. This function should take a table as input and return a modified table. This is useful for performing any necessary transformations or filtering on the data before the validation step is applied.\nThe preprocessing function can be any callable that takes a table as input and returns a modified table. For example, you could use a lambda function to filter the table based on certain criteria or to apply a transformation to the data. Note that you can refer to a column via columns= that is expected to be present in the transformed table, but may not exist in the table before preprocessing. Regarding the lifetime of the transformed table, it only exists during the validation step and is not stored in the Validate object or used in subsequent validation steps."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#thresholds",
    "href": "reference/Validate.col_vals_not_null.html#thresholds",
    "title": "Validate.col_vals_not_null",
    "section": "Thresholds",
    "text": "Thresholds\nThe thresholds= parameter is used to set the failure-condition levels for the validation step. If they are set here at the step level, these thresholds will override any thresholds set at the global level in Validate(thresholds=...).\nThere are three threshold levels: ‘warning’, ‘error’, and ‘critical’. The threshold values can either be set as a proportion failing of all test units (a value between 0 to 1), or, the absolute number of failing test units (as integer that’s 1 or greater).\nThresholds can be defined using one of these input schemes:\n\nuse the Thresholds class (the most direct way to create thresholds)\nprovide a tuple of 1-3 values, where position 0 is the ‘warning’ level, position 1 is the ‘error’ level, and position 2 is the ‘critical’ level\ncreate a dictionary of 1-3 value entries; the valid keys: are ‘warning’, ‘error’, and ‘critical’\na single integer/float value denoting absolute number or fraction of failing test units for the ‘warning’ level only\n\nIf the number of failing test units exceeds set thresholds, the validation step will be marked as ‘warning’, ‘error’, or ‘critical’. All of the threshold levels don’t need to be set, you’re free to set any combination of them.\nAside from reporting failure conditions, thresholds can be used to determine the actions to take for each level of failure (using the actions= parameter)."
  },
  {
    "objectID": "reference/Validate.col_vals_not_null.html#examples",
    "href": "reference/Validate.col_vals_not_null.html#examples",
    "title": "Validate.col_vals_not_null",
    "section": "Examples",
    "text": "Examples\nFor the examples here, we’ll use a simple Polars DataFrame with two numeric columns (a and b). The table is shown below:\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [4, 7, 2, 8],\n        \"b\": [5, None, 1, None],\n    }\n)\n\npb.preview(tbl)\n\n\n\n\n\n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n\n\n\n  \n    1\n    4\n    5\n  \n  \n    2\n    7\n    None\n  \n  \n    3\n    2\n    1\n  \n  \n    4\n    8\n    None\n  \n\n\n\n\n\n\n        \n\n\nLet’s validate that none of the values in column a are Null values. We’ll determine if this validation had any failing test units (there are four test units, one for each row).\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_null(columns=\"a\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    a\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    41.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nPrinting the validation object shows the validation table in an HTML viewing environment. The validation table shows the single entry that corresponds to the validation step created by using col_vals_not_null(). All test units passed, and there are no failing test units.\nNow, let’s use that same set of values for a validation on column b.\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_not_null(columns=\"b\")\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    b\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    4\n    20.50\n    20.50\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe validation table reports two failing test units. The specific failing cases are for the two Null values in column b."
  },
  {
    "objectID": "reference/first_n.html",
    "href": "reference/first_n.html",
    "title": "first_n",
    "section": "",
    "text": "first_n(n, offset=0)\nSelect the first n columns in the column list.\nMany validation methods have a columns= argument that can be used to specify the columns for validation (e.g., col_vals_gt(), col_vals_regex(), etc.). The first_n() selector function can be used to select n columns positioned at the start of the column list. So if the set of table columns consists of\n[rev_01, rev_02, profit_01, profit_02, age]\nand you want to validate the first two columns, you can use columns=first_n(2). This will select the rev_01 and rev_02 columns and a validation step will be created for each.\nThe offset= parameter can be used to skip a certain number of columns from the start of the column list. So if you want to select the third and fourth columns, you can use columns=first_n(2, offset=2)."
  },
  {
    "objectID": "reference/first_n.html#parameters",
    "href": "reference/first_n.html#parameters",
    "title": "first_n",
    "section": "Parameters",
    "text": "Parameters\n\nn : int\n\nThe number of columns to select from the start of the column list. Should be a positive integer value. If n is greater than the number of columns in the table, all columns will be selected.\n\noffset : int = 0\n\nThe offset from the start of the column list. The default is 0. If offset is greater than the number of columns in the table, no columns will be selected."
  },
  {
    "objectID": "reference/first_n.html#returns",
    "href": "reference/first_n.html#returns",
    "title": "first_n",
    "section": "Returns",
    "text": "Returns\n\n : FirstN\n\nA FirstN object, which can be used to select the first n columns."
  },
  {
    "objectID": "reference/first_n.html#relevant-validation-methods-where-first_n-can-be-used",
    "href": "reference/first_n.html#relevant-validation-methods-where-first_n-can-be-used",
    "title": "first_n",
    "section": "Relevant Validation Methods where first_n() can be Used",
    "text": "Relevant Validation Methods where first_n() can be Used\nThis selector function can be used in the columns= argument of the following validation methods:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\ncol_exists()\n\nThe first_n() selector function doesn’t need to be used in isolation. Read the next section for information on how to compose it with other column selectors for more refined ways to select columns."
  },
  {
    "objectID": "reference/first_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "href": "reference/first_n.html#additional-flexibilty-through-composition-with-other-column-selectors",
    "title": "first_n",
    "section": "Additional Flexibilty through Composition with Other Column Selectors",
    "text": "Additional Flexibilty through Composition with Other Column Selectors\nThe first_n() function can be composed with other column selectors to create fine-grained column selections. For example, to select all column names starting with “rev” along with the first two columns, you can use the first_n() and starts_with() functions together. The only condition is that the expressions are wrapped in the col() function, like this:\ncol(first_n(2) | starts_with(\"rev\"))\nThere are four operators that can be used to compose column selectors:\n\n& (and)\n| (or)\n- (difference)\n~ (not)\n\nThe & operator is used to select columns that satisfy both conditions. The | operator is used to select columns that satisfy either condition. The - operator is used to select columns that satisfy the first condition but not the second. The ~ operator is used to select columns that don’t satisfy the condition. As many selector functions can be used as needed and the operators can be combined to create complex column selection criteria (parentheses can be used to group conditions and control the order of evaluation)."
  },
  {
    "objectID": "reference/first_n.html#examples",
    "href": "reference/first_n.html#examples",
    "title": "first_n",
    "section": "Examples",
    "text": "Examples\nSuppose we have a table with columns paid_2021, paid_2022, paid_2023, paid_2024, and name and we’d like to validate that the values in the first four columns are greater than 10. We can use the first_n() column selector function to specify that the first four columns in the table are the columns to validate.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.first_n(4), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2023\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    4\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get four validation steps. The values in all those columns were all greater than 10.\nWe can also use the first_n() function in combination with other column selectors (within col()) to create more complex column selection criteria (i.e., to select columns that satisfy multiple conditions). For example, to select the first four columns but also omit those columns that end with \"2023\", we can use the - operator to combine column selectors.\n\ntbl = pl.DataFrame(\n    {\n        \"paid_2021\": [17.94, 16.55, 17.85],\n        \"paid_2022\": [18.62, 16.95, 18.25],\n        \"paid_2023\": [19.29, 17.75, 18.35],\n        \"paid_2024\": [20.73, 18.35, 20.10],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=pb.col(pb.first_n(4) - pb.ends_with(\"2023\")), value=10)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2021\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    2\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2022\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n  \n    #4CA64C\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    paid_2024\n    10\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    3\n    31.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nFrom the results of the validation table we get three validation steps, one for paid_2021, paid_2022, and paid_2024."
  },
  {
    "objectID": "reference/missing_vals_tbl.html",
    "href": "reference/missing_vals_tbl.html",
    "title": "missing_vals_tbl",
    "section": "",
    "text": "missing_vals_tbl(data)\nDisplay a table that shows the missing values in the input table.\nThe missing_vals_tbl() function generates a table that shows the missing values in the input table. The table is displayed using the Great Tables API, which allows for further customization of the table’s appearance if so desired."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#parameters",
    "href": "reference/missing_vals_tbl.html#parameters",
    "title": "missing_vals_tbl",
    "section": "Parameters",
    "text": "Parameters\n\ndata : FrameT | Any\n\nThe table for which to display the missing values. This could be a DataFrame object or an Ibis table object. Read the Supported Input Table Types section for details on the supported table types."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#returns",
    "href": "reference/missing_vals_tbl.html#returns",
    "title": "missing_vals_tbl",
    "section": "Returns",
    "text": "Returns\n\n : GT\n\nA GT object that displays the table of missing values in the input table."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#supported-input-table-types",
    "href": "reference/missing_vals_tbl.html#supported-input-table-types",
    "title": "missing_vals_tbl",
    "section": "Supported Input Table Types",
    "text": "Supported Input Table Types\nThe data= parameter can be given any of the following table types:\n\nPolars DataFrame (\"polars\")\nPandas DataFrame (\"pandas\")\nDuckDB table (\"duckdb\")*\nMySQL table (\"mysql\")*\nPostgreSQL table (\"postgresql\")*\nSQLite table (\"sqlite\")*\nParquet table (\"parquet\")*\n\nThe table types marked with an asterisk need to be prepared as Ibis tables (with type of ibis.expr.types.relations.Table). Furthermore, using missing_vals_tbl() with these types of tables requires the Ibis library (v9.5.0 or above) to be installed. If the input table is a Polars or Pandas DataFrame, the availability of Ibis is not needed."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#the-missing-values-table",
    "href": "reference/missing_vals_tbl.html#the-missing-values-table",
    "title": "missing_vals_tbl",
    "section": "The Missing Values Table",
    "text": "The Missing Values Table\nThe missing values table shows the proportion of missing values in each column of the input table. The table is divided into sectors, with each sector representing a range of rows in the table. The proportion of missing values in each sector is calculated for each column. The table is displayed using the Great Tables API, which allows for further customization of the table’s appearance.\nTo ensure that the table can scale to tables with many columns, each row in the reporting table represents a column in the input table. There are 10 sectors shown in the table, where the first sector represents the first 10% of the rows, the second sector represents the next 10% of the rows, and so on. Any sectors that are light blue indicate that there are no missing values in that sector. If there are missing values, the proportion of missing values is shown by a gray color (light gray for low proportions, dark gray to black for very high proportions)."
  },
  {
    "objectID": "reference/missing_vals_tbl.html#examples",
    "href": "reference/missing_vals_tbl.html#examples",
    "title": "missing_vals_tbl",
    "section": "Examples",
    "text": "Examples\nThe missing_vals_tbl() function is useful for quickly identifying columns with missing values in a table. Here’s an example using the nycflights dataset (loaded as a Polars DataFrame using the load_dataset() function):\n\nimport pointblank as pb\n\nnycflights = pb.load_dataset(\"nycflights\", tbl_type=\"polars\")\n\npb.missing_vals_tbl(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    Missing Values   46,595 in total\n  \n  \n    PolarsRows336,776Columns18\n  \n\n  Column\n  \n    Row Sector\n  \n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  10\n\n\n\n  \n    year\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    month\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    day\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_dep_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dep_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    sched_arr_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    arr_delay\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    carrier\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    flight\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    tailnum\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    origin\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    dest\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    air_time\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    distance\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    hour\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    minute\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n\n  \n  \n  \n    NO MISSING VALUES     PROPORTION MISSING:  0%100%ROW SECTORS1 – 3367733678 – 6735467355 – 101031101032 – 134708134709 – 168385168386 – 202062202063 – 235739235740 – 269416269417 – 303093303094 – 336776\n  \n\n\n\n\n\n\n        \n\n\nThe table shows the proportion of missing values in each column of the nycflights dataset. The table is divided into sectors, with each sector representing a range of rows in the table (with around 34,000 rows per sector). The proportion of missing values in each sector is calculated for each column. The various shades of gray indicate the proportion of missing values in each sector. Many columns have no missing values at all, and those sectors are colored light blue."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html",
    "href": "reference/Validate.get_data_extracts.html",
    "title": "Validate.get_data_extracts",
    "section": "",
    "text": "Validate.get_data_extracts(i=None, frame=False)\nGet the rows that failed for each validation step.\nAfter the interrogate() method has been called, the get_data_extracts() method can be used to extract the rows that failed in each row-based validation step (e.g., col_vals_gt(), etc.). The method returns a dictionary of tables containing the rows that failed in every row-based validation function. If frame=True and i= is a scalar, the value is conveniently returned as a table (forgoing the dictionary structure)."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#parameters",
    "href": "reference/Validate.get_data_extracts.html#parameters",
    "title": "Validate.get_data_extracts",
    "section": "Parameters",
    "text": "Parameters\n\ni : int | list[int] | None = None\n\nThe validation step number(s) from which the failed rows are obtained. Can be provided as a list of integers or a single integer. If None, all steps are included.\n\nframe : bool = False\n\nIf True and i= is a scalar, return the value as a DataFrame instead of a dictionary."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#returns",
    "href": "reference/Validate.get_data_extracts.html#returns",
    "title": "Validate.get_data_extracts",
    "section": "Returns",
    "text": "Returns\n\n : dict[int, FrameT | None] | FrameT | None\n\nA dictionary of tables containing the rows that failed in every row-based validation step or a DataFrame."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#validation-methods-that-are-row-based",
    "href": "reference/Validate.get_data_extracts.html#validation-methods-that-are-row-based",
    "title": "Validate.get_data_extracts",
    "section": "Validation Methods that are Row-Based",
    "text": "Validation Methods that are Row-Based\nThe following validation methods are row-based and will have rows extracted when there are failing test units.\n\ncol_vals_gt()\ncol_vals_ge()\ncol_vals_lt()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\nrows_distinct()\n\nAn extracted row means that a test unit failed for that row in the validation step. The extracted rows are a subset of the original table and are useful for further analysis or for understanding the nature of the failing test units."
  },
  {
    "objectID": "reference/Validate.get_data_extracts.html#examples",
    "href": "reference/Validate.get_data_extracts.html#examples",
    "title": "Validate.get_data_extracts",
    "section": "Examples",
    "text": "Examples\nLet’s perform a series of validation steps on a Polars DataFrame. We’ll use the col_vals_gt() in the first step, col_vals_lt() in the second step, and col_vals_ge() in the third step. The interrogate() method executes the validation; then, we can extract the rows that failed for each validation step.\n\nimport pointblank as pb\nimport polars as pl\n\ntbl = pl.DataFrame(\n    {\n        \"a\": [5, 6, 5, 3, 6, 1],\n        \"b\": [1, 2, 1, 5, 2, 6],\n        \"c\": [3, 7, 2, 6, 3, 1],\n    }\n)\n\nvalidation = (\n    pb.Validate(data=tbl)\n    .col_vals_gt(columns=\"a\", value=4)\n    .col_vals_lt(columns=\"c\", value=5)\n    .col_vals_ge(columns=\"b\", value=1)\n    .interrogate()\n)\n\nvalidation.get_data_extracts()\n\n{1: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n │ 6         ┆ 1   ┆ 6   ┆ 1   │\n └───────────┴─────┴─────┴─────┘,\n 2: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 2         ┆ 6   ┆ 2   ┆ 7   │\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n └───────────┴─────┴─────┴─────┘,\n 3: shape: (0, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n └───────────┴─────┴─────┴─────┘}\n\n\nThe get_data_extracts() method returns a dictionary of tables, where each table contains a subset of rows from the table. These are the rows that failed for each validation step.\nIn the first step, thecol_vals_gt() method was used to check if the values in column a were greater than 4. The extracted table shows the rows where this condition was not met; look at the a column: all values are less than 4.\nIn the second step, the col_vals_lt() method was used to check if the values in column c were less than 5. In the extracted two-row table, we see that the values in column c are greater than 5.\nThe third step (col_vals_ge()) checked if the values in column b were greater than or equal to 1. There were no failing test units, so the extracted table is empty (i.e., has columns but no rows).\nThe i= argument can be used to narrow down the extraction to one or more steps. For example, to extract the rows that failed in the first step only:\n\nvalidation.get_data_extracts(i=1)\n\n{1: shape: (2, 4)\n ┌───────────┬─────┬─────┬─────┐\n │ _row_num_ ┆ a   ┆ b   ┆ c   │\n │ ---       ┆ --- ┆ --- ┆ --- │\n │ u32       ┆ i64 ┆ i64 ┆ i64 │\n ╞═══════════╪═════╪═════╪═════╡\n │ 4         ┆ 3   ┆ 5   ┆ 6   │\n │ 6         ┆ 1   ┆ 6   ┆ 1   │\n └───────────┴─────┴─────┴─────┘}\n\n\nNote that the first validation step is indexed at 1 (not 0). This 1-based indexing is in place here to match the step numbers reported in the validation table. What we get back is still a dictionary, but it only contains one table (the one for the first step).\nIf you want to get the extracted table as a DataFrame, set frame=True and provide a scalar value for i. For example, to get the extracted table for the second step as a DataFrame:\n\npb.preview(validation.get_data_extracts(i=2, frame=True))\n\n\n\n\n\n  \n  \n  \n  \n\n\n\n\n\n  \n  aInt64\n  bInt64\n  cInt64\n\n\n\n  \n    2\n    6\n    2\n    7\n  \n  \n    4\n    3\n    5\n    6\n  \n\n\n\n\n\n\n        \n\n\nThe extracted table is now a DataFrame, which can serve as a more convenient format for further analysis or visualization. We further used the preview() function to show the DataFrame in an HTML view."
  },
  {
    "objectID": "user-guide/types.html",
    "href": "user-guide/types.html",
    "title": "Validation Types",
    "section": "",
    "text": "The collection of validation methods in Pointblank allows you to express all sorts of checks on your DataFrames and database tables. We’ll use the small_table dataset for all of the examples shown here. Here’s a preview of it:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Validation Types"
    ]
  },
  {
    "objectID": "user-guide/types.html#more-information",
    "href": "user-guide/types.html#more-information",
    "title": "Validation Types",
    "section": "More Information",
    "text": "More Information\nThese are just a few examples of the many validation methods available in Pointblank. For more detailed information, check out the individual reference pages in the API Reference.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Validation Types"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html",
    "href": "user-guide/col-summary-tbl.html",
    "title": "Getting Column Summaries",
    "section": "",
    "text": "While previewing a table with preview() is undoubtedly a good thing to do, sometimes you need more. This is where summarizing a table comes in. When you view a summary of a table, the column-by-column info can quickly increase your understanding of a dataset. Plus, it allows you to quickly catch anomalies in your data (e.g., the maximum value of a column could be far outside the realm of possibility).\nPointblank provides a function to make it extremely easy to view column-level summaries in a single table. That function is called col_summary_tbl() and, just like preview() does, it supports the use of any table that Pointblank can use for validation. And no matter what the input data is, the resultant reporting table is consistent in its design and construction.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Getting Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#trying-out-col_summary_tbl",
    "href": "user-guide/col-summary-tbl.html#trying-out-col_summary_tbl",
    "title": "Getting Column Summaries",
    "section": "Trying out col_summary_tbl()",
    "text": "Trying out col_summary_tbl()\nThe function only requires a table. Let’s use the small_table dataset (a very simple table) to start us off:\n\nimport pointblank as pb\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\")\npb.col_summary_tbl(small_table)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows13Columns8\n  \n\n  \n  \n  Column\n  NA\n  UQ\n  Mean\n  SD\n  Min\n  P5\n  Q1\n  Med\n  Q3\n  P95\n  Max\n  IQR\n\n\n\n  \n    1\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    date_timeDatetime(time_unit='us', time_zone=None)\n    0 0.00\n    12 0.92\n    —\n    —\n     2016-01-04 00:32:00 – 2016-01-30 11:23:00\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    2\n    \n    date\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dateDate\n    0 0.00\n    11 0.85\n    —\n    —\n     2016-01-04 – 2016-01-30\n    \n    \n    \n    \n    \n    \n    —\n  \n  \n    3\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    aInt64\n    0 0.00\n    7 0.54\n    3.77\n    2.09\n    1.00\n    1.60\n    2.00\n    3.00\n    4.00\n    7.40\n    8.00\n    2.00\n  \n  \n    4\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    bString\n    0 0.00\n    12 0.92\n    9.00SL\n    0.00SL\n    9SL\n    —\n    —\n    9SL\n    —\n    —\n    9SL\n    —\n  \n  \n    5\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    cInt64\n    2 0.15\n    6 0.46\n    5.73\n    2.72\n    2.00\n    2.50\n    3.00\n    7.00\n    8.00\n    9.00\n    9.00\n    5.00\n  \n  \n    6\n    \n    numeric\n    \n        \n            \n            \n                \n            \n        \n    \n\n    dFloat64\n    0 0.00\n    12 0.92\n    2305\n    2631\n    108\n    214\n    838\n    1036\n    3291\n    6335\n    10000\n    2453\n  \n  \n    7\n    \n    boolean\n    \n        \n            \n            \n                \n            \n            \n                \n            \n            \n        \n    \n\n    eBoolean\n    0 0.00\n    T 0.61F 0.39\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n    —\n  \n  \n    8\n    \n    string\n    \n        \n            \n            \n                \n            \n        \n    \n\n    fString\n    0 0.00\n    3 0.23\n    3.46SL\n    0.52SL\n    3SL\n    —\n    —\n    3SL\n    —\n    —\n    4SL\n    —\n  \n\n\n\n\n\n\n        \n\n\nThe header provides the type of table we’re looking at (POLARS, since this is a Polars DataFrame) and the table dimensions. The rest of the table focuses on the column-level summaries. As such, each row represents a summary of a column in the small_table dataset. There’s a lot of information in this summary table to digest. Some of it is intuitive since this sort of table summarization isn’t all that uncommon, but other aspects of it could also give some pause. So we’ll carefully wade through how to interpret this report.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Getting Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#data-categories-in-the-column-summary-table",
    "href": "user-guide/col-summary-tbl.html#data-categories-in-the-column-summary-table",
    "title": "Getting Column Summaries",
    "section": "Data Categories in the Column Summary Table",
    "text": "Data Categories in the Column Summary Table\nOn the left side of the table are icons of different colors. These represent categories that the columns fall into. There are only five categories and columns can only be of one type. The categories (and their letter marks) are:\n\nN: numeric\nS: string-based\nD: date/datetime\nT/F: boolean\nO: object\n\nThe numeric category (N) takes data types such as floats and integers. The S category is for string-based columns. Date or datetime values are lumped into the D category. Boolean columns (T/F) have their own category and are not considered numeric (e.g., 0/1). The O category is a catchall for all other types of columns. Given the disparity of these categories and that we want them in the same table, some statistical measures will be sensible for certain column categories but not for others. Given that, we’ll explain how each category is represented in the column summary table.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Getting Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#numeric-data",
    "href": "user-guide/col-summary-tbl.html#numeric-data",
    "title": "Getting Column Summaries",
    "section": "Numeric Data",
    "text": "Numeric Data\nThree columns in small_table are numeric: a (Int64), c (Int64), and d (Float64). The common measures of the missing count/proportion (NA) and the unique value count/proportion (UQ) are provided for the numeric data type. For these two measures, the top number is the absolute count of missing values and the count of unique values. The bottom number is a proportion of the absolute count divided by the row count; this makes each proportion a value between 0 and 1 (bounds included).\nThe next two columns represent the mean (Mean) and the standard deviation (SD). The minumum (Min), maximum, (Max) and a set of quantiles occupy the next few columns (includes P5, Q1, Med for median, Q3, and P95). Finally, the interquartile range (IQR: Q3 - Q1) is the last measure provided.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Getting Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#string-data",
    "href": "user-guide/col-summary-tbl.html#string-data",
    "title": "Getting Column Summaries",
    "section": "String Data",
    "text": "String Data\nString data is present in small_table, being in columns b and f. The missing value (NA) and uniqueness (UQ) measures are accounted for here. The statistical measures are all based on string lengths, so what happens is that all strings in a column are converted to those numeric values and a subset of stats values is presented. To avoid some understandable confusion when reading the table, the stats values in each of the cells with values are annotated with the text \"SL\". It makes less sense to provide a full suite of quantile values so only the minimum (Min), median (Med), and maximum (Max) are provided.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Getting Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/col-summary-tbl.html#datedatetime-data-and-boolean-data",
    "href": "user-guide/col-summary-tbl.html#datedatetime-data-and-boolean-data",
    "title": "Getting Column Summaries",
    "section": "Date/Datetime Data and Boolean Data",
    "text": "Date/Datetime Data and Boolean Data\nWe see that in the first two rows of our summary table there are summaries of the date_time and date columns. The summaries we provide for a date/datetime category (notice the green D to the left of the column names) are:\n\nthe missing count/proportion (NA)\nthe unique value count/proportion (UQ)\nthe minimum and maximum dates/datetimes\n\nOne column, e, is of the Boolean type. Because columns of this type could only have True, False, or missing values, we provide summary data for missingness (under NA) and proportions of True and False values (under UQ).",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Getting Column Summaries"
    ]
  },
  {
    "objectID": "user-guide/extracts.html",
    "href": "user-guide/extracts.html",
    "title": "Data Extracts",
    "section": "",
    "text": "Data extracts consist of target table rows containing at least one cell that was found to be a failing test unit. Many of the validation methods check values down a column according to some rule (e.g., values are not null/None, values are greater than zero, etc.). So if any of those test units (which are really cells) failed during a validation step, the row is marked as failing for the purposes of data extract collection. This article will:\nData extracts can be useful after interrogation since they reveal which rows resulted in failures during interrogation. It is hoped that having quick access to entire rows of data with failing elements can be useful in uncovering the root causes of data quality issues.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#the-validation-methods-that-work-with-data-extracts",
    "href": "user-guide/extracts.html#the-validation-methods-that-work-with-data-extracts",
    "title": "Data Extracts",
    "section": "The Validation Methods that Work with Data Extracts",
    "text": "The Validation Methods that Work with Data Extracts\nThe following validation methods are row-based and will have rows extracted when there are failing test units:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\n\nAn extracted row means that a test unit failed for that row in the validation step. The extracted rows are a subset of the original table and are useful for further analysis or understanding the nature of the failing test units.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#data-extracts-as-csv-data-in-the-validation-report",
    "href": "user-guide/extracts.html#data-extracts-as-csv-data-in-the-validation-report",
    "title": "Data Extracts",
    "section": "Data Extracts as CSV Data in the Validation Report",
    "text": "Data Extracts as CSV Data in the Validation Report\nData extracts are embedded within validation report tables. Let’s look at an example, using the small_table dataset, where data extracts are collected in a single validation step due to failing test units:\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\", tbl_type=\"polars\"))\n    .col_vals_lt(columns=\"d\", value=3000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_lt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_lt()\n        \n        \n        \n    d\n    3000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThe single validation step checks whether values in d are less than 3000. Within that column values range from 108.34 to 9999.99, so it makes sense that we can see 4 failing test units in the FAIL column.\nIf you look at the far right of the validation report you’ll find there’s a CSV button. Pressing it initiates the download of a CSV, and that CSV contains the data extract for this validation step. The CSV button only appears when:\n\nthere is a non-zero number of failing test units\nthe validation step is based on the use of a row-based validation method (the methods outlined above)\n\nAccess to CSV data for the row-based errors is useful when the validation report is shared with other data quality stakeholders, since it is easily accessible and doesn’t require futher use of Pointblank.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#using-the-get_data_extracts-method-to-collect",
    "href": "user-guide/extracts.html#using-the-get_data_extracts-method-to-collect",
    "title": "Data Extracts",
    "section": "Using the get_data_extracts() Method to Collect",
    "text": "Using the get_data_extracts() Method to Collect\nAside from the low-tech CSV buttons in validation report tables, we can more directly pull out the data extracts from the validation object created above. We do that with the get_data_extracts() method, supplying the step number (1) to the i= parameter:\n\nextract_1 = validation.get_data_extracts(i=1, frame=True)\n\nextract_1\n\n\nshape: (4, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr12016-01-04 11:00:002016-01-042\"1-bcd-345\"33423.29true\"high\"22016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"42016-01-06 17:23:002016-01-062\"5-jdo-903\"null3892.4false\"mid\"62016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"\n\n\nThe extracted table is of the same type (a Polars DataFrame) as the target table. We used load_dataset() with the tbl_type=\"polars\" option to fetch the dataset in that form.\nNotice that the frame=True option was used above. What this does is return the table itself as normally the return type is a dictionary. This only works if what’s provided to i= is a scalar integer (which is the case here).\nAlso notice that within the DataFrame returned, we get all the columns of the original dataset (i.e., not just the column being checked in the validation step) plus an additional column: _row_num_. That column provides the 1-indexed row numbers from the original dataset. The combination of rows in their entirety plus row numbers is to provide more context on where data failures occurred.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/extracts.html#viewing-data-extracts-with-preview",
    "href": "user-guide/extracts.html#viewing-data-extracts-with-preview",
    "title": "Data Extracts",
    "section": "Viewing Data Extracts with preview()",
    "text": "Viewing Data Extracts with preview()\nTo get a consistent HTML representation of any data extract (regardless of the table type), we can use the preview() function:\n\npb.preview(data=extract_1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows4Columns9\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n\n\n\n\n\n\n        \n\n\nThe view is optimized for readability, with column names and data types displayed in a compact format. Notice that the _row_num_ column is now part of the table stub and doesn’t steal focus from the table’s original columns.\nThe preview() function is designed to provide the head and tail (5 rows each) of the table so very large extracts won’t overflow the display.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Data Extracts"
    ]
  },
  {
    "objectID": "user-guide/across.html",
    "href": "user-guide/across.html",
    "title": "Comparing Values Across Columns",
    "section": "",
    "text": "The previous section demonstrated the use of the many validation methods. In all of the examples where column values were compared against another value, that value was always one that was fixed. These sorts of comparisons (e.g., are values in column x greater than or equal to zero?) are useful but sometimes you need more.\nFor a more dynamic type of comparison check, you can also compare a column’s values against those of another column. The name of the comparison column is provided as the value= parameter (or left= and right= as needed in the col_vals_between() and col_vals_outside() validation methods).\nLet’s see how this type of validation is made possible through a few examples using the small_table dataset shown below:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Comparing Values Across Columns"
    ]
  },
  {
    "objectID": "user-guide/across.html#using-col-to-specify-a-value-column",
    "href": "user-guide/across.html#using-col-to-specify-a-value-column",
    "title": "Comparing Values Across Columns",
    "section": "Using col() to Specify a Value Column",
    "text": "Using col() to Specify a Value Column\nWe know to use the columns= to supply the target column. Just to recap, if we wanted to check that every value in column a is greater than 2 then the validation step is written something like this:\n...\n.col_vals_gt(columns=\"a\", value=2, ...)\n...\nWhat if you have two numerical columns and have good reason to compare values across those columns? This could be a check that expects every value in a is greater than every value in x (the comparison column). That would take the form:\n...\n.col_vals_gt(columns=\"a\", value=pb.col(\"x\"), ...)\n...\nUsing the col() helper function here in value= is key. It lets Pointblank know that you’re not using a literal, fixed value for the comparison (you’re specifying a column). So long as you do this, the validation will involve checking that every value a is greater than every adjacent value in x. Here’s an example of this:\n\nimport pointblank as pb\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_gt(columns=\"d\", value=pb.col(\"c\"))\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nNotice that in the COLUMNS and VALUES columns of the validation report, the two column names are shown: d and c. A thing that could be surprising is that there are two failing test units, even though values in d are consistently larger than values in column c. The reason is that there are missing values (i.e., None values) in column c and any missing value in a comparison check will result in a failing test unit.\nWhen doing a comparison against a fixed value, we only had to worry about missing values in the target column. When comparing across columns, there is potential for missing values in both columns and that could result in correspondingly more failing test units. The corrective here is to use na_pass=True. If you feel missing values (in either column) should be disregarded, this setting is a reasonable choice (and you could always use col_vals_not_null() to perform missing value checks on these columns anyway).\nLet’s take a quick look at the results when na_pass=True is used:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_gt(columns=\"d\", value=pb.col(\"c\"), na_pass=True)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    d\n    c\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nWith that change, every test unit in that single step passes validation.\nThe validation methods that accept a col() expression in their value= parameter include:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Comparing Values Across Columns"
    ]
  },
  {
    "objectID": "user-guide/across.html#using-col-in-range-checks",
    "href": "user-guide/across.html#using-col-in-range-checks",
    "title": "Comparing Values Across Columns",
    "section": "Using col() in Range Checks",
    "text": "Using col() in Range Checks\nTwo validation methods deal with checking values within and outside a range:\n\ncol_vals_between()\ncol_vals_outside()\n\nThese validation methods both have left= and right= arguments. You can use a mix of literal values and col()-based expressions for these parameters. Here’s an example where we check values in d to be in the range of lower-bound values in column c and a fixed upper-bound value of 10000:\n\n(\n    pb.Validate(data=pb.load_dataset(\"small_table\"))\n    .col_vals_between(columns=\"d\", left=pb.col(\"c\"), right=10_000, na_pass=True)\n    .interrogate()\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C\n    1\n    \n        \n            \n\n    col_vals_between\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_between()\n        \n        \n        \n    d\n    [c, 10000]\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    131.00\n    00.00\n    —\n    —\n    —\n    —\n  \n\n\n\n\n\n\n        \n\n\nObserve that the range reported in the VALUES column is [c, 10000]. This is our assurance that the left bound is dependent on values in column c and that the right bound is fixed to a value of 10000. All test units passed here as we were careful about missing values (using na_pass=True) as in the previous example.",
    "crumbs": [
      "User Guide",
      "Defining Validation Steps",
      "Comparing Values Across Columns"
    ]
  },
  {
    "objectID": "user-guide/sundering.html",
    "href": "user-guide/sundering.html",
    "title": "Sundering Data",
    "section": "",
    "text": "Sundering data? First off, let’s get the correct meaning across here. Sundering is really just splitting, dividing, cutting into two pieces. And it’s a useful thing we can do in Pointblank to any data that we are validating. When you interrogate the data, you learn about which rows have test failures within them. With more validation steps, we get an even better picture of this simply by virtue of more testing.\nLet’s use the small_table in our examples to show just how sundering is done. Here’s that table:\nPolarsRows13Columns8\n  \n\n  \n  date_timeDatetime\n  dateDate\n  aInt64\n  bString\n  cInt64\n  dFloat64\n  eBoolean\n  fString\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04\n    2\n    1-bcd-345\n    3\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04\n    3\n    5-egh-163\n    8\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05\n    6\n    8-kdg-938\n    3\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06\n    2\n    5-jdo-903\n    None\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09\n    8\n    3-ldm-038\n    7\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11\n    4\n    2-dhe-923\n    4\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15\n    7\n    1-knw-093\n    3\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17\n    4\n    5-boe-639\n    2\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20\n    3\n    5-bce-642\n    9\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26\n    4\n    2-dmx-010\n    7\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28\n    2\n    7-dmx-010\n    8\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30\n    1\n    3-dka-303\n    None\n    2230.09\n    True\n    high",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#a-simple-example-where-data-is-torn-asunder",
    "href": "user-guide/sundering.html#a-simple-example-where-data-is-torn-asunder",
    "title": "Sundering Data",
    "section": "A Simple Example Where Data is Torn Asunder",
    "text": "A Simple Example Where Data is Torn Asunder\nWe’ll begin with a very simple validation plan, having only a single step. There will be failing test units here.\n\nimport pointblank as pb\n\nvalidation = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_ge(columns=\"d\", value=1000)\n    .interrogate()\n)\n\nvalidation\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nWe see six failing test units in FAIL column of the above validation report table. There is a data extract (collection of failing rows) available. Let’s use the get_data_extracts() method to have a look at it.\n\nvalidation.get_data_extracts(i=1, frame=True)\n\n\nshape: (6, 9)_row_num_date_timedateabcdefu32datetime[μs]datei64stri64f64boolstr52016-01-09 12:36:002016-01-098\"3-ldm-038\"7283.94true\"low\"72016-01-15 18:46:002016-01-157\"1-knw-093\"3843.34true\"high\"92016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"102016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"112016-01-26 20:07:002016-01-264\"2-dmx-010\"7833.98true\"low\"122016-01-28 02:51:002016-01-282\"7-dmx-010\"8108.34false\"low\"\n\n\nThis is six rows of data that had failing test units in column d. Indeed we can see that all values in that column are less than 1000 (and we asserted that values should be greater than or equal to 1000). This is the ‘bad’ data, if you will. Using the get_sundered_data() method, we get the ‘good’ part:\n\nvalidation.get_sundered_data()\n\n\nshape: (7, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-04 11:00:002016-01-042\"1-bcd-345\"33423.29true\"high\"2016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"2016-01-05 13:32:002016-01-056\"8-kdg-938\"32343.23true\"high\"2016-01-06 17:23:002016-01-062\"5-jdo-903\"null3892.4false\"mid\"2016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"2016-01-17 11:27:002016-01-174\"5-boe-639\"21035.64false\"low\"2016-01-30 11:23:002016-01-301\"3-dka-303\"null2230.09true\"high\"\n\n\nThis is a Polars DataFrame of seven rows. All values in d were passing test units (i.e., fulfilled the expectation outlined in the validation step) and, in many ways, this is like a ‘good extract’.\nYou can always collect the failing rows with get_sundered_data() by using the type=\"fail\" option. Trying that here\n\nvalidation.get_sundered_data(type=\"fail\")\n\n\nshape: (6, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-09 12:36:002016-01-098\"3-ldm-038\"7283.94true\"low\"2016-01-15 18:46:002016-01-157\"1-knw-093\"3843.34true\"high\"2016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"2016-01-20 04:30:002016-01-203\"5-bce-642\"9837.93false\"high\"2016-01-26 20:07:002016-01-264\"2-dmx-010\"7833.98true\"low\"2016-01-28 02:51:002016-01-282\"7-dmx-010\"8108.34false\"low\"\n\n\ngives us the same rows as in the DataFrame obtained from using validation.get_data_extracts(i=1, frame=True). Two important things to know about get_sundered_data() is that the table rows returned from type=pass (the default) and type=fail are:\n\nthe sum of rows across these returned tables will be equal to that of the original table\nthe rows in each split table are mutually exclusive (i.e., you won’t find the same row in both)\n\nYou can think of sundered data as a filtered version of the original dataset based on validation results. While the simple example illustrates how this process works on a basic level, the value of the method is better seen in a slightly more complex example.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#using-get_sundered_data-with-a-more-comprehensive-validation",
    "href": "user-guide/sundering.html#using-get_sundered_data-with-a-more-comprehensive-validation",
    "title": "Sundering Data",
    "section": "Using get_sundered_data() with a More Comprehensive Validation",
    "text": "Using get_sundered_data() with a More Comprehensive Validation\nThe previous example used exactly one valiation step. You’re likely to use more than that in standard practice so let’s see how get_sundered_data() works in those common situations. Here’s a validation with three steps:\n\nvalidation_2 = (\n    pb.Validate(data=pb.load_dataset(dataset=\"small_table\"))\n    .col_vals_ge(columns=\"d\", value=1000)\n    .col_vals_not_null(columns=\"c\")\n    .col_vals_gt(columns=\"a\", value=2)\n    .interrogate()\n)\n\nvalidation_2\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  STEP\n  COLUMNS\n  VALUES\n  TBL\n  EVAL\n  UNITS\n  PASS\n  FAIL\n  W\n  E\n  C\n  EXT\n\n\n\n  \n    #4CA64C66\n    1\n    \n        \n            \n\n    col_vals_gte\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_ge()\n        \n        \n        \n    d\n    1000\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    70.54\n    60.46\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    2\n    \n        \n            \n\n    col_vals_not_null\n    \n        \n            \n            \n            \n            \n        \n    \n\n        \n        \n            col_vals_not_null()\n        \n        \n        \n    c\n    —\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    110.85\n    20.15\n    —\n    —\n    —\n    CSV\n  \n  \n    #4CA64C66\n    3\n    \n        \n            \n\n    col_vals_gt\n    \n        \n            \n            \n        \n    \n\n        \n        \n            col_vals_gt()\n        \n        \n        \n    a\n    2\n    \n    \n        \n            \n            \n            \n        \n    \n\n    ✓\n    13\n    90.69\n    40.31\n    —\n    —\n    —\n    CSV\n  \n\n\n\n\n\n\n        \n\n\nThere are quite a few failures here across the three validation steps. In the FAIL column of the validation report table, there are 12 failing test units if we were to tally them up. So if the input table has 13 rows in total, does this mean there would be one row in the table returned by get_sundered_data()? Not so:\n\nvalidation_2.get_sundered_data()\n\n\nshape: (4, 8)date_timedateabcdefdatetime[μs]datei64stri64f64boolstr2016-01-04 00:32:002016-01-043\"5-egh-163\"89999.99true\"low\"2016-01-05 13:32:002016-01-056\"8-kdg-938\"32343.23true\"high\"2016-01-11 06:15:002016-01-114\"2-dhe-923\"43291.03true\"mid\"2016-01-17 11:27:002016-01-174\"5-boe-639\"21035.64false\"low\"\n\n\nThere are four rows. This is because the different validation steps tested values in different columns of the table. Some of the failing test units had to have occurred in more than once in certain rows. The rows that didn’t have any failing test units across the three different tests (in three different columns) are the ones seen above. This brings us to the third important thing about the sundering process:\n\nthe absence of test-unit failures in a row across all validation steps means those rows are returned as the \"pass\" set, all others are placed in the \"fail\" set\n\nIn validations where many validation steps are used, we can be more confident about the level of data quality for those rows returned in the \"pass\" set. But not every type of validation step is considered within this splitting procedure. The next section will explain the rules on that.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/sundering.html#the-validation-methods-considered-when-sundering",
    "href": "user-guide/sundering.html#the-validation-methods-considered-when-sundering",
    "title": "Sundering Data",
    "section": "The Validation Methods Considered When Sundering",
    "text": "The Validation Methods Considered When Sundering\nThe sundering procedure relies on row-level validation types to be used. This makes sense as it’s impossible to judge the quality of a row when using the col_exists() validation method, for example. Luckily, we have many row-level validation methods; here’s a list:\n\ncol_vals_gt()\ncol_vals_lt()\ncol_vals_ge()\ncol_vals_le()\ncol_vals_eq()\ncol_vals_ne()\ncol_vals_between()\ncol_vals_outside()\ncol_vals_in_set()\ncol_vals_not_in_set()\ncol_vals_null()\ncol_vals_not_null()\ncol_vals_regex()\n\nThis is the same list of validation methods that are considered when creating data extracts.\nThere are some additional caveats though. Even if using a validation method drawn from the set above, the validation step won’t be used for sundering if:\n\nthe active= parameter for that step has been set to False\nthe pre= parameter has been used\n\nThe first one makes intuitive sense (you decided to skip this validation step entirely), the second one requires some explanation. Using pre= allows you to modify the target table, there’s no easy or practical way to compare rows in a mutated table compared to the original table (e.g., a mutation may drastically reduce the number of rows).\nSo long as you’re aware of the rules and limitations of sundering, you’ll hopefully find it to be a simple and useful way to filter your input table on the basis of a validation plan.",
    "crumbs": [
      "User Guide",
      "Post Interrogation",
      "Sundering Data"
    ]
  },
  {
    "objectID": "user-guide/preview.html",
    "href": "user-guide/preview.html",
    "title": "Previewing Data",
    "section": "",
    "text": "In many cases, it’s good to look at your data tables. Before validating a table, you’ll likely want to inspect a portion of it before diving into the creation of data-quality rules. This is pretty easily done with Polars and Pandas DataFrames, however, it’s not as easy with database tables and each table backend displays things differently.\nTo make this common task a little better, you can use the preview() function in Pointblank. It has been designed to work with every table that the package supports (i.e., DataFrames and Ibis-backend tables, the latter of which are largely database tables). Plus, what’s shown in the output is consistent, no matter what type of data you’re looking at.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  },
  {
    "objectID": "user-guide/preview.html#viewing-a-table-with-preview",
    "href": "user-guide/preview.html#viewing-a-table-with-preview",
    "title": "Previewing Data",
    "section": "Viewing a Table with preview()",
    "text": "Viewing a Table with preview()\nLet’s look at how preview() works. It requires only a table and, for this first example, let’s use the nycflights dataset:\n\nimport pointblank as pb\n\nnycflights = pb.load_dataset(dataset=\"nycflights\", tbl_type=\"polars\")\npb.preview(nycflights)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  yearInt64\n  monthInt64\n  dayInt64\n  dep_timeInt64\n  sched_dep_timeInt64\n  dep_delayInt64\n  arr_timeInt64\n  sched_arr_timeInt64\n  arr_delayInt64\n  carrierString\n  flightInt64\n  tailnumString\n  originString\n  destString\n  air_timeInt64\n  distanceInt64\n  hourInt64\n  minuteInt64\n\n\n\n  \n    1\n    2013\n    1\n    1\n    517\n    515\n    2\n    830\n    819\n    11\n    UA\n    1545\n    N14228\n    EWR\n    IAH\n    227\n    1400\n    5\n    15\n  \n  \n    2\n    2013\n    1\n    1\n    533\n    529\n    4\n    850\n    830\n    20\n    UA\n    1714\n    N24211\n    LGA\n    IAH\n    227\n    1416\n    5\n    29\n  \n  \n    3\n    2013\n    1\n    1\n    542\n    540\n    2\n    923\n    850\n    33\n    AA\n    1141\n    N619AA\n    JFK\n    MIA\n    160\n    1089\n    5\n    40\n  \n  \n    4\n    2013\n    1\n    1\n    544\n    545\n    -1\n    1004\n    1022\n    -18\n    B6\n    725\n    N804JB\n    JFK\n    BQN\n    183\n    1576\n    5\n    45\n  \n  \n    5\n    2013\n    1\n    1\n    554\n    600\n    -6\n    812\n    837\n    -25\n    DL\n    461\n    N668DN\n    LGA\n    ATL\n    116\n    762\n    6\n    0\n  \n  \n    336772\n    2013\n    9\n    30\n    None\n    1455\n    None\n    None\n    1634\n    None\n    9E\n    3393\n    None\n    JFK\n    DCA\n    None\n    213\n    14\n    55\n  \n  \n    336773\n    2013\n    9\n    30\n    None\n    2200\n    None\n    None\n    2312\n    None\n    9E\n    3525\n    None\n    LGA\n    SYR\n    None\n    198\n    22\n    0\n  \n  \n    336774\n    2013\n    9\n    30\n    None\n    1210\n    None\n    None\n    1330\n    None\n    MQ\n    3461\n    N535MQ\n    LGA\n    BNA\n    None\n    764\n    12\n    10\n  \n  \n    336775\n    2013\n    9\n    30\n    None\n    1159\n    None\n    None\n    1344\n    None\n    MQ\n    3572\n    N511MQ\n    LGA\n    CLE\n    None\n    419\n    11\n    59\n  \n  \n    336776\n    2013\n    9\n    30\n    None\n    840\n    None\n    None\n    1020\n    None\n    MQ\n    3531\n    N839MQ\n    LGA\n    RDU\n    None\n    431\n    8\n    40\n  \n\n\n\n\n\n\n        \n\n\nThis is an HTML table using the style of the other reporting tables in the library. The header is more minimal here, only showing the type of table we’re looking at (POLARS in this case) along with the table dimensions. The column headers provide both the column names and the column data types.\nBy default, we’re getting the first five rows and the last five rows. Row numbers (from the original dataset) provide an indication of which rows are the head and tail rows. The blue lines provide additional demarcation of the column containing the row numbers and the head and tail row groups. Finally, any cells with missing values are prominently styled with red lettering and a lighter red background.\nIf you’d rather not see the row numbers in the table, you can use the show_row_numbers=False option. Let’s try that with the game_revenue dataset as a DuckDB table:\n\ngame_revenue = pb.load_dataset(dataset=\"game_revenue\", tbl_type=\"duckdb\")\npb.preview(game_revenue, show_row_numbers=False)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    DuckDBRows2,000Columns11\n  \n\n  player_idstring\n  session_idstring\n  session_starttimestamp\n  timetimestamp\n  item_typestring\n  item_namestring\n  item_revenuefloat64\n  session_durationfloat64\n  start_daydate\n  acquisitionstring\n  countrystring\n\n\n\n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:31:27+00:00\n    iap\n    offer2\n    8.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:36:57+00:00\n    iap\n    gems3\n    22.49\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:37:45+00:00\n    iap\n    gold7\n    107.99\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-eol2j8bs\n    2015-01-01 01:31:03+00:00\n    2015-01-01 01:42:33+00:00\n    ad\n    ad_20sec\n    0.76\n    16.3\n    2015-01-01\n    google\n    Germany\n  \n  \n    ECPANOIXLZHF896\n    ECPANOIXLZHF896-hdu9jkls\n    2015-01-01 11:50:02+00:00\n    2015-01-01 11:55:20+00:00\n    ad\n    ad_5sec\n    0.03\n    35.2\n    2015-01-01\n    google\n    Germany\n  \n  \n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:02:50+00:00\n    ad\n    ad_survey\n    1.332\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    NAOJRDMCSEBI281\n    NAOJRDMCSEBI281-j2vs9ilp\n    2015-01-21 01:57:50+00:00\n    2015-01-21 02:22:14+00:00\n    ad\n    ad_survey\n    1.35\n    25.8\n    2015-01-11\n    organic\n    Norway\n  \n  \n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:40:00+00:00\n    ad\n    ad_5sec\n    0.03\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    RMOSWHJGELCI675\n    RMOSWHJGELCI675-vbhcsmtr\n    2015-01-21 02:39:48+00:00\n    2015-01-21 02:47:12+00:00\n    iap\n    offer5\n    26.09\n    8.4\n    2015-01-10\n    other_campaign\n    France\n  \n  \n    GJCXNTWEBIPQ369\n    GJCXNTWEBIPQ369-9elq67md\n    2015-01-21 03:59:23+00:00\n    2015-01-21 04:06:29+00:00\n    ad\n    ad_5sec\n    0.12\n    18.5\n    2015-01-14\n    organic\n    United States\n  \n\n\n\n\n\n\n        \n\n\nWith the above preview, the row numbers are gone. The horizontal blue line still serves to divide the top and bottom rows of the table, however.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  },
  {
    "objectID": "user-guide/preview.html#adjusting-the-number-of-rows-shown",
    "href": "user-guide/preview.html#adjusting-the-number-of-rows-shown",
    "title": "Previewing Data",
    "section": "Adjusting the Number of Rows Shown",
    "text": "Adjusting the Number of Rows Shown\nIt could be that displaying the five top and bottom rows is not preferred. This can be changed with the n_head= and n_tail=. Maybe, you want three from the top along with the last row? Let’s try that out with the small_table dataset as a Pandas DataFrame:\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\npb.preview(small_table, n_head=3, n_tail=1)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nIf you’re looking at a small table and want to see the entirety of it, you can enlarge the n_head= and n_tail= values:\n\nsmall_table = pb.load_dataset(dataset=\"small_table\", tbl_type=\"pandas\")\npb.preview(small_table, n_head=10, n_tail=10)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PandasRows13Columns8\n  \n\n  \n  date_timedatetime64[ns]\n  datedatetime64[ns]\n  aint64\n  bobject\n  cfloat64\n  dfloat64\n  ebool\n  fobject\n\n\n\n  \n    1\n    2016-01-04 11:00:00\n    2016-01-04 00:00:00\n    2\n    1-bcd-345\n    3.0\n    3423.29\n    True\n    high\n  \n  \n    2\n    2016-01-04 00:32:00\n    2016-01-04 00:00:00\n    3\n    5-egh-163\n    8.0\n    9999.99\n    True\n    low\n  \n  \n    3\n    2016-01-05 13:32:00\n    2016-01-05 00:00:00\n    6\n    8-kdg-938\n    3.0\n    2343.23\n    True\n    high\n  \n  \n    4\n    2016-01-06 17:23:00\n    2016-01-06 00:00:00\n    2\n    5-jdo-903\n    NA\n    3892.4\n    False\n    mid\n  \n  \n    5\n    2016-01-09 12:36:00\n    2016-01-09 00:00:00\n    8\n    3-ldm-038\n    7.0\n    283.94\n    True\n    low\n  \n  \n    6\n    2016-01-11 06:15:00\n    2016-01-11 00:00:00\n    4\n    2-dhe-923\n    4.0\n    3291.03\n    True\n    mid\n  \n  \n    7\n    2016-01-15 18:46:00\n    2016-01-15 00:00:00\n    7\n    1-knw-093\n    3.0\n    843.34\n    True\n    high\n  \n  \n    8\n    2016-01-17 11:27:00\n    2016-01-17 00:00:00\n    4\n    5-boe-639\n    2.0\n    1035.64\n    False\n    low\n  \n  \n    9\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    10\n    2016-01-20 04:30:00\n    2016-01-20 00:00:00\n    3\n    5-bce-642\n    9.0\n    837.93\n    False\n    high\n  \n  \n    11\n    2016-01-26 20:07:00\n    2016-01-26 00:00:00\n    4\n    2-dmx-010\n    7.0\n    833.98\n    True\n    low\n  \n  \n    12\n    2016-01-28 02:51:00\n    2016-01-28 00:00:00\n    2\n    7-dmx-010\n    8.0\n    108.34\n    False\n    low\n  \n  \n    13\n    2016-01-30 11:23:00\n    2016-01-30 00:00:00\n    1\n    3-dka-303\n    NA\n    2230.09\n    True\n    high\n  \n\n\n\n\n\n\n        \n\n\nGiven that the table has 13 rows, asking for 20 rows to be displayed effectively shows the entire table.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  },
  {
    "objectID": "user-guide/preview.html#previewing-a-subset-of-columns",
    "href": "user-guide/preview.html#previewing-a-subset-of-columns",
    "title": "Previewing Data",
    "section": "Previewing a Subset of Columns",
    "text": "Previewing a Subset of Columns\nThe preview scales well to tables that have many columns by allowing for a horizontal scroll. However, previewing data from all columns can be impractical if you’re only concerned with a key set of them. To preview only a subset of a table’s columns, we can use the columns_subset= argument. Let’s do this with the nycflights dataset and provide a list of six columns from that table.\n\npb.preview(\n    nycflights,\n    columns_subset=[\"hour\", \"minute\", \"sched_dep_time\", \"year\", \"month\", \"day\"]\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  hourInt64\n  minuteInt64\n  sched_dep_timeInt64\n  yearInt64\n  monthInt64\n  dayInt64\n\n\n\n  \n    1\n    5\n    15\n    515\n    2013\n    1\n    1\n  \n  \n    2\n    5\n    29\n    529\n    2013\n    1\n    1\n  \n  \n    3\n    5\n    40\n    540\n    2013\n    1\n    1\n  \n  \n    4\n    5\n    45\n    545\n    2013\n    1\n    1\n  \n  \n    5\n    6\n    0\n    600\n    2013\n    1\n    1\n  \n  \n    336772\n    14\n    55\n    1455\n    2013\n    9\n    30\n  \n  \n    336773\n    22\n    0\n    2200\n    2013\n    9\n    30\n  \n  \n    336774\n    12\n    10\n    1210\n    2013\n    9\n    30\n  \n  \n    336775\n    11\n    59\n    1159\n    2013\n    9\n    30\n  \n  \n    336776\n    8\n    40\n    840\n    2013\n    9\n    30\n  \n\n\n\n\n\n\n        \n\n\nWhat we see are the six columns we specified from the nycflights dataset.\nNote that the columns are displayed in the order provided in the columns_subset= list. This can be useful for making quick, side-by-side comparisons. In the example above, we placed hour and minute next to the sched_dep_time column. In the original dataset, sched_dep_time is far apart from the other two columns, but, it’s useful to have them next to each other in the preview since hour and minute are derived from sched_dep_time (and this lets us spot check any issues).\nWe can also use column selectors within columns_subset=. Suppose we want to only see those columns that have \"dep_\" or \"arr_\" in the name. To do that, we use the matches() column selector function:\n\npb.preview(nycflights, columns_subset=pb.matches(\"dep_|arr_\"))\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  dep_timeInt64\n  sched_dep_timeInt64\n  dep_delayInt64\n  arr_timeInt64\n  sched_arr_timeInt64\n  arr_delayInt64\n\n\n\n  \n    1\n    517\n    515\n    2\n    830\n    819\n    11\n  \n  \n    2\n    533\n    529\n    4\n    850\n    830\n    20\n  \n  \n    3\n    542\n    540\n    2\n    923\n    850\n    33\n  \n  \n    4\n    544\n    545\n    -1\n    1004\n    1022\n    -18\n  \n  \n    5\n    554\n    600\n    -6\n    812\n    837\n    -25\n  \n  \n    336772\n    None\n    1455\n    None\n    None\n    1634\n    None\n  \n  \n    336773\n    None\n    2200\n    None\n    None\n    2312\n    None\n  \n  \n    336774\n    None\n    1210\n    None\n    None\n    1330\n    None\n  \n  \n    336775\n    None\n    1159\n    None\n    None\n    1344\n    None\n  \n  \n    336776\n    None\n    840\n    None\n    None\n    1020\n    None\n  \n\n\n\n\n\n\n        \n\n\nSeveral selectors can be combined together through use of the col() function and operators such as & (and), | (or), - (difference), and ~ (not). Let’s look at a column selection case where:\n\nthe first three columns are selected\nall columns containing \"dep_\" or \"arr_\" are selected\nany columns beginning with \"sched\" are omitted\n\nThis is how we put that together within col():\n\npb.preview(\n    nycflights,\n    columns_subset=pb.col((pb.first_n(3) | pb.matches(\"dep_|arr_\")) & ~ pb.starts_with(\"sched\"))\n)\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n  \n    PolarsRows336,776Columns18\n  \n\n  \n  yearInt64\n  monthInt64\n  dayInt64\n  dep_timeInt64\n  dep_delayInt64\n  arr_timeInt64\n  arr_delayInt64\n\n\n\n  \n    1\n    2013\n    1\n    1\n    517\n    2\n    830\n    11\n  \n  \n    2\n    2013\n    1\n    1\n    533\n    4\n    850\n    20\n  \n  \n    3\n    2013\n    1\n    1\n    542\n    2\n    923\n    33\n  \n  \n    4\n    2013\n    1\n    1\n    544\n    -1\n    1004\n    -18\n  \n  \n    5\n    2013\n    1\n    1\n    554\n    -6\n    812\n    -25\n  \n  \n    336772\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336773\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336774\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336775\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n  \n    336776\n    2013\n    9\n    30\n    None\n    None\n    None\n    None\n  \n\n\n\n\n\n\n        \n\n\nThis gives us a preview with only the columns that fit the specific selection rules. Incidentally, using selectors with a dataset through preview() is a good way to test out the use of selectors more generally. Since they are primarily used to select columns for validation, trying them beforehand with preview() can help verify that your selection logic is sound.",
    "crumbs": [
      "User Guide",
      "Data Inspection",
      "Previewing Data"
    ]
  }
]